{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DNN 기반 분류 모델 구현\n",
    "- 데이터 : iris.csv\n",
    "- 피처/속성 : 4개 Sepal.Length, Sepal.Width, Petal.Length, Petal.Width\n",
    "- 타겟/라벨 : 1개 Setosa와 나머지\n",
    "- 학습-방법 : 지도학습 > 분류> 이진분류\n",
    "- 학습 알고리즘 : 인공신경망(ANN) -> 심층 신경망 (MLP, DNN) : 은닉층이 많은 구성\n",
    "- 프레임워크 : Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] 모듈 로딩 및 데이터 준비 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 로딩\n",
    "# 모델 관련 모듈\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchmetrics.classification import F1Score, BinaryF1Score, BinaryConfusionMatrix\n",
    "from torchinfo import summary\n",
    "\n",
    "# 데이터 및 시각화 관련 모듈\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch v.2.4.1\n",
      "Pandas v.2.0.3\n"
     ]
    }
   ],
   "source": [
    "# 활용 패키지 버전 체크 ==> 사용자 정의 함수로 구현하기~~~\n",
    "print(f'Pytorch v.{torch.__version__}')\n",
    "print(f'Pandas v.{pd.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.length  sepal.width  petal.length  petal.width variety\n",
       "0           5.1          3.5           1.4          0.2  Setosa"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 데이터 로딩\n",
    "DATA_FILE = '../../ML/Data/iris.csv'\n",
    "\n",
    "### CSV => DataFrame\n",
    "irisDF = pd.read_csv(DATA_FILE)\n",
    "\n",
    "### 확인\n",
    "irisDF.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Setosa', 'Versicolor', 'Virginica'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 타겟 변경 => 정수화\n",
    "irisDF['variety'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 3개 => 2개 (replace로 변경도 가능함 / Setosa를 False로 주는 것도 가능)\n",
    "irisDF['variety'] = (irisDF['variety'] == 'Setosa')\n",
    "\n",
    "# 방법 2 \n",
    "# labels = dict(zip(irisDF['variety'].unique().tolist(),range(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "145    0\n",
       "146    0\n",
       "147    0\n",
       "148    0\n",
       "149    0\n",
       "Name: variety, Length: 150, dtype: int32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisDF['variety'] = irisDF['variety'].astype('int')\n",
    "irisDF['variety']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "고유값 : [1 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.length  sepal.width  petal.length  petal.width  variety\n",
       "0           5.1          3.5           1.4          0.2        1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 고유값 확인\n",
    "print(f\"고유값 : {irisDF['variety'].unique()}\")\n",
    "irisDF.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] 모델 클래스 설계 및 정의 <hr>\n",
    "- 클래스 목적 : iris 데이터를 학습 및 추론 목적\n",
    "- 클래스 이름 : IrisBCFModel\n",
    "- 부모 클래스 : nn.Module\n",
    "- 매 개 변 수 : 층별 입출력 개수 고정하므로 필요 없음\n",
    "- 속성 / 필드 :\n",
    "- 기능 / 역할 : __init__() : 모델 구조 설정, forward() : 순방향 학습 <=오버라이딩(상속관계에서만 가능)\n",
    "- 클래스 구조\n",
    "    * 입력층 : 입력 4개(피처 개수) / 출력 10개(퍼셉트론/뉴런 개수 10개)\n",
    "    * 은닉층 : 입력 10개          / 출력 5개\n",
    "    * 출력층 : 입력 5개          / 출력 1개(이진분류)\n",
    "- - -\n",
    "- 손실함수 / 활성화 함수\n",
    "    * 클래스 형태 ==> nn.MESLoss, nn.ReLU ==> __init__() 메서드\n",
    "    * 함수 형태 ==> torch.nn.fuctional 아래에 ==> forward() 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisBCFModel(nn.Module):\n",
    "    # 모델 구조 구성 및 인스턴스 생성 메서드\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.in_layer = nn.Linear(4,10)\n",
    "        self.hidden_layer = nn.Linear(10,5)\n",
    "        self.out_layer = nn.Linear(5,1)\n",
    "\n",
    "    #순방향 학습 진행 메서드\n",
    "    def forward(self,x) : \n",
    "        # 입력층\n",
    "        y = self.in_layer(x)    # \n",
    "        y=F.relu(y)             # relu 값의 범위 : 0<=y / 시그모이드 : 0~1\n",
    "        # 은닉층 : 10개 숫자의 값(>=0)\n",
    "        y = self.hidden_layer(y)\n",
    "        y = F.relu(y)\n",
    "        # 출력층 : 5개 숫자값 / 분류이므로 시그모이드 함수 적용해서 반환\n",
    "        return F.sigmoid(self.out_layer(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IrisBCFModel(\n",
      "  (in_layer): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (hidden_layer): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (out_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# [테스트] 모델 인스턴스 생성\n",
    "model = IrisBCFModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "IrisBCFModel                             [17, 1]                   --\n",
       "├─Linear: 1-1                            [17, 10]                  50\n",
       "├─Linear: 1-2                            [17, 5]                   55\n",
       "├─Linear: 1-3                            [17, 1]                   6\n",
       "==========================================================================================\n",
       "Total params: 111\n",
       "Trainable params: 111\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [테스트] 모델 사용 메모리 정보 확인\n",
    "summary(model, input_size=(17,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] 데이터셋 클래스 설계 및 정의 <hr>\n",
    "- 데이터셋 : iris.csv\n",
    "- 피처 개수 : 4개\n",
    "- 타겟 개수 : 1개\n",
    "- 클래스 이름 : IrisDataset\n",
    "- 부모 클래스 : utils.data.Dataset\n",
    "- 속성 / 필드 : featureDF, targetDF, n_rows, n_features\n",
    "- 필수 메서드\n",
    "    * _ _init_ _(self) : 데이터셋 저장 및 전처리, 개발자가 필요한 속성 설정\n",
    "    * _ _len_ _(self) : 데이터의 개수 반환\n",
    "    * _ _getItem_ _(self, index) : 특정 인덱스의 피처와 타겟 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisDataset(Dataset):\n",
    "\n",
    "    def __init__(self, featureDF, targetDF):\n",
    "        self.featureDF = featureDF\n",
    "        self.targetDF = targetDF\n",
    "        self.n_rows = featureDF.shape[0]\n",
    "        self.n_features = featureDF.shape[1]\n",
    "\n",
    "    def __len__(self) : \n",
    "        return self.n_rows\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 텐서화\n",
    "        feaureTS = torch.FloatTensor(self.featureDF.iloc[index].values)\n",
    "        targetTS = torch.FloatTensor(self.targetDF.iloc[index].values)\n",
    "        \n",
    "        # 피처와 타겟 반환\n",
    "        return feaureTS, targetTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4]) torch.Size([1, 1]) tensor([[5.1000, 3.5000, 1.4000, 0.2000]]) tensor([[1.]])\n"
     ]
    }
   ],
   "source": [
    "## [테스트] 데이터셋 인스턴스 생성\n",
    "# 피처와 타겟 데이터 추출\n",
    "featureDF = irisDF[irisDF.columns[:-1]]     # 2D (150,4)\n",
    "targetDF = irisDF[irisDF.columns[-1:]]      # 2D (150,1)\n",
    "\n",
    "# 커스텀데이터셋 인스턴스 생성\n",
    "irisDS = IrisDataset(featureDF, targetDF)\n",
    "\n",
    "# 데이터로더 인스턴스 생성\n",
    "irisDL = DataLoader(irisDS)\n",
    "for feature, label in irisDL:\n",
    "    print(feature.shape, label.shape, feature, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] 학습 준비 <hr>\n",
    "- 학습 횟수 : EPOCH   <- 처음부터 끝까지 공부하는 단위\n",
    "- 배치 크기 : BATCH_SIZE   <- 한번에 학습할 데이터셋 양\n",
    "- 위치 지정 : DEVICE  <- 텐서 저장 및 실행 위치 (GPU/CPU)\n",
    "- 학 습 율  : LR 가중치와 절편 업데이트 시 경사 하강법으로 업데이트 간격 설정 0.001~0.1 (하이퍼파라미터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 학습 진행 관련 설정\n",
    "EPOCH = 1000\n",
    "BATCH_SIZE = 10\n",
    "BATCH_CNT = irisDF.shape[0]//BATCH_SIZE # 선택사항 - 코드에 넣을 수도 있음\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 인스턴스/객체 : 모델, 데이터셋, 최적화 (, 손실함수, 성능지표)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스\n",
    "model = IrisBCFModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 4) (38, 4) (28, 4)\n",
      "(84, 1) (38, 1) (28, 1)\n"
     ]
    }
   ],
   "source": [
    "### DS과 DL 인스턴스\n",
    "\n",
    "# 학습/검증/테스트용 데이터 분리\n",
    "X_train,X_test, y_train, y_test = train_test_split(featureDF, targetDF, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=1)\n",
    "print(f'{X_train.shape} {X_test.shape} {X_val.shape}')\n",
    "print(f'{y_train.shape} {y_test.shape} {y_val.shape}')\n",
    "\n",
    "## 학습/검증/테스트용 데이터셋\n",
    "# irisDS = IrisDataset(X_train, y_train)\n",
    "trainDS = IrisDataset(X_train, y_train)\n",
    "valDS = IrisDataset(X_val, y_val)\n",
    "testDS = IrisDataset(X_test, y_test)\n",
    "\n",
    "# 학습용 데이터로더 인스턴스 (검증용은 필요 없음, 테스트는 양이 많을 때 개발자가 선택하여 인스턴스 생성)\n",
    "# irisDL = DataLoader(irisDS, batch_size = BATCH_SIZE)\n",
    "trainDL = DataLoader(trainDS, batch_size = BATCH_SIZE, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적화, 손실함수 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화 인스턴스 => W,b 텐서, 즉 model.parameters() 전달 - 최적화하는 이유 : 오차를 줄이기 위해서!\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# 손실함수 인스턴스 => 분류 => 이진분류 : BinaryCrossEntropyLoss => BCELoss\n",
    "#                            예측값은 확률값으로 전달 ==> sigmoid() AF 처리 후 전달\n",
    "reqLoss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNT =>9\n",
      "[0/1000]\n",
      "- Train Loss : 7.620583670270914e-07 Score : 1.0\n",
      "- Val Loss : 6.271666279644705e-07 Score : 1.0\n",
      "[1/1000]\n",
      "- Train Loss : 7.587528693243762e-07 Score : 1.0\n",
      "- Val Loss : 6.232936584638082e-07 Score : 1.0\n",
      "[2/1000]\n",
      "- Train Loss : 7.524881213940818e-07 Score : 1.0\n",
      "- Val Loss : 6.210398737493961e-07 Score : 1.0\n",
      "[3/1000]\n",
      "- Train Loss : 7.484982636659273e-07 Score : 1.0\n",
      "- Val Loss : 6.194233037604135e-07 Score : 1.0\n",
      "[4/1000]\n",
      "- Train Loss : 7.422298280188342e-07 Score : 1.0\n",
      "- Val Loss : 6.137498189673352e-07 Score : 1.0\n",
      "[5/1000]\n",
      "- Train Loss : 7.400457692578281e-07 Score : 1.0\n",
      "- Val Loss : 6.123800062596274e-07 Score : 1.0\n",
      "[6/1000]\n",
      "- Train Loss : 7.352386681702683e-07 Score : 1.0\n",
      "- Val Loss : 6.11013717843889e-07 Score : 1.0\n",
      "[7/1000]\n",
      "- Train Loss : 7.330804688226383e-07 Score : 1.0\n",
      "- Val Loss : 6.096401534705365e-07 Score : 1.0\n",
      "[8/1000]\n",
      "- Train Loss : 7.2959296638765e-07 Score : 1.0\n",
      "- Val Loss : 6.082577215238416e-07 Score : 1.0\n",
      "[9/1000]\n",
      "- Train Loss : 7.247745597756724e-07 Score : 1.0\n",
      "- Val Loss : 6.068534048608853e-07 Score : 1.0\n",
      "[10/1000]\n",
      "- Train Loss : 7.21267790579658e-07 Score : 1.0\n",
      "- Val Loss : 6.054280561329506e-07 Score : 1.0\n",
      "[11/1000]\n",
      "- Train Loss : 7.177413685610645e-07 Score : 1.0\n",
      "- Val Loss : 6.039394406798237e-07 Score : 1.0\n",
      "[12/1000]\n",
      "- Train Loss : 7.141772656717017e-07 Score : 1.0\n",
      "- Val Loss : 5.9815994291057e-07 Score : 1.0\n",
      "[13/1000]\n",
      "- Train Loss : 7.10592068742244e-07 Score : 1.0\n",
      "- Val Loss : 5.966173262095253e-07 Score : 1.0\n",
      "[14/1000]\n",
      "- Train Loss : 7.069943058645044e-07 Score : 1.0\n",
      "- Val Loss : 5.950644776930858e-07 Score : 1.0\n",
      "[15/1000]\n",
      "- Train Loss : 7.033894375594072e-07 Score : 1.0\n",
      "- Val Loss : 5.935043532190321e-07 Score : 1.0\n",
      "[16/1000]\n",
      "- Train Loss : 7.011053778032622e-07 Score : 1.0\n",
      "- Val Loss : 5.876800059922971e-07 Score : 1.0\n",
      "[17/1000]\n",
      "- Train Loss : 6.961678048848322e-07 Score : 1.0\n",
      "- Val Loss : 5.733296575272107e-07 Score : 1.0\n",
      "[18/1000]\n",
      "- Train Loss : 6.925460793480751e-07 Score : 1.0\n",
      "- Val Loss : 5.717424755857792e-07 Score : 1.0\n",
      "[19/1000]\n",
      "- Train Loss : 6.90244279313548e-07 Score : 1.0\n",
      "- Val Loss : 5.701543273062271e-07 Score : 1.0\n",
      "[20/1000]\n",
      "- Train Loss : 6.852910487964436e-07 Score : 1.0\n",
      "- Val Loss : 5.685357677975844e-07 Score : 1.0\n",
      "[21/1000]\n",
      "- Train Loss : 6.81642196711765e-07 Score : 1.0\n",
      "- Val Loss : 5.66895039355586e-07 Score : 1.0\n",
      "[22/1000]\n",
      "- Train Loss : 6.77980882496993e-07 Score : 1.0\n",
      "- Val Loss : 5.652456707139208e-07 Score : 1.0\n",
      "[23/1000]\n",
      "- Train Loss : 6.743133928763554e-07 Score : 1.0\n",
      "- Val Loss : 5.635912998513959e-07 Score : 1.0\n",
      "[24/1000]\n",
      "- Train Loss : 6.719680859153717e-07 Score : 1.0\n",
      "- Val Loss : 5.619403395940026e-07 Score : 1.0\n",
      "[25/1000]\n",
      "- Train Loss : 6.696252421692053e-07 Score : 1.0\n",
      "- Val Loss : 5.602998385256797e-07 Score : 1.0\n",
      "[26/1000]\n",
      "- Train Loss : 6.672886409357185e-07 Score : 1.0\n",
      "- Val Loss : 5.586676365965104e-07 Score : 1.0\n",
      "[27/1000]\n",
      "- Train Loss : 6.649586753818918e-07 Score : 1.0\n",
      "- Val Loss : 5.570515781982976e-07 Score : 1.0\n",
      "[28/1000]\n",
      "- Train Loss : 6.626390565145357e-07 Score : 1.0\n",
      "- Val Loss : 5.554462632062496e-07 Score : 1.0\n",
      "[29/1000]\n",
      "- Train Loss : 6.590017262750633e-07 Score : 1.0\n",
      "- Val Loss : 5.538508389690833e-07 Score : 1.0\n",
      "[30/1000]\n",
      "- Train Loss : 6.553677600606989e-07 Score : 1.0\n",
      "- Val Loss : 5.522293236026599e-07 Score : 1.0\n",
      "[31/1000]\n",
      "- Train Loss : 6.530418856540463e-07 Score : 1.0\n",
      "- Val Loss : 5.463463708110794e-07 Score : 1.0\n",
      "[32/1000]\n",
      "- Train Loss : 6.507171682478151e-07 Score : 1.0\n",
      "- Val Loss : 5.447490707410907e-07 Score : 1.0\n",
      "[33/1000]\n",
      "- Train Loss : 6.470851489205465e-07 Score : 1.0\n",
      "- Val Loss : 5.431653562482097e-07 Score : 1.0\n",
      "[34/1000]\n",
      "- Train Loss : 6.434604213407308e-07 Score : 1.0\n",
      "- Val Loss : 5.415780037765217e-07 Score : 1.0\n",
      "[35/1000]\n",
      "- Train Loss : 6.38507517673285e-07 Score : 1.0\n",
      "- Val Loss : 5.399765541369561e-07 Score : 1.0\n",
      "[36/1000]\n",
      "- Train Loss : 6.361949237051098e-07 Score : 1.0\n",
      "- Val Loss : 5.341157702787314e-07 Score : 1.0\n",
      "[37/1000]\n",
      "- Train Loss : 6.325571320076051e-07 Score : 1.0\n",
      "- Val Loss : 5.282544748297369e-07 Score : 1.0\n",
      "[38/1000]\n",
      "- Train Loss : 6.289190781987802e-07 Score : 1.0\n",
      "- Val Loss : 5.266480798127304e-07 Score : 1.0\n",
      "[39/1000]\n",
      "- Train Loss : 6.266030175715464e-07 Score : 1.0\n",
      "- Val Loss : 5.250483923191496e-07 Score : 1.0\n",
      "[40/1000]\n",
      "- Train Loss : 6.183335629196386e-07 Score : 1.0\n",
      "- Val Loss : 5.234423952060752e-07 Score : 1.0\n",
      "[41/1000]\n",
      "- Train Loss : 6.146900075363091e-07 Score : 1.0\n",
      "- Val Loss : 5.217992793404846e-07 Score : 1.0\n",
      "[42/1000]\n",
      "- Train Loss : 6.110247091505194e-07 Score : 1.0\n",
      "- Val Loss : 5.15884948981693e-07 Score : 1.0\n",
      "[43/1000]\n",
      "- Train Loss : 6.086767301541032e-07 Score : 1.0\n",
      "- Val Loss : 5.142288728166022e-07 Score : 1.0\n",
      "[44/1000]\n",
      "- Train Loss : 6.063285932593013e-07 Score : 1.0\n",
      "- Val Loss : 5.125806410433142e-07 Score : 1.0\n",
      "[45/1000]\n",
      "- Train Loss : 6.039863672905691e-07 Score : 1.0\n",
      "- Val Loss : 5.109486096444016e-07 Score : 1.0\n",
      "[46/1000]\n",
      "- Train Loss : 6.016583573082496e-07 Score : 1.0\n",
      "- Val Loss : 5.093552886137331e-07 Score : 1.0\n",
      "[47/1000]\n",
      "- Train Loss : 5.993524412575526e-07 Score : 1.0\n",
      "- Val Loss : 5.07782260683598e-07 Score : 1.0\n",
      "[48/1000]\n",
      "- Train Loss : 5.970600523615607e-07 Score : 1.0\n",
      "- Val Loss : 5.01968145272258e-07 Score : 1.0\n",
      "[49/1000]\n",
      "- Train Loss : 5.947770189449228e-07 Score : 1.0\n",
      "- Val Loss : 4.961640911460563e-07 Score : 1.0\n",
      "[50/1000]\n",
      "- Train Loss : 5.938255623523977e-07 Score : 1.0\n",
      "- Val Loss : 4.946394369653717e-07 Score : 1.0\n",
      "[51/1000]\n",
      "- Train Loss : 5.902407988540087e-07 Score : 1.0\n",
      "- Val Loss : 4.931256398776895e-07 Score : 1.0\n",
      "[52/1000]\n",
      "- Train Loss : 5.853346532407816e-07 Score : 1.0\n",
      "- Val Loss : 4.915987119602505e-07 Score : 1.0\n",
      "[53/1000]\n",
      "- Train Loss : 5.817439811847988e-07 Score : 1.0\n",
      "- Val Loss : 4.900577437183529e-07 Score : 1.0\n",
      "[54/1000]\n",
      "- Train Loss : 5.768198623930706e-07 Score : 1.0\n",
      "- Val Loss : 4.885005751020799e-07 Score : 1.0\n",
      "[55/1000]\n",
      "- Train Loss : 5.758593266433662e-07 Score : 1.0\n",
      "- Val Loss : 4.869484087066667e-07 Score : 1.0\n",
      "[56/1000]\n",
      "- Train Loss : 5.709319215914851e-07 Score : 1.0\n",
      "- Val Loss : 4.85417672280164e-07 Score : 1.0\n",
      "[57/1000]\n",
      "- Train Loss : 5.673392446207496e-07 Score : 1.0\n",
      "- Val Loss : 4.838801714868168e-07 Score : 1.0\n",
      "[58/1000]\n",
      "- Train Loss : 5.650678025246078e-07 Score : 1.0\n",
      "- Val Loss : 4.780893618772097e-07 Score : 1.0\n",
      "[59/1000]\n",
      "- Train Loss : 5.627990466747532e-07 Score : 1.0\n",
      "- Val Loss : 4.7656996571276977e-07 Score : 1.0\n",
      "[60/1000]\n",
      "- Train Loss : 5.605414566092451e-07 Score : 1.0\n",
      "- Val Loss : 4.750790765228885e-07 Score : 1.0\n",
      "[61/1000]\n",
      "- Train Loss : 5.583015827426152e-07 Score : 1.0\n",
      "- Val Loss : 4.693469861649646e-07 Score : 1.0\n",
      "[62/1000]\n",
      "- Train Loss : 5.560704221036556e-07 Score : 1.0\n",
      "- Val Loss : 4.6788150598331413e-07 Score : 1.0\n",
      "[63/1000]\n",
      "- Train Loss : 5.538451392321045e-07 Score : 1.0\n",
      "- Val Loss : 4.621504103852203e-07 Score : 1.0\n",
      "[64/1000]\n",
      "- Train Loss : 5.516152220429326e-07 Score : 1.0\n",
      "- Val Loss : 4.564353730529547e-07 Score : 1.0\n",
      "[65/1000]\n",
      "- Train Loss : 5.480708631624667e-07 Score : 1.0\n",
      "- Val Loss : 4.549786751795182e-07 Score : 1.0\n",
      "[66/1000]\n",
      "- Train Loss : 5.445268279736915e-07 Score : 1.0\n",
      "- Val Loss : 4.535183393272746e-07 Score : 1.0\n",
      "[67/1000]\n",
      "- Train Loss : 5.423036080445501e-07 Score : 1.0\n",
      "- Val Loss : 4.5206544996290177e-07 Score : 1.0\n",
      "[68/1000]\n",
      "- Train Loss : 5.400883560626982e-07 Score : 1.0\n",
      "- Val Loss : 4.5063481479701295e-07 Score : 1.0\n",
      "[69/1000]\n",
      "- Train Loss : 5.36561088892995e-07 Score : 1.0\n",
      "- Val Loss : 4.492061123073654e-07 Score : 1.0\n",
      "[70/1000]\n",
      "- Train Loss : 5.330336835622044e-07 Score : 1.0\n",
      "- Val Loss : 4.4777610241908405e-07 Score : 1.0\n",
      "[71/1000]\n",
      "- Train Loss : 5.308324975632988e-07 Score : 1.0\n",
      "- Val Loss : 4.4636686880039633e-07 Score : 1.0\n",
      "[72/1000]\n",
      "- Train Loss : 5.273197203337077e-07 Score : 1.0\n",
      "- Val Loss : 4.4496451323539077e-07 Score : 1.0\n",
      "[73/1000]\n",
      "- Train Loss : 5.238086815653434e-07 Score : 1.0\n",
      "- Val Loss : 4.4355147110763937e-07 Score : 1.0\n",
      "[74/1000]\n",
      "- Train Loss : 5.229411195431213e-07 Score : 1.0\n",
      "- Val Loss : 4.4214652916707564e-07 Score : 1.0\n",
      "[75/1000]\n",
      "- Train Loss : 5.194328889975471e-07 Score : 1.0\n",
      "- Val Loss : 4.407689573326934e-07 Score : 1.0\n",
      "[76/1000]\n",
      "- Train Loss : 5.146124099692568e-07 Score : 1.0\n",
      "- Val Loss : 4.3935776261605497e-07 Score : 1.0\n",
      "[77/1000]\n",
      "- Train Loss : 5.124177763586127e-07 Score : 1.0\n",
      "- Val Loss : 4.336710048846726e-07 Score : 1.0\n",
      "[78/1000]\n",
      "- Train Loss : 5.102169918163529e-07 Score : 1.0\n",
      "- Val Loss : 4.322630218211998e-07 Score : 1.0\n",
      "[79/1000]\n",
      "- Train Loss : 5.067036086517065e-07 Score : 1.0\n",
      "- Val Loss : 4.3085898937533784e-07 Score : 1.0\n",
      "[80/1000]\n",
      "- Train Loss : 5.058414818866671e-07 Score : 1.0\n",
      "- Val Loss : 4.2946678036059893e-07 Score : 1.0\n",
      "[81/1000]\n",
      "- Train Loss : 5.036654384424663e-07 Score : 1.0\n",
      "- Val Loss : 4.281056646959769e-07 Score : 1.0\n",
      "[82/1000]\n",
      "- Train Loss : 5.015078130554684e-07 Score : 1.0\n",
      "- Val Loss : 4.267575093308551e-07 Score : 1.0\n",
      "[83/1000]\n",
      "- Train Loss : 5.006818387048245e-07 Score : 1.0\n",
      "- Val Loss : 4.2542538380985206e-07 Score : 1.0\n",
      "[84/1000]\n",
      "- Train Loss : 4.985440899613675e-07 Score : 1.0\n",
      "- Val Loss : 4.241247495428979e-07 Score : 1.0\n",
      "[85/1000]\n",
      "- Train Loss : 4.964251808638032e-07 Score : 1.0\n",
      "- Val Loss : 4.2283448919988587e-07 Score : 1.0\n",
      "[86/1000]\n",
      "- Train Loss : 4.943118246577191e-07 Score : 1.0\n",
      "- Val Loss : 4.1729725808181684e-07 Score : 1.0\n",
      "[87/1000]\n",
      "- Train Loss : 4.92206916804763e-07 Score : 1.0\n",
      "- Val Loss : 4.160394269092649e-07 Score : 1.0\n",
      "[88/1000]\n",
      "- Train Loss : 4.887902085057602e-07 Score : 1.0\n",
      "- Val Loss : 4.1052564370147593e-07 Score : 1.0\n",
      "[89/1000]\n",
      "- Train Loss : 4.880218441706397e-07 Score : 1.0\n",
      "- Val Loss : 4.0501549847249407e-07 Score : 1.0\n",
      "[90/1000]\n",
      "- Train Loss : 4.8328334085252e-07 Score : 1.0\n",
      "- Val Loss : 4.037479186536075e-07 Score : 1.0\n",
      "[91/1000]\n",
      "- Train Loss : 4.79857401024396e-07 Score : 1.0\n",
      "- Val Loss : 4.0245666355076537e-07 Score : 1.0\n",
      "[92/1000]\n",
      "- Train Loss : 4.777411580410748e-07 Score : 1.0\n",
      "- Val Loss : 4.011548639937246e-07 Score : 1.0\n",
      "[93/1000]\n",
      "- Train Loss : 4.7429679857676393e-07 Score : 1.0\n",
      "- Val Loss : 3.9986622368815006e-07 Score : 1.0\n",
      "[94/1000]\n",
      "- Train Loss : 4.721831542061257e-07 Score : 1.0\n",
      "- Val Loss : 3.985753096458211e-07 Score : 1.0\n",
      "[95/1000]\n",
      "- Train Loss : 4.700685000753108e-07 Score : 1.0\n",
      "- Val Loss : 3.972869819790503e-07 Score : 1.0\n",
      "[96/1000]\n",
      "- Train Loss : 4.6795789919605744e-07 Score : 1.0\n",
      "- Val Loss : 3.9602082324563526e-07 Score : 1.0\n",
      "[97/1000]\n",
      "- Train Loss : 4.6585872029128143e-07 Score : 1.0\n",
      "- Val Loss : 3.905059315911785e-07 Score : 1.0\n",
      "[98/1000]\n",
      "- Train Loss : 4.6376842356544836e-07 Score : 1.0\n",
      "- Val Loss : 3.8927379364395165e-07 Score : 1.0\n",
      "[99/1000]\n",
      "- Train Loss : 4.603664365276927e-07 Score : 1.0\n",
      "- Val Loss : 3.8803889879091e-07 Score : 1.0\n",
      "[100/1000]\n",
      "- Train Loss : 4.582870695281195e-07 Score : 1.0\n",
      "- Val Loss : 3.8680656189171714e-07 Score : 1.0\n",
      "[101/1000]\n",
      "- Train Loss : 4.5356148782439556e-07 Score : 1.0\n",
      "- Val Loss : 3.813170224020723e-07 Score : 1.0\n",
      "[102/1000]\n",
      "- Train Loss : 4.5280815146813643e-07 Score : 1.0\n",
      "- Val Loss : 3.8008920455467887e-07 Score : 1.0\n",
      "[103/1000]\n",
      "- Train Loss : 4.5073575094534135e-07 Score : 1.0\n",
      "- Val Loss : 3.7888571569055784e-07 Score : 1.0\n",
      "[104/1000]\n",
      "- Train Loss : 4.486775821987976e-07 Score : 1.0\n",
      "- Val Loss : 3.7768788274661347e-07 Score : 1.0\n",
      "[105/1000]\n",
      "- Train Loss : 4.4794644161861976e-07 Score : 1.0\n",
      "- Val Loss : 3.764988321108831e-07 Score : 1.0\n",
      "[106/1000]\n",
      "- Train Loss : 4.445737723636335e-07 Score : 1.0\n",
      "- Val Loss : 3.753077919554926e-07 Score : 1.0\n",
      "[107/1000]\n",
      "- Train Loss : 4.4384549448118884e-07 Score : 1.0\n",
      "- Val Loss : 3.69855513326911e-07 Score : 1.0\n",
      "[108/1000]\n",
      "- Train Loss : 4.404692401433547e-07 Score : 1.0\n",
      "- Val Loss : 3.686743923481117e-07 Score : 1.0\n",
      "[109/1000]\n",
      "- Train Loss : 4.384243818359816e-07 Score : 1.0\n",
      "- Val Loss : 3.6749216292264464e-07 Score : 1.0\n",
      "[110/1000]\n",
      "- Train Loss : 4.3637805112616155e-07 Score : 1.0\n",
      "- Val Loss : 3.663110135221359e-07 Score : 1.0\n",
      "[111/1000]\n",
      "- Train Loss : 4.3433480417181477e-07 Score : 1.0\n",
      "- Val Loss : 3.6514580870061764e-07 Score : 1.0\n",
      "[112/1000]\n",
      "- Train Loss : 4.322989740993914e-07 Score : 1.0\n",
      "- Val Loss : 3.639823091816652e-07 Score : 1.0\n",
      "[113/1000]\n",
      "- Train Loss : 4.3026801045521603e-07 Score : 1.0\n",
      "- Val Loss : 3.628415754519665e-07 Score : 1.0\n",
      "[114/1000]\n",
      "- Train Loss : 4.282489468228808e-07 Score : 1.0\n",
      "- Val Loss : 3.61703627049792e-07 Score : 1.0\n",
      "[115/1000]\n",
      "- Train Loss : 4.275555419831613e-07 Score : 1.0\n",
      "- Val Loss : 3.605726135447185e-07 Score : 1.0\n",
      "[116/1000]\n",
      "- Train Loss : 4.2554454577409865e-07 Score : 1.0\n",
      "- Val Loss : 3.594637689730007e-07 Score : 1.0\n",
      "[117/1000]\n",
      "- Train Loss : 4.2487061098957685e-07 Score : 1.0\n",
      "- Val Loss : 3.583681973395869e-07 Score : 1.0\n",
      "[118/1000]\n",
      "- Train Loss : 4.215580812822534e-07 Score : 1.0\n",
      "- Val Loss : 3.5728743341678637e-07 Score : 1.0\n",
      "[119/1000]\n",
      "- Train Loss : 4.2090096642589793e-07 Score : 1.0\n",
      "- Val Loss : 3.562126380529662e-07 Score : 1.0\n",
      "[120/1000]\n",
      "- Train Loss : 4.1760110924826677e-07 Score : 1.0\n",
      "- Val Loss : 3.5515017771103885e-07 Score : 1.0\n",
      "[121/1000]\n",
      "- Train Loss : 4.156315053115299e-07 Score : 1.0\n",
      "- Val Loss : 3.49824404111132e-07 Score : 1.0\n",
      "[122/1000]\n",
      "- Train Loss : 4.136565725016478e-07 Score : 1.0\n",
      "- Val Loss : 3.4872891774284653e-07 Score : 1.0\n",
      "[123/1000]\n",
      "- Train Loss : 4.1166539609270734e-07 Score : 1.0\n",
      "- Val Loss : 3.476325503015687e-07 Score : 1.0\n",
      "[124/1000]\n",
      "- Train Loss : 4.083480883802319e-07 Score : 1.0\n",
      "- Val Loss : 3.465255815626733e-07 Score : 1.0\n",
      "[125/1000]\n",
      "- Train Loss : 4.063508838056704e-07 Score : 1.0\n",
      "- Val Loss : 3.4542804883130884e-07 Score : 1.0\n",
      "[126/1000]\n",
      "- Train Loss : 4.0303404584089247e-07 Score : 1.0\n",
      "- Val Loss : 3.4432900974934455e-07 Score : 1.0\n",
      "[127/1000]\n",
      "- Train Loss : 4.0104159242875744e-07 Score : 1.0\n",
      "- Val Loss : 3.432364508171304e-07 Score : 1.0\n",
      "[128/1000]\n",
      "- Train Loss : 3.990522999449316e-07 Score : 1.0\n",
      "- Val Loss : 3.378918052021618e-07 Score : 1.0\n",
      "[129/1000]\n",
      "- Train Loss : 3.9574238167953507e-07 Score : 1.0\n",
      "- Val Loss : 3.368083980603842e-07 Score : 1.0\n",
      "[130/1000]\n",
      "- Train Loss : 3.9508288768925263e-07 Score : 1.0\n",
      "- Val Loss : 3.3572828783690056e-07 Score : 1.0\n",
      "[131/1000]\n",
      "- Train Loss : 3.897925762904227e-07 Score : 1.0\n",
      "- Val Loss : 3.346650032653997e-07 Score : 1.0\n",
      "[132/1000]\n",
      "- Train Loss : 3.864949182425587e-07 Score : 1.0\n",
      "- Val Loss : 3.3357000006617454e-07 Score : 1.0\n",
      "[133/1000]\n",
      "- Train Loss : 3.8582583646492213e-07 Score : 1.0\n",
      "- Val Loss : 3.2820554451973294e-07 Score : 1.0\n",
      "[134/1000]\n",
      "- Train Loss : 3.851530914447368e-07 Score : 1.0\n",
      "- Val Loss : 3.2712537745283043e-07 Score : 1.0\n",
      "[135/1000]\n",
      "- Train Loss : 3.831739713670383e-07 Score : 1.0\n",
      "- Val Loss : 3.2607366051706776e-07 Score : 1.0\n",
      "[136/1000]\n",
      "- Train Loss : 3.81210908963049e-07 Score : 1.0\n",
      "- Val Loss : 3.165085331602313e-07 Score : 1.0\n",
      "[137/1000]\n",
      "- Train Loss : 3.7924722483416593e-07 Score : 1.0\n",
      "- Val Loss : 3.1545488354822737e-07 Score : 1.0\n",
      "[138/1000]\n",
      "- Train Loss : 3.759583329099314e-07 Score : 1.0\n",
      "- Val Loss : 3.1440330872101185e-07 Score : 1.0\n",
      "[139/1000]\n",
      "- Train Loss : 3.7531861510034207e-07 Score : 1.0\n",
      "- Val Loss : 3.133558834633732e-07 Score : 1.0\n",
      "[140/1000]\n",
      "- Train Loss : 3.720334144456198e-07 Score : 1.0\n",
      "- Val Loss : 3.123031149243616e-07 Score : 1.0\n",
      "[141/1000]\n",
      "- Train Loss : 3.6874448087568606e-07 Score : 1.0\n",
      "- Val Loss : 3.069903868890833e-07 Score : 1.0\n",
      "[142/1000]\n",
      "- Train Loss : 3.681018997762446e-07 Score : 1.0\n",
      "- Val Loss : 3.059352877698984e-07 Score : 1.0\n",
      "[143/1000]\n",
      "- Train Loss : 3.6613692660441506e-07 Score : 1.0\n",
      "- Val Loss : 3.0489590585602855e-07 Score : 1.0\n",
      "[144/1000]\n",
      "- Train Loss : 3.6550559556758344e-07 Score : 1.0\n",
      "- Val Loss : 3.038676652522554e-07 Score : 1.0\n",
      "[145/1000]\n",
      "- Train Loss : 3.6355862675918615e-07 Score : 1.0\n",
      "- Val Loss : 3.0286028618320415e-07 Score : 1.0\n",
      "[146/1000]\n",
      "- Train Loss : 3.6294708473229144e-07 Score : 1.0\n",
      "- Val Loss : 3.0186154731381976e-07 Score : 1.0\n",
      "[147/1000]\n",
      "- Train Loss : 3.610195644654368e-07 Score : 1.0\n",
      "- Val Loss : 3.008835278706101e-07 Score : 1.0\n",
      "[148/1000]\n",
      "- Train Loss : 3.6042631753237703e-07 Score : 1.0\n",
      "- Val Loss : 2.999146033744182e-07 Score : 1.0\n",
      "[149/1000]\n",
      "- Train Loss : 3.5454292242794355e-07 Score : 1.0\n",
      "- Val Loss : 2.94686799406918e-07 Score : 1.0\n",
      "[150/1000]\n",
      "- Train Loss : 3.539532558407801e-07 Score : 1.0\n",
      "- Val Loss : 2.894467741043627e-07 Score : 1.0\n",
      "[151/1000]\n",
      "- Train Loss : 3.520341480051709e-07 Score : 1.0\n",
      "- Val Loss : 2.8847452426816744e-07 Score : 1.0\n",
      "[152/1000]\n",
      "- Train Loss : 3.5144432194063785e-07 Score : 1.0\n",
      "- Val Loss : 2.8750673664035276e-07 Score : 1.0\n",
      "[153/1000]\n",
      "- Train Loss : 3.482099469999235e-07 Score : 1.0\n",
      "- Val Loss : 2.865470207780163e-07 Score : 1.0\n",
      "[154/1000]\n",
      "- Train Loss : 3.4762770479485676e-07 Score : 1.0\n",
      "- Val Loss : 2.8559102815961523e-07 Score : 1.0\n",
      "[155/1000]\n",
      "- Train Loss : 3.444010125974728e-07 Score : 1.0\n",
      "- Val Loss : 2.8464279466788867e-07 Score : 1.0\n",
      "[156/1000]\n",
      "- Train Loss : 3.4382599108558526e-07 Score : 1.0\n",
      "- Val Loss : 2.836956980445393e-07 Score : 1.0\n",
      "[157/1000]\n",
      "- Train Loss : 3.419299737177855e-07 Score : 1.0\n",
      "- Val Loss : 2.8276318175812776e-07 Score : 1.0\n",
      "[158/1000]\n",
      "- Train Loss : 3.4004098164391257e-07 Score : 1.0\n",
      "- Val Loss : 2.818334792209498e-07 Score : 1.0\n",
      "[159/1000]\n",
      "- Train Loss : 3.394795780783018e-07 Score : 1.0\n",
      "- Val Loss : 2.809233592415694e-07 Score : 1.0\n",
      "[160/1000]\n",
      "- Train Loss : 3.3760748113170143e-07 Score : 1.0\n",
      "- Val Loss : 2.8003577767776733e-07 Score : 1.0\n",
      "[161/1000]\n",
      "- Train Loss : 3.3442127506728207e-07 Score : 1.0\n",
      "- Val Loss : 2.7486325393510924e-07 Score : 1.0\n",
      "[162/1000]\n",
      "- Train Loss : 3.312184498030193e-07 Score : 1.0\n",
      "- Val Loss : 2.739347735314368e-07 Score : 1.0\n",
      "[163/1000]\n",
      "- Train Loss : 3.2932898363924323e-07 Score : 1.0\n",
      "- Val Loss : 2.7298497684569156e-07 Score : 1.0\n",
      "[164/1000]\n",
      "- Train Loss : 3.274296239573636e-07 Score : 1.0\n",
      "- Val Loss : 2.720404097544815e-07 Score : 1.0\n",
      "[165/1000]\n",
      "- Train Loss : 3.268568609845109e-07 Score : 1.0\n",
      "- Val Loss : 2.7109814482173533e-07 Score : 1.0\n",
      "[166/1000]\n",
      "- Train Loss : 3.2496180541525314e-07 Score : 1.0\n",
      "- Val Loss : 2.701726486975531e-07 Score : 1.0\n",
      "[167/1000]\n",
      "- Train Loss : 3.217534262726455e-07 Score : 1.0\n",
      "- Val Loss : 2.6925613383355085e-07 Score : 1.0\n",
      "[168/1000]\n",
      "- Train Loss : 3.211985263441698e-07 Score : 1.0\n",
      "- Val Loss : 2.6834396749109146e-07 Score : 1.0\n",
      "[169/1000]\n",
      "- Train Loss : 3.193232303762746e-07 Score : 1.0\n",
      "- Val Loss : 2.674453867257398e-07 Score : 1.0\n",
      "[170/1000]\n",
      "- Train Loss : 3.187800725612533e-07 Score : 1.0\n",
      "- Val Loss : 2.622948898078903e-07 Score : 1.0\n",
      "[171/1000]\n",
      "- Train Loss : 3.1691728550084815e-07 Score : 1.0\n",
      "- Val Loss : 2.6141643161281536e-07 Score : 1.0\n",
      "[172/1000]\n",
      "- Train Loss : 3.1638613625280715e-07 Score : 1.0\n",
      "- Val Loss : 2.605414977097098e-07 Score : 1.0\n",
      "[173/1000]\n",
      "- Train Loss : 3.158595516497245e-07 Score : 1.0\n",
      "- Val Loss : 2.5969274020098965e-07 Score : 1.0\n",
      "[174/1000]\n",
      "- Train Loss : 3.140257113108823e-07 Score : 1.0\n",
      "- Val Loss : 2.5886581056511204e-07 Score : 1.0\n",
      "[175/1000]\n",
      "- Train Loss : 3.135274246643702e-07 Score : 1.0\n",
      "- Val Loss : 2.5804430947573564e-07 Score : 1.0\n",
      "[176/1000]\n",
      "- Train Loss : 3.103851514573029e-07 Score : 1.0\n",
      "- Val Loss : 2.572306243564526e-07 Score : 1.0\n",
      "[177/1000]\n",
      "- Train Loss : 3.098940422792869e-07 Score : 1.0\n",
      "- Val Loss : 2.56410629617676e-07 Score : 1.0\n",
      "[178/1000]\n",
      "- Train Loss : 3.0807740220634764e-07 Score : 1.0\n",
      "- Val Loss : 2.5559958771737e-07 Score : 1.0\n",
      "[179/1000]\n",
      "- Train Loss : 3.0758874657187283e-07 Score : 1.0\n",
      "- Val Loss : 2.547877215874905e-07 Score : 1.0\n",
      "[180/1000]\n",
      "- Train Loss : 3.0710146958767837e-07 Score : 1.0\n",
      "- Val Loss : 2.5399774017387244e-07 Score : 1.0\n",
      "[181/1000]\n",
      "- Train Loss : 3.0530457273414413e-07 Score : 1.0\n",
      "- Val Loss : 2.5322700025753875e-07 Score : 1.0\n",
      "[182/1000]\n",
      "- Train Loss : 3.0351688648819516e-07 Score : 1.0\n",
      "- Val Loss : 2.524522813018848e-07 Score : 1.0\n",
      "[183/1000]\n",
      "- Train Loss : 3.017280404786031e-07 Score : 1.0\n",
      "- Val Loss : 2.516819392894831e-07 Score : 1.0\n",
      "[184/1000]\n",
      "- Train Loss : 2.9861417615172235e-07 Score : 1.0\n",
      "- Val Loss : 2.508716363536223e-07 Score : 1.0\n",
      "[185/1000]\n",
      "- Train Loss : 2.968003975922784e-07 Score : 1.0\n",
      "- Val Loss : 2.500447067177447e-07 Score : 1.0\n",
      "[186/1000]\n",
      "- Train Loss : 2.9629992701371873e-07 Score : 1.0\n",
      "- Val Loss : 2.49207801061857e-07 Score : 1.0\n",
      "[187/1000]\n",
      "- Train Loss : 2.9447092132607446e-07 Score : 1.0\n",
      "- Val Loss : 2.4412389620920294e-07 Score : 1.0\n",
      "[188/1000]\n",
      "- Train Loss : 2.9132473184429523e-07 Score : 1.0\n",
      "- Val Loss : 2.433036456750415e-07 Score : 1.0\n",
      "[189/1000]\n",
      "- Train Loss : 2.908289338737152e-07 Score : 1.0\n",
      "- Val Loss : 2.4247967189694464e-07 Score : 1.0\n",
      "[190/1000]\n",
      "- Train Loss : 2.8900839114479976e-07 Score : 1.0\n",
      "- Val Loss : 2.4166891421373293e-07 Score : 1.0\n",
      "[191/1000]\n",
      "- Train Loss : 2.8719720744795217e-07 Score : 1.0\n",
      "- Val Loss : 2.4087231054181757e-07 Score : 1.0\n",
      "[192/1000]\n",
      "- Train Loss : 2.867169107566446e-07 Score : 1.0\n",
      "- Val Loss : 2.4007707111195487e-07 Score : 1.0\n",
      "[193/1000]\n",
      "- Train Loss : 2.8491495839154544e-07 Score : 1.0\n",
      "- Val Loss : 2.3929257508825685e-07 Score : 1.0\n",
      "[194/1000]\n",
      "- Train Loss : 2.844419907511843e-07 Score : 1.0\n",
      "- Val Loss : 2.3850856223361916e-07 Score : 1.0\n",
      "[195/1000]\n",
      "- Train Loss : 2.839713763888893e-07 Score : 1.0\n",
      "- Val Loss : 2.377442029910526e-07 Score : 1.0\n",
      "[196/1000]\n",
      "- Train Loss : 2.821902435180669e-07 Score : 1.0\n",
      "- Val Loss : 2.369985168115818e-07 Score : 1.0\n",
      "[197/1000]\n",
      "- Train Loss : 2.804173922202027e-07 Score : 1.0\n",
      "- Val Loss : 2.3625069900390372e-07 Score : 1.0\n",
      "[198/1000]\n",
      "- Train Loss : 2.7864430821959257e-07 Score : 1.0\n",
      "- Val Loss : 2.3550251171400305e-07 Score : 1.0\n",
      "[199/1000]\n",
      "- Train Loss : 2.7819428143832033e-07 Score : 1.0\n",
      "- Val Loss : 2.3049308595091134e-07 Score : 1.0\n",
      "[200/1000]\n",
      "- Train Loss : 2.7774357707560034e-07 Score : 1.0\n",
      "- Val Loss : 2.2975928004598245e-07 Score : 1.0\n",
      "[201/1000]\n",
      "- Train Loss : 2.759807329826774e-07 Score : 1.0\n",
      "- Val Loss : 2.2904191609995905e-07 Score : 1.0\n",
      "[202/1000]\n",
      "- Train Loss : 2.7422428673497103e-07 Score : 1.0\n",
      "- Val Loss : 2.2405511401757394e-07 Score : 1.0\n",
      "[203/1000]\n",
      "- Train Loss : 2.724629416898476e-07 Score : 1.0\n",
      "- Val Loss : 2.2333792060180713e-07 Score : 1.0\n",
      "[204/1000]\n",
      "- Train Loss : 2.693850373387047e-07 Score : 1.0\n",
      "- Val Loss : 2.2261777132825955e-07 Score : 1.0\n",
      "[205/1000]\n",
      "- Train Loss : 2.6895183956475597e-07 Score : 1.0\n",
      "- Val Loss : 2.2188896764419042e-07 Score : 1.0\n",
      "[206/1000]\n",
      "- Train Loss : 2.658661044405461e-07 Score : 1.0\n",
      "- Val Loss : 2.2115973763447982e-07 Score : 1.0\n",
      "[207/1000]\n",
      "- Train Loss : 2.6542645750440545e-07 Score : 1.0\n",
      "- Val Loss : 2.2041895419988577e-07 Score : 1.0\n",
      "[208/1000]\n",
      "- Train Loss : 2.649826943547219e-07 Score : 1.0\n",
      "- Val Loss : 2.196953374777877e-07 Score : 1.0\n",
      "[209/1000]\n",
      "- Train Loss : 2.6322562973746244e-07 Score : 1.0\n",
      "- Val Loss : 2.1898561897160107e-07 Score : 1.0\n",
      "[210/1000]\n",
      "- Train Loss : 2.6147377951701856e-07 Score : 1.0\n",
      "- Val Loss : 2.182471519063256e-07 Score : 1.0\n",
      "[211/1000]\n",
      "- Train Loss : 2.6102901864691023e-07 Score : 1.0\n",
      "- Val Loss : 2.1324994747828896e-07 Score : 1.0\n",
      "[212/1000]\n",
      "- Train Loss : 2.5926122601857867e-07 Score : 1.0\n",
      "- Val Loss : 2.125189695334484e-07 Score : 1.0\n",
      "[213/1000]\n",
      "- Train Loss : 2.5882133966901063e-07 Score : 1.0\n",
      "- Val Loss : 2.117856183758704e-07 Score : 1.0\n",
      "[214/1000]\n",
      "- Train Loss : 2.58381924646124e-07 Score : 1.0\n",
      "- Val Loss : 2.110723755777144e-07 Score : 1.0\n",
      "[215/1000]\n",
      "- Train Loss : 2.5663136228438867e-07 Score : 1.0\n",
      "- Val Loss : 2.0611561524219724e-07 Score : 1.0\n",
      "[216/1000]\n",
      "- Train Loss : 2.5621156317533354e-07 Score : 1.0\n",
      "- Val Loss : 2.0541641276849987e-07 Score : 1.0\n",
      "[217/1000]\n",
      "- Train Loss : 2.5579361739858086e-07 Score : 1.0\n",
      "- Val Loss : 2.047364802137963e-07 Score : 1.0\n",
      "[218/1000]\n",
      "- Train Loss : 2.540639809151596e-07 Score : 1.0\n",
      "- Val Loss : 2.0407040324244008e-07 Score : 1.0\n",
      "[219/1000]\n",
      "- Train Loss : 2.536648876135435e-07 Score : 1.0\n",
      "- Val Loss : 2.034027062336463e-07 Score : 1.0\n",
      "[220/1000]\n",
      "- Train Loss : 2.5326638544400896e-07 Score : 1.0\n",
      "- Val Loss : 2.0275233225675038e-07 Score : 1.0\n",
      "[221/1000]\n",
      "- Train Loss : 2.515553160343921e-07 Score : 1.0\n",
      "- Val Loss : 2.0211412277149066e-07 Score : 1.0\n",
      "[222/1000]\n",
      "- Train Loss : 2.5117368353101534e-07 Score : 1.0\n",
      "- Val Loss : 2.0147304269357846e-07 Score : 1.0\n",
      "[223/1000]\n",
      "- Train Loss : 2.4946762364505855e-07 Score : 1.0\n",
      "- Val Loss : 2.0084397078790062e-07 Score : 1.0\n",
      "[224/1000]\n",
      "- Train Loss : 2.464440935767698e-07 Score : 1.0\n",
      "- Val Loss : 2.002107208909365e-07 Score : 1.0\n",
      "[225/1000]\n",
      "- Train Loss : 2.4606484672062834e-07 Score : 1.0\n",
      "- Val Loss : 1.9956492280925886e-07 Score : 1.0\n",
      "[226/1000]\n",
      "- Train Loss : 2.4435503852436744e-07 Score : 1.0\n",
      "- Val Loss : 1.989207021324546e-07 Score : 1.0\n",
      "[227/1000]\n",
      "- Train Loss : 2.4396868012230596e-07 Score : 1.0\n",
      "- Val Loss : 1.982685802204287e-07 Score : 1.0\n",
      "[228/1000]\n",
      "- Train Loss : 2.422543653087435e-07 Score : 1.0\n",
      "- Val Loss : 1.976216026378097e-07 Score : 1.0\n",
      "[229/1000]\n",
      "- Train Loss : 2.392196467582153e-07 Score : 1.0\n",
      "- Val Loss : 1.9697573350185849e-07 Score : 1.0\n",
      "[230/1000]\n",
      "- Train Loss : 2.3883188542899665e-07 Score : 1.0\n",
      "- Val Loss : 1.9631794145880122e-07 Score : 1.0\n",
      "[231/1000]\n",
      "- Train Loss : 2.384390391796387e-07 Score : 1.0\n",
      "- Val Loss : 1.956738486796894e-07 Score : 1.0\n",
      "[232/1000]\n",
      "- Train Loss : 2.367306858196293e-07 Score : 1.0\n",
      "- Val Loss : 1.9504615522691893e-07 Score : 1.0\n",
      "[233/1000]\n",
      "- Train Loss : 2.3503307369468177e-07 Score : 1.0\n",
      "- Val Loss : 1.944261782682588e-07 Score : 1.0\n",
      "[234/1000]\n",
      "- Train Loss : 2.3466205275231763e-07 Score : 1.0\n",
      "- Val Loss : 1.8954334279897012e-07 Score : 1.0\n",
      "[235/1000]\n",
      "- Train Loss : 2.3428966618628996e-07 Score : 1.0\n",
      "- Val Loss : 1.8893248920903716e-07 Score : 1.0\n",
      "[236/1000]\n",
      "- Train Loss : 2.326024348933389e-07 Score : 1.0\n",
      "- Val Loss : 1.883329474594575e-07 Score : 1.0\n",
      "[237/1000]\n",
      "- Train Loss : 2.3224428740636287e-07 Score : 1.0\n",
      "- Val Loss : 1.8772740872918803e-07 Score : 1.0\n",
      "[238/1000]\n",
      "- Train Loss : 2.3188409493791875e-07 Score : 1.0\n",
      "- Val Loss : 1.871360240102149e-07 Score : 1.0\n",
      "[239/1000]\n",
      "- Train Loss : 2.3020903839999897e-07 Score : 1.0\n",
      "- Val Loss : 1.865544021484311e-07 Score : 1.0\n",
      "[240/1000]\n",
      "- Train Loss : 2.2853637777270958e-07 Score : 1.0\n",
      "- Val Loss : 1.8593756578866305e-07 Score : 1.0\n",
      "[241/1000]\n",
      "- Train Loss : 2.2684261126294638e-07 Score : 1.0\n",
      "- Val Loss : 1.8531066814375663e-07 Score : 1.0\n",
      "[242/1000]\n",
      "- Train Loss : 2.2381787504836362e-07 Score : 1.0\n",
      "- Val Loss : 1.8466849383003137e-07 Score : 1.0\n",
      "[243/1000]\n",
      "- Train Loss : 2.2343152769918913e-07 Score : 1.0\n",
      "- Val Loss : 1.8401253498723236e-07 Score : 1.0\n",
      "[244/1000]\n",
      "- Train Loss : 2.2303911803886774e-07 Score : 1.0\n",
      "- Val Loss : 1.8336993434786564e-07 Score : 1.0\n",
      "[245/1000]\n",
      "- Train Loss : 2.2133130429659155e-07 Score : 1.0\n",
      "- Val Loss : 1.827376934215863e-07 Score : 1.0\n",
      "[246/1000]\n",
      "- Train Loss : 2.1962751242357274e-07 Score : 1.0\n",
      "- Val Loss : 1.8209756547094003e-07 Score : 1.0\n",
      "[247/1000]\n",
      "- Train Loss : 2.192441043528485e-07 Score : 1.0\n",
      "- Val Loss : 1.8146488400816452e-07 Score : 1.0\n",
      "[248/1000]\n",
      "- Train Loss : 2.1886667214390502e-07 Score : 1.0\n",
      "- Val Loss : 1.8085314934523922e-07 Score : 1.0\n",
      "[249/1000]\n",
      "- Train Loss : 2.1717862668740256e-07 Score : 1.0\n",
      "- Val Loss : 1.8025529868737067e-07 Score : 1.0\n",
      "[250/1000]\n",
      "- Train Loss : 2.1682095921151915e-07 Score : 1.0\n",
      "- Val Loss : 1.796523889652235e-07 Score : 1.0\n",
      "[251/1000]\n",
      "- Train Loss : 2.1646172518627634e-07 Score : 1.0\n",
      "- Val Loss : 1.7906400273659528e-07 Score : 1.0\n",
      "[252/1000]\n",
      "- Train Loss : 2.1213851810999688e-07 Score : 1.0\n",
      "- Val Loss : 1.7847307276497304e-07 Score : 1.0\n",
      "[253/1000]\n",
      "- Train Loss : 2.1045893383586645e-07 Score : 1.0\n",
      "- Val Loss : 1.7785919226298574e-07 Score : 1.0\n",
      "[254/1000]\n",
      "- Train Loss : 2.0876630320761555e-07 Score : 1.0\n",
      "- Val Loss : 1.7297979582053813e-07 Score : 1.0\n",
      "[255/1000]\n",
      "- Train Loss : 2.0706948175883451e-07 Score : 1.0\n",
      "- Val Loss : 1.7235973359674972e-07 Score : 1.0\n",
      "[256/1000]\n",
      "- Train Loss : 2.0405022224976062e-07 Score : 1.0\n",
      "- Val Loss : 1.7173803712466906e-07 Score : 1.0\n",
      "[257/1000]\n",
      "- Train Loss : 2.0367587359891735e-07 Score : 1.0\n",
      "- Val Loss : 1.7110330929881457e-07 Score : 1.0\n",
      "[258/1000]\n",
      "- Train Loss : 2.0329563533828656e-07 Score : 1.0\n",
      "- Val Loss : 1.7048070333203214e-07 Score : 1.0\n",
      "[259/1000]\n",
      "- Train Loss : 2.0292433452103373e-07 Score : 1.0\n",
      "- Val Loss : 1.698809057870676e-07 Score : 1.0\n",
      "[260/1000]\n",
      "- Train Loss : 2.0124278875420663e-07 Score : 1.0\n",
      "- Val Loss : 1.6929446644553536e-07 Score : 1.0\n",
      "[261/1000]\n",
      "- Train Loss : 2.0089151872879381e-07 Score : 1.0\n",
      "- Val Loss : 1.6870311014827166e-07 Score : 1.0\n",
      "[262/1000]\n",
      "- Train Loss : 1.978899846951284e-07 Score : 1.0\n",
      "- Val Loss : 1.6811743819289404e-07 Score : 1.0\n",
      "[263/1000]\n",
      "- Train Loss : 1.9754014384747936e-07 Score : 1.0\n",
      "- Val Loss : 1.675375500553855e-07 Score : 1.0\n",
      "[264/1000]\n",
      "- Train Loss : 1.9587044578865212e-07 Score : 1.0\n",
      "- Val Loss : 1.6696438365215727e-07 Score : 1.0\n",
      "[265/1000]\n",
      "- Train Loss : 1.95527176277071e-07 Score : 1.0\n",
      "- Val Loss : 1.6638365707422054e-07 Score : 1.0\n",
      "[266/1000]\n",
      "- Train Loss : 1.9518080011475072e-07 Score : 1.0\n",
      "- Val Loss : 1.658152513073219e-07 Score : 1.0\n",
      "[267/1000]\n",
      "- Train Loss : 1.9484296112340064e-07 Score : 1.0\n",
      "- Val Loss : 1.6526777812941873e-07 Score : 1.0\n",
      "[268/1000]\n",
      "- Train Loss : 1.9319410624360796e-07 Score : 1.0\n",
      "- Val Loss : 1.64731162044518e-07 Score : 1.0\n",
      "[269/1000]\n",
      "- Train Loss : 1.9287403220226275e-07 Score : 1.0\n",
      "- Val Loss : 1.6418904635884246e-07 Score : 1.0\n",
      "[270/1000]\n",
      "- Train Loss : 1.9255155692121662e-07 Score : 1.0\n",
      "- Val Loss : 1.6365812882668251e-07 Score : 1.0\n",
      "[271/1000]\n",
      "- Train Loss : 1.922372674872324e-07 Score : 1.0\n",
      "- Val Loss : 1.6314582751419948e-07 Score : 1.0\n",
      "[272/1000]\n",
      "- Train Loss : 1.9061016814308434e-07 Score : 1.0\n",
      "- Val Loss : 1.6264370117369253e-07 Score : 1.0\n",
      "[273/1000]\n",
      "- Train Loss : 1.9031146110866834e-07 Score : 1.0\n",
      "- Val Loss : 1.6213304832035647e-07 Score : 1.0\n",
      "[274/1000]\n",
      "- Train Loss : 1.900089513876916e-07 Score : 1.0\n",
      "- Val Loss : 1.6163303939720208e-07 Score : 1.0\n",
      "[275/1000]\n",
      "- Train Loss : 1.88389169449484e-07 Score : 1.0\n",
      "- Val Loss : 1.6113899903302809e-07 Score : 1.0\n",
      "[276/1000]\n",
      "- Train Loss : 1.8677017808875638e-07 Score : 1.0\n",
      "- Val Loss : 1.606263566600319e-07 Score : 1.0\n",
      "[277/1000]\n",
      "- Train Loss : 1.864656429262368e-07 Score : 1.0\n",
      "- Val Loss : 1.601176364829371e-07 Score : 1.0\n",
      "[278/1000]\n",
      "- Train Loss : 1.848390484621771e-07 Score : 1.0\n",
      "- Val Loss : 1.5533852604221465e-07 Score : 1.0\n",
      "[279/1000]\n",
      "- Train Loss : 1.8187819714496652e-07 Score : 1.0\n",
      "- Val Loss : 1.547946055779903e-07 Score : 1.0\n",
      "[280/1000]\n",
      "- Train Loss : 1.8155175733151432e-07 Score : 1.0\n",
      "- Val Loss : 1.542320404723796e-07 Score : 1.0\n",
      "[281/1000]\n",
      "- Train Loss : 1.8121569864446326e-07 Score : 1.0\n",
      "- Val Loss : 1.5367773187335843e-07 Score : 1.0\n",
      "[282/1000]\n",
      "- Train Loss : 1.8088594326097032e-07 Score : 1.0\n",
      "- Val Loss : 1.531418831746123e-07 Score : 1.0\n",
      "[283/1000]\n",
      "- Train Loss : 1.805679608798035e-07 Score : 1.0\n",
      "- Val Loss : 1.5262762076417857e-07 Score : 1.0\n",
      "[284/1000]\n",
      "- Train Loss : 1.7893910521217373e-07 Score : 1.0\n",
      "- Val Loss : 1.5212393122965295e-07 Score : 1.0\n",
      "[285/1000]\n",
      "- Train Loss : 1.786387757718444e-07 Score : 1.0\n",
      "- Val Loss : 1.5151864829476835e-07 Score : 1.0\n",
      "[286/1000]\n",
      "- Train Loss : 1.782539062528738e-07 Score : 1.0\n",
      "- Val Loss : 1.507771969500027e-07 Score : 1.0\n",
      "[287/1000]\n",
      "- Train Loss : 1.7781995767210094e-07 Score : 1.0\n",
      "- Val Loss : 1.5021187493857724e-07 Score : 1.0\n",
      "[288/1000]\n",
      "- Train Loss : 1.761643150861087e-07 Score : 1.0\n",
      "- Val Loss : 1.4972329154261388e-07 Score : 1.0\n",
      "[289/1000]\n",
      "- Train Loss : 1.7455266074526162e-07 Score : 1.0\n",
      "- Val Loss : 1.4926112612556608e-07 Score : 1.0\n",
      "[290/1000]\n",
      "- Train Loss : 1.7427870776150675e-07 Score : 1.0\n",
      "- Val Loss : 1.487053111759451e-07 Score : 1.0\n",
      "[291/1000]\n",
      "- Train Loss : 1.7392542398890024e-07 Score : 1.0\n",
      "- Val Loss : 1.480171079037973e-07 Score : 1.0\n",
      "[292/1000]\n",
      "- Train Loss : 1.735243471381567e-07 Score : 1.0\n",
      "- Val Loss : 1.475003585937884e-07 Score : 1.0\n",
      "[293/1000]\n",
      "- Train Loss : 1.7189881167883274e-07 Score : 1.0\n",
      "- Val Loss : 1.4705187822983135e-07 Score : 1.0\n",
      "[294/1000]\n",
      "- Train Loss : 1.7163452222431394e-07 Score : 1.0\n",
      "- Val Loss : 1.4652680135895935e-07 Score : 1.0\n",
      "[295/1000]\n",
      "- Train Loss : 1.6997611515831875e-07 Score : 1.0\n",
      "- Val Loss : 1.4586768770641356e-07 Score : 1.0\n",
      "[296/1000]\n",
      "- Train Loss : 1.6959270089501723e-07 Score : 1.0\n",
      "- Val Loss : 1.4537373260736786e-07 Score : 1.0\n",
      "[297/1000]\n",
      "- Train Loss : 1.6930539044797874e-07 Score : 1.0\n",
      "- Val Loss : 1.449566582323314e-07 Score : 1.0\n",
      "[298/1000]\n",
      "- Train Loss : 1.6773860907666935e-07 Score : 1.0\n",
      "- Val Loss : 1.4031097350653e-07 Score : 1.0\n",
      "[299/1000]\n",
      "- Train Loss : 1.6618604319834023e-07 Score : 1.0\n",
      "- Val Loss : 1.3982567281800584e-07 Score : 1.0\n",
      "[300/1000]\n",
      "- Train Loss : 1.6587719439995237e-07 Score : 1.0\n",
      "- Val Loss : 1.3920546848567028e-07 Score : 1.0\n",
      "[301/1000]\n",
      "- Train Loss : 1.6551757490527544e-07 Score : 1.0\n",
      "- Val Loss : 1.3874412729819596e-07 Score : 1.0\n",
      "[302/1000]\n",
      "- Train Loss : 1.6525015287014196e-07 Score : 1.0\n",
      "- Val Loss : 1.3826922895532334e-07 Score : 1.0\n",
      "[303/1000]\n",
      "- Train Loss : 1.6362660456200237e-07 Score : 1.0\n",
      "- Val Loss : 1.3342726390419557e-07 Score : 1.0\n",
      "[304/1000]\n",
      "- Train Loss : 1.6328754321510215e-07 Score : 1.0\n",
      "- Val Loss : 1.3298422629759443e-07 Score : 1.0\n",
      "[305/1000]\n",
      "- Train Loss : 1.630308952036172e-07 Score : 1.0\n",
      "- Val Loss : 1.3260714126772655e-07 Score : 1.0\n",
      "[306/1000]\n",
      "- Train Loss : 1.6281310139593725e-07 Score : 1.0\n",
      "- Val Loss : 1.3218041772233846e-07 Score : 1.0\n",
      "[307/1000]\n",
      "- Train Loss : 1.625433425387099e-07 Score : 1.0\n",
      "- Val Loss : 1.3164272161247936e-07 Score : 1.0\n",
      "[308/1000]\n",
      "- Train Loss : 1.609106913886333e-07 Score : 1.0\n",
      "- Val Loss : 1.31253884205762e-07 Score : 1.0\n",
      "[309/1000]\n",
      "- Train Loss : 1.6068594487094066e-07 Score : 1.0\n",
      "- Val Loss : 1.3082363636840455e-07 Score : 1.0\n",
      "[310/1000]\n",
      "- Train Loss : 1.6041451520207613e-07 Score : 1.0\n",
      "- Val Loss : 1.3028848400153947e-07 Score : 1.0\n",
      "[311/1000]\n",
      "- Train Loss : 1.6010759903058633e-07 Score : 1.0\n",
      "- Val Loss : 1.2991165476705646e-07 Score : 1.0\n",
      "[312/1000]\n",
      "- Train Loss : 1.5856826123755101e-07 Score : 1.0\n",
      "- Val Loss : 1.2951240080383286e-07 Score : 1.0\n",
      "[313/1000]\n",
      "- Train Loss : 1.5831486854800403e-07 Score : 1.0\n",
      "- Val Loss : 1.2899154455681128e-07 Score : 1.0\n",
      "[314/1000]\n",
      "- Train Loss : 1.5801614582243593e-07 Score : 1.0\n",
      "- Val Loss : 1.2861781328865618e-07 Score : 1.0\n",
      "[315/1000]\n",
      "- Train Loss : 1.578027233575499e-07 Score : 1.0\n",
      "- Val Loss : 1.2831073092911538e-07 Score : 1.0\n",
      "[316/1000]\n",
      "- Train Loss : 1.5762828262408687e-07 Score : 1.0\n",
      "- Val Loss : 1.2369861224215128e-07 Score : 1.0\n",
      "[317/1000]\n",
      "- Train Loss : 1.56079516126321e-07 Score : 1.0\n",
      "- Val Loss : 1.2322433917688613e-07 Score : 1.0\n",
      "[318/1000]\n",
      "- Train Loss : 1.5580740813586007e-07 Score : 1.0\n",
      "- Val Loss : 1.227916897050818e-07 Score : 1.0\n",
      "[319/1000]\n",
      "- Train Loss : 1.5553775171521048e-07 Score : 1.0\n",
      "- Val Loss : 1.2229031653987477e-07 Score : 1.0\n",
      "[320/1000]\n",
      "- Train Loss : 1.5525238398576371e-07 Score : 1.0\n",
      "- Val Loss : 1.2195229714961897e-07 Score : 1.0\n",
      "[321/1000]\n",
      "- Train Loss : 1.5506153390928054e-07 Score : 1.0\n",
      "- Val Loss : 1.2168617047336738e-07 Score : 1.0\n",
      "[322/1000]\n",
      "- Train Loss : 1.5358812283711782e-07 Score : 1.0\n",
      "- Val Loss : 1.2136486304825667e-07 Score : 1.0\n",
      "[323/1000]\n",
      "- Train Loss : 1.5073381274019778e-07 Score : 1.0\n",
      "- Val Loss : 1.2090370660189365e-07 Score : 1.0\n",
      "[324/1000]\n",
      "- Train Loss : 1.491445198453789e-07 Score : 1.0\n",
      "- Val Loss : 1.2047387087932293e-07 Score : 1.0\n",
      "[325/1000]\n",
      "- Train Loss : 1.4755068954307325e-07 Score : 1.0\n",
      "- Val Loss : 1.19943351251095e-07 Score : 1.0\n",
      "[326/1000]\n",
      "- Train Loss : 1.47244441676501e-07 Score : 1.0\n",
      "- Val Loss : 1.1955178536027233e-07 Score : 1.0\n",
      "[327/1000]\n",
      "- Train Loss : 1.4569450648396457e-07 Score : 1.0\n",
      "- Val Loss : 1.1921322595753736e-07 Score : 1.0\n",
      "[328/1000]\n",
      "- Train Loss : 1.4549680060760295e-07 Score : 1.0\n",
      "- Val Loss : 1.1880188566237848e-07 Score : 1.0\n",
      "[329/1000]\n",
      "- Train Loss : 1.4523711939279988e-07 Score : 1.0\n",
      "- Val Loss : 1.1828178969608416e-07 Score : 1.0\n",
      "[330/1000]\n",
      "- Train Loss : 1.4493759244634438e-07 Score : 1.0\n",
      "- Val Loss : 1.1790251619459013e-07 Score : 1.0\n",
      "[331/1000]\n",
      "- Train Loss : 1.4471949031739464e-07 Score : 1.0\n",
      "- Val Loss : 1.1758671547568156e-07 Score : 1.0\n",
      "[332/1000]\n",
      "- Train Loss : 1.3990280988634356e-07 Score : 1.0\n",
      "- Val Loss : 1.1721373738282637e-07 Score : 1.0\n",
      "[333/1000]\n",
      "- Train Loss : 1.3966352307067295e-07 Score : 1.0\n",
      "- Val Loss : 1.1669096977584559e-07 Score : 1.0\n",
      "[334/1000]\n",
      "- Train Loss : 1.3936029227217755e-07 Score : 1.0\n",
      "- Val Loss : 1.1628836915633656e-07 Score : 1.0\n",
      "[335/1000]\n",
      "- Train Loss : 1.391267684700301e-07 Score : 1.0\n",
      "- Val Loss : 1.1586889314685322e-07 Score : 1.0\n",
      "[336/1000]\n",
      "- Train Loss : 1.3886385364431715e-07 Score : 1.0\n",
      "- Val Loss : 1.1536534572087476e-07 Score : 1.0\n",
      "[337/1000]\n",
      "- Train Loss : 1.385745198528237e-07 Score : 1.0\n",
      "- Val Loss : 1.1500598873226409e-07 Score : 1.0\n",
      "[338/1000]\n",
      "- Train Loss : 1.3704443243340365e-07 Score : 1.0\n",
      "- Val Loss : 1.147005974644344e-07 Score : 1.0\n",
      "[339/1000]\n",
      "- Train Loss : 1.368671223485792e-07 Score : 1.0\n",
      "- Val Loss : 1.1432599933414167e-07 Score : 1.0\n",
      "[340/1000]\n",
      "- Train Loss : 1.36630807239123e-07 Score : 1.0\n",
      "- Val Loss : 1.1384859988083917e-07 Score : 1.0\n",
      "[341/1000]\n",
      "- Train Loss : 1.3635646679986833e-07 Score : 1.0\n",
      "- Val Loss : 1.1350277162591738e-07 Score : 1.0\n",
      "[342/1000]\n",
      "- Train Loss : 1.335096180940212e-07 Score : 1.0\n",
      "- Val Loss : 1.1313489522990494e-07 Score : 1.0\n",
      "[343/1000]\n",
      "- Train Loss : 1.3327821291967781e-07 Score : 1.0\n",
      "- Val Loss : 1.1267172794759972e-07 Score : 1.0\n",
      "[344/1000]\n",
      "- Train Loss : 1.316885977009013e-07 Score : 1.0\n",
      "- Val Loss : 1.1233191798964981e-07 Score : 1.0\n",
      "[345/1000]\n",
      "- Train Loss : 1.3149183901748103e-07 Score : 1.0\n",
      "- Val Loss : 1.1202284611044888e-07 Score : 1.0\n",
      "[346/1000]\n",
      "- Train Loss : 1.2998847810332918e-07 Score : 1.0\n",
      "- Val Loss : 1.1165592894712972e-07 Score : 1.0\n",
      "[347/1000]\n",
      "- Train Loss : 1.2975704327369523e-07 Score : 1.0\n",
      "- Val Loss : 1.1118898868289762e-07 Score : 1.0\n",
      "[348/1000]\n",
      "- Train Loss : 1.2816359649022355e-07 Score : 1.0\n",
      "- Val Loss : 1.108413698602817e-07 Score : 1.0\n",
      "[349/1000]\n",
      "- Train Loss : 1.2663921818608279e-07 Score : 1.0\n",
      "- Val Loss : 1.1046925862956414e-07 Score : 1.0\n",
      "[350/1000]\n",
      "- Train Loss : 1.264039964520168e-07 Score : 1.0\n",
      "- Val Loss : 1.0999065835903821e-07 Score : 1.0\n",
      "[351/1000]\n",
      "- Train Loss : 1.2612798758039458e-07 Score : 1.0\n",
      "- Val Loss : 1.0963426433363566e-07 Score : 1.0\n",
      "[352/1000]\n",
      "- Train Loss : 1.259225493707408e-07 Score : 1.0\n",
      "- Val Loss : 1.0933398186807608e-07 Score : 1.0\n",
      "[353/1000]\n",
      "- Train Loss : 1.2442546746939244e-07 Score : 1.0\n",
      "- Val Loss : 1.0898878599618911e-07 Score : 1.0\n",
      "[354/1000]\n",
      "- Train Loss : 1.2420814217582885e-07 Score : 1.0\n",
      "- Val Loss : 1.0429128849409608e-07 Score : 1.0\n",
      "[355/1000]\n",
      "- Train Loss : 1.2395619327735621e-07 Score : 1.0\n",
      "- Val Loss : 1.0397601357681197e-07 Score : 1.0\n",
      "[356/1000]\n",
      "- Train Loss : 1.224520747562642e-07 Score : 1.0\n",
      "- Val Loss : 1.0363991975737008e-07 Score : 1.0\n",
      "[357/1000]\n",
      "- Train Loss : 1.2223967109833053e-07 Score : 1.0\n",
      "- Val Loss : 1.0320007959307986e-07 Score : 1.0\n",
      "[358/1000]\n",
      "- Train Loss : 1.219869509448844e-07 Score : 1.0\n",
      "- Val Loss : 1.0287723029023255e-07 Score : 1.0\n",
      "[359/1000]\n",
      "- Train Loss : 1.2047738581457938e-07 Score : 1.0\n",
      "- Val Loss : 9.827680713669906e-08 Score : 1.0\n",
      "[360/1000]\n",
      "- Train Loss : 1.1893770302590707e-07 Score : 1.0\n",
      "- Val Loss : 9.784461951767298e-08 Score : 1.0\n",
      "[361/1000]\n",
      "- Train Loss : 1.186899806339249e-07 Score : 1.0\n",
      "- Val Loss : 9.753218677133191e-08 Score : 1.0\n",
      "[362/1000]\n",
      "- Train Loss : 1.1718700650297107e-07 Score : 1.0\n",
      "- Val Loss : 9.726303318302598e-08 Score : 1.0\n",
      "[363/1000]\n",
      "- Train Loss : 1.1703086897413901e-07 Score : 1.0\n",
      "- Val Loss : 9.692849545217541e-08 Score : 1.0\n",
      "[364/1000]\n",
      "- Train Loss : 1.1682021043568636e-07 Score : 1.0\n",
      "- Val Loss : 9.6500116342213e-08 Score : 1.0\n",
      "[365/1000]\n",
      "- Train Loss : 1.1657444366609661e-07 Score : 1.0\n",
      "- Val Loss : 9.618860730142842e-08 Score : 1.0\n",
      "[366/1000]\n",
      "- Train Loss : 1.1507083326632737e-07 Score : 1.0\n",
      "- Val Loss : 9.585601645767383e-08 Score : 1.0\n",
      "[367/1000]\n",
      "- Train Loss : 1.1486289593693528e-07 Score : 1.0\n",
      "- Val Loss : 9.544681489614959e-08 Score : 1.0\n",
      "[368/1000]\n",
      "- Train Loss : 1.146296062700299e-07 Score : 1.0\n",
      "- Val Loss : 9.516043775192884e-08 Score : 1.0\n",
      "[369/1000]\n",
      "- Train Loss : 1.1314266071569021e-07 Score : 1.0\n",
      "- Val Loss : 9.48553733337576e-08 Score : 1.0\n",
      "[370/1000]\n",
      "- Train Loss : 1.1294996324659776e-07 Score : 1.0\n",
      "- Val Loss : 9.44509750411271e-08 Score : 1.0\n",
      "[371/1000]\n",
      "- Train Loss : 1.1271843301025659e-07 Score : 1.0\n",
      "- Val Loss : 9.415592927553007e-08 Score : 1.0\n",
      "[372/1000]\n",
      "- Train Loss : 1.125501209638576e-07 Score : 1.0\n",
      "- Val Loss : 9.384903165710057e-08 Score : 1.0\n",
      "[373/1000]\n",
      "- Train Loss : 1.1235866163861186e-07 Score : 1.0\n",
      "- Val Loss : 9.347091634026583e-08 Score : 1.0\n",
      "[374/1000]\n",
      "- Train Loss : 1.1214414012077838e-07 Score : 1.0\n",
      "- Val Loss : 9.321197325107278e-08 Score : 1.0\n",
      "[375/1000]\n",
      "- Train Loss : 1.106738580148376e-07 Score : 1.0\n",
      "- Val Loss : 9.293439262592074e-08 Score : 1.0\n",
      "[376/1000]\n",
      "- Train Loss : 1.104984692788956e-07 Score : 1.0\n",
      "- Val Loss : 9.25588068412253e-08 Score : 1.0\n",
      "[377/1000]\n",
      "- Train Loss : 1.1028444443937789e-07 Score : 1.0\n",
      "- Val Loss : 8.803072404361956e-08 Score : 1.0\n",
      "[378/1000]\n",
      "- Train Loss : 1.1013084881704592e-07 Score : 1.0\n",
      "- Val Loss : 8.774780724252196e-08 Score : 1.0\n",
      "[379/1000]\n",
      "- Train Loss : 1.0995452994396378e-07 Score : 1.0\n",
      "- Val Loss : 8.739474566255012e-08 Score : 1.0\n",
      "[380/1000]\n",
      "- Train Loss : 1.0975515671616007e-07 Score : 1.0\n",
      "- Val Loss : 8.715618804444603e-08 Score : 1.0\n",
      "[381/1000]\n",
      "- Train Loss : 1.0962206064187481e-07 Score : 1.0\n",
      "- Val Loss : 8.690883390727322e-08 Score : 1.0\n",
      "[382/1000]\n",
      "- Train Loss : 1.0814299604885076e-07 Score : 1.0\n",
      "- Val Loss : 8.657945471668427e-08 Score : 1.0\n",
      "[383/1000]\n",
      "- Train Loss : 1.0795589224417886e-07 Score : 1.0\n",
      "- Val Loss : 8.627709746633627e-08 Score : 1.0\n",
      "[384/1000]\n",
      "- Train Loss : 1.07768832125241e-07 Score : 1.0\n",
      "- Val Loss : 8.166511378249197e-08 Score : 1.0\n",
      "[385/1000]\n",
      "- Train Loss : 1.075687185590911e-07 Score : 1.0\n",
      "- Val Loss : 8.1428446208065e-08 Score : 1.0\n",
      "[386/1000]\n",
      "- Train Loss : 1.0743643773545062e-07 Score : 1.0\n",
      "- Val Loss : 8.118512084820395e-08 Score : 1.0\n",
      "[387/1000]\n",
      "- Train Loss : 1.072850280528195e-07 Score : 1.0\n",
      "- Val Loss : 8.087492631148052e-08 Score : 1.0\n",
      "[388/1000]\n",
      "- Train Loss : 1.0446165702202335e-07 Score : 1.0\n",
      "- Val Loss : 8.063582157546989e-08 Score : 1.0\n",
      "[389/1000]\n",
      "- Train Loss : 1.043234482319318e-07 Score : 1.0\n",
      "- Val Loss : 8.03399657911541e-08 Score : 1.0\n",
      "[390/1000]\n",
      "- Train Loss : 1.0413797756995506e-07 Score : 1.0\n",
      "- Val Loss : 7.996172968205428e-08 Score : 1.0\n",
      "[391/1000]\n",
      "- Train Loss : 1.0392163941885801e-07 Score : 1.0\n",
      "- Val Loss : 7.968650095335761e-08 Score : 1.0\n",
      "[392/1000]\n",
      "- Train Loss : 1.0376469703076028e-07 Score : 1.0\n",
      "- Val Loss : 7.940213464507906e-08 Score : 1.0\n",
      "[393/1000]\n",
      "- Train Loss : 1.0358787592207936e-07 Score : 1.0\n",
      "- Val Loss : 7.905354237891515e-08 Score : 1.0\n",
      "[394/1000]\n",
      "- Train Loss : 1.0339037387032926e-07 Score : 1.0\n",
      "- Val Loss : 7.88139544738442e-08 Score : 1.0\n",
      "[395/1000]\n",
      "- Train Loss : 1.0325561266967389e-07 Score : 1.0\n",
      "- Val Loss : 7.856596795363657e-08 Score : 1.0\n",
      "[396/1000]\n",
      "- Train Loss : 1.0177684501967556e-07 Score : 1.0\n",
      "- Val Loss : 7.824173309245452e-08 Score : 1.0\n",
      "[397/1000]\n",
      "- Train Loss : 1.0159192287571992e-07 Score : 1.0\n",
      "- Val Loss : 7.799641110750599e-08 Score : 1.0\n",
      "[398/1000]\n",
      "- Train Loss : 1.0145256220765268e-07 Score : 1.0\n",
      "- Val Loss : 7.773304844249651e-08 Score : 1.0\n",
      "[399/1000]\n",
      "- Train Loss : 9.996416519090594e-08 Score : 1.0\n",
      "- Val Loss : 7.739848939536387e-08 Score : 1.0\n",
      "[400/1000]\n",
      "- Train Loss : 9.977423717931631e-08 Score : 1.0\n",
      "- Val Loss : 7.71614026007228e-08 Score : 1.0\n",
      "[401/1000]\n",
      "- Train Loss : 9.964046749108866e-08 Score : 1.0\n",
      "- Val Loss : 7.691327397196801e-08 Score : 1.0\n",
      "[402/1000]\n",
      "- Train Loss : 9.948628210286623e-08 Score : 1.0\n",
      "- Val Loss : 7.660156597921741e-08 Score : 1.0\n",
      "[403/1000]\n",
      "- Train Loss : 9.798657600590399e-08 Score : 1.0\n",
      "- Val Loss : 7.638293197942403e-08 Score : 1.0\n",
      "[404/1000]\n",
      "- Train Loss : 9.786183474691512e-08 Score : 1.0\n",
      "- Val Loss : 7.612747765506356e-08 Score : 1.0\n",
      "[405/1000]\n",
      "- Train Loss : 9.770252522455101e-08 Score : 1.0\n",
      "- Val Loss : 7.580119643080252e-08 Score : 1.0\n",
      "[406/1000]\n",
      "- Train Loss : 9.751765823863403e-08 Score : 1.0\n",
      "- Val Loss : 7.557235193189626e-08 Score : 1.0\n",
      "[407/1000]\n",
      "- Train Loss : 9.738882150722716e-08 Score : 1.0\n",
      "- Val Loss : 7.533432011541663e-08 Score : 1.0\n",
      "[408/1000]\n",
      "- Train Loss : 9.724117218081998e-08 Score : 1.0\n",
      "- Val Loss : 7.50355724221663e-08 Score : 1.0\n",
      "[409/1000]\n",
      "- Train Loss : 9.70734418126451e-08 Score : 1.0\n",
      "- Val Loss : 7.478489294499013e-08 Score : 1.0\n",
      "[410/1000]\n",
      "- Train Loss : 9.691975364088821e-08 Score : 1.0\n",
      "- Val Loss : 7.449562389183484e-08 Score : 1.0\n",
      "[411/1000]\n",
      "- Train Loss : 9.543413607827746e-08 Score : 1.0\n",
      "- Val Loss : 7.430396919971827e-08 Score : 1.0\n",
      "[412/1000]\n",
      "- Train Loss : 9.532610718688952e-08 Score : 1.0\n",
      "- Val Loss : 7.407864899278138e-08 Score : 1.0\n",
      "[413/1000]\n",
      "- Train Loss : 9.51858154796259e-08 Score : 1.0\n",
      "- Val Loss : 7.378514510492096e-08 Score : 1.0\n",
      "[414/1000]\n",
      "- Train Loss : 9.50205533500275e-08 Score : 1.0\n",
      "- Val Loss : 7.358507048138563e-08 Score : 1.0\n",
      "[415/1000]\n",
      "- Train Loss : 9.490921949662563e-08 Score : 1.0\n",
      "- Val Loss : 7.337514063010531e-08 Score : 1.0\n",
      "[416/1000]\n",
      "- Train Loss : 9.47791369608332e-08 Score : 1.0\n",
      "- Val Loss : 7.310556071615792e-08 Score : 1.0\n",
      "[417/1000]\n",
      "- Train Loss : 9.462904344913974e-08 Score : 1.0\n",
      "- Val Loss : 7.288149816986333e-08 Score : 1.0\n",
      "[418/1000]\n",
      "- Train Loss : 9.316725322323762e-08 Score : 1.0\n",
      "- Val Loss : 7.26078042134759e-08 Score : 1.0\n",
      "[419/1000]\n",
      "- Train Loss : 9.301274749018429e-08 Score : 1.0\n",
      "- Val Loss : 7.240875277148007e-08 Score : 1.0\n",
      "[420/1000]\n",
      "- Train Loss : 9.290151022387258e-08 Score : 1.0\n",
      "- Val Loss : 7.219534126079452e-08 Score : 1.0\n",
      "[421/1000]\n",
      "- Train Loss : 9.276914914344323e-08 Score : 1.0\n",
      "- Val Loss : 7.192234363628813e-08 Score : 1.0\n",
      "[422/1000]\n",
      "- Train Loss : 9.261653484415877e-08 Score : 1.0\n",
      "- Val Loss : 7.174185157055035e-08 Score : 1.0\n",
      "[423/1000]\n",
      "- Train Loss : 9.25170590215179e-08 Score : 1.0\n",
      "- Val Loss : 7.155254877488915e-08 Score : 1.0\n",
      "[424/1000]\n",
      "- Train Loss : 9.239984337075204e-08 Score : 1.0\n",
      "- Val Loss : 7.130523016485313e-08 Score : 1.0\n",
      "[425/1000]\n",
      "- Train Loss : 9.093862215188277e-08 Score : 1.0\n",
      "- Val Loss : 7.109186128673173e-08 Score : 1.0\n",
      "[426/1000]\n",
      "- Train Loss : 9.080599990536466e-08 Score : 1.0\n",
      "- Val Loss : 7.081486330662301e-08 Score : 1.0\n",
      "[427/1000]\n",
      "- Train Loss : 9.065022128696483e-08 Score : 1.0\n",
      "- Val Loss : 7.06245231185676e-08 Score : 1.0\n",
      "[428/1000]\n",
      "- Train Loss : 9.054434440562851e-08 Score : 1.0\n",
      "- Val Loss : 7.042545036028969e-08 Score : 1.0\n",
      "[429/1000]\n",
      "- Train Loss : 9.04213623451364e-08 Score : 1.0\n",
      "- Val Loss : 7.017108316631493e-08 Score : 1.0\n",
      "[430/1000]\n",
      "- Train Loss : 8.895521682841492e-08 Score : 1.0\n",
      "- Val Loss : 6.995563950340511e-08 Score : 1.0\n",
      "[431/1000]\n",
      "- Train Loss : 8.882295512296912e-08 Score : 1.0\n",
      "- Val Loss : 6.96966395707932e-08 Score : 1.0\n",
      "[432/1000]\n",
      "- Train Loss : 8.735394037361837e-08 Score : 1.0\n",
      "- Val Loss : 6.952316056185737e-08 Score : 1.0\n",
      "[433/1000]\n",
      "- Train Loss : 8.5933728683084e-08 Score : 1.0\n",
      "- Val Loss : 6.93289408104647e-08 Score : 1.0\n",
      "[434/1000]\n",
      "- Train Loss : 8.581126601267839e-08 Score : 1.0\n",
      "- Val Loss : 6.905113281163722e-08 Score : 1.0\n",
      "[435/1000]\n",
      "- Train Loss : 8.565383162693434e-08 Score : 1.0\n",
      "- Val Loss : 6.884865655365502e-08 Score : 1.0\n",
      "[436/1000]\n",
      "- Train Loss : 8.553988758600337e-08 Score : 1.0\n",
      "- Val Loss : 6.863529478096098e-08 Score : 1.0\n",
      "[437/1000]\n",
      "- Train Loss : 8.540799215161426e-08 Score : 1.0\n",
      "- Val Loss : 6.836823729372554e-08 Score : 1.0\n",
      "[438/1000]\n",
      "- Train Loss : 8.525813715900184e-08 Score : 1.0\n",
      "- Val Loss : 6.81882923458943e-08 Score : 1.0\n",
      "[439/1000]\n",
      "- Train Loss : 8.515831252356134e-08 Score : 1.0\n",
      "- Val Loss : 6.80010501241668e-08 Score : 1.0\n",
      "[440/1000]\n",
      "- Train Loss : 8.504261493085128e-08 Score : 1.0\n",
      "- Val Loss : 6.776048877554786e-08 Score : 1.0\n",
      "[441/1000]\n",
      "- Train Loss : 8.358453431472901e-08 Score : 1.0\n",
      "- Val Loss : 6.755139025926837e-08 Score : 1.0\n",
      "[442/1000]\n",
      "- Train Loss : 8.345480862080743e-08 Score : 1.0\n",
      "- Val Loss : 6.728163270963705e-08 Score : 1.0\n",
      "[443/1000]\n",
      "- Train Loss : 8.197793278467518e-08 Score : 1.0\n",
      "- Val Loss : 6.708595634563608e-08 Score : 1.0\n",
      "[444/1000]\n",
      "- Train Loss : 8.186741393828578e-08 Score : 1.0\n",
      "- Val Loss : 6.687532305704735e-08 Score : 1.0\n",
      "[445/1000]\n",
      "- Train Loss : 8.173730000785337e-08 Score : 1.0\n",
      "- Val Loss : 6.661184670520015e-08 Score : 1.0\n",
      "[446/1000]\n",
      "- Train Loss : 8.158900087022285e-08 Score : 1.0\n",
      "- Val Loss : 6.6431248058052e-08 Score : 1.0\n",
      "[447/1000]\n",
      "- Train Loss : 8.148849698722908e-08 Score : 1.0\n",
      "- Val Loss : 6.624391346576886e-08 Score : 1.0\n",
      "[448/1000]\n",
      "- Train Loss : 8.137295029310278e-08 Score : 1.0\n",
      "- Val Loss : 6.600596691441751e-08 Score : 1.0\n",
      "[449/1000]\n",
      "- Train Loss : 8.124056527117649e-08 Score : 1.0\n",
      "- Val Loss : 6.58512888662699e-08 Score : 1.0\n",
      "[450/1000]\n",
      "- Train Loss : 7.983137415167195e-08 Score : 1.0\n",
      "- Val Loss : 6.567918120481409e-08 Score : 1.0\n",
      "[451/1000]\n",
      "- Train Loss : 7.972317703836613e-08 Score : 1.0\n",
      "- Val Loss : 6.542877173387751e-08 Score : 1.0\n",
      "[452/1000]\n",
      "- Train Loss : 7.958162230129832e-08 Score : 1.0\n",
      "- Val Loss : 6.520536288689982e-08 Score : 1.0\n",
      "[453/1000]\n",
      "- Train Loss : 7.812019224253921e-08 Score : 1.0\n",
      "- Val Loss : 6.493953463859725e-08 Score : 1.0\n",
      "[454/1000]\n",
      "- Train Loss : 7.79700226073247e-08 Score : 1.0\n",
      "- Val Loss : 6.475412561712801e-08 Score : 1.0\n",
      "[455/1000]\n",
      "- Train Loss : 7.654082497902485e-08 Score : 1.0\n",
      "- Val Loss : 6.459485746290738e-08 Score : 1.0\n",
      "[456/1000]\n",
      "- Train Loss : 7.645142153989766e-08 Score : 1.0\n",
      "- Val Loss : 6.440806998853077e-08 Score : 1.0\n",
      "[457/1000]\n",
      "- Train Loss : 7.633570490532075e-08 Score : 1.0\n",
      "- Val Loss : 6.416482989379801e-08 Score : 1.0\n",
      "[458/1000]\n",
      "- Train Loss : 7.619907161036481e-08 Score : 1.0\n",
      "- Val Loss : 6.395689666760518e-08 Score : 1.0\n",
      "[459/1000]\n",
      "- Train Loss : 7.474769754468098e-08 Score : 1.0\n",
      "- Val Loss : 6.370640193154031e-08 Score : 1.0\n",
      "[460/1000]\n",
      "- Train Loss : 7.328023447232699e-08 Score : 1.0\n",
      "- Val Loss : 6.350749970351899e-08 Score : 1.0\n",
      "[461/1000]\n",
      "- Train Loss : 7.316651853442633e-08 Score : 1.0\n",
      "- Val Loss : 6.3328215560432e-08 Score : 1.0\n",
      "[462/1000]\n",
      "- Train Loss : 7.306425958311966e-08 Score : 1.0\n",
      "- Val Loss : 6.312102840411171e-08 Score : 1.0\n",
      "[463/1000]\n",
      "- Train Loss : 7.293608209906087e-08 Score : 1.0\n",
      "- Val Loss : 6.286089160312258e-08 Score : 1.0\n",
      "[464/1000]\n",
      "- Train Loss : 7.27886550739977e-08 Score : 1.0\n",
      "- Val Loss : 6.267526941883261e-08 Score : 1.0\n",
      "[465/1000]\n",
      "- Train Loss : 7.135935306468783e-08 Score : 1.0\n",
      "- Val Loss : 6.247558559380195e-08 Score : 1.0\n",
      "[466/1000]\n",
      "- Train Loss : 7.123593536116173e-08 Score : 1.0\n",
      "- Val Loss : 6.222501269803615e-08 Score : 1.0\n",
      "[467/1000]\n",
      "- Train Loss : 7.109444552511057e-08 Score : 1.0\n",
      "- Val Loss : 6.204988522995336e-08 Score : 1.0\n",
      "[468/1000]\n",
      "- Train Loss : 7.09962073767846e-08 Score : 1.0\n",
      "- Val Loss : 6.186847656408645e-08 Score : 1.0\n",
      "[469/1000]\n",
      "- Train Loss : 6.955995466048945e-08 Score : 1.0\n",
      "- Val Loss : 6.162956367461447e-08 Score : 1.0\n",
      "[470/1000]\n",
      "- Train Loss : 6.942320528015146e-08 Score : 1.0\n",
      "- Val Loss : 6.139958941275836e-08 Score : 1.0\n",
      "[471/1000]\n",
      "- Train Loss : 6.795768679811958e-08 Score : 1.0\n",
      "- Val Loss : 6.112814787684329e-08 Score : 1.0\n",
      "[472/1000]\n",
      "- Train Loss : 6.78025874866391e-08 Score : 1.0\n",
      "- Val Loss : 6.092701454463167e-08 Score : 1.0\n",
      "[473/1000]\n",
      "- Train Loss : 6.768812381353135e-08 Score : 1.0\n",
      "- Val Loss : 6.075891434420555e-08 Score : 1.0\n",
      "[474/1000]\n",
      "- Train Loss : 6.626735827976029e-08 Score : 1.0\n",
      "- Val Loss : 6.05589605129353e-08 Score : 1.0\n",
      "[475/1000]\n",
      "- Train Loss : 6.614369613827664e-08 Score : 1.0\n",
      "- Val Loss : 6.030709442939042e-08 Score : 1.0\n",
      "[476/1000]\n",
      "- Train Loss : 6.60005510961607e-08 Score : 1.0\n",
      "- Val Loss : 6.012499653706982e-08 Score : 1.0\n",
      "[477/1000]\n",
      "- Train Loss : 6.589766597752616e-08 Score : 1.0\n",
      "- Val Loss : 5.993699403461505e-08 Score : 1.0\n",
      "[478/1000]\n",
      "- Train Loss : 6.445642548869035e-08 Score : 1.0\n",
      "- Val Loss : 5.967699934217308e-08 Score : 1.0\n",
      "[479/1000]\n",
      "- Train Loss : 6.430695586836757e-08 Score : 1.0\n",
      "- Val Loss : 5.947084602553332e-08 Score : 1.0\n",
      "[480/1000]\n",
      "- Train Loss : 6.418872066510373e-08 Score : 1.0\n",
      "- Val Loss : 5.9291046738962905e-08 Score : 1.0\n",
      "[481/1000]\n",
      "- Train Loss : 6.27611367837486e-08 Score : 1.0\n",
      "- Val Loss : 5.907878275479561e-08 Score : 1.0\n",
      "[482/1000]\n",
      "- Train Loss : 6.262796868451561e-08 Score : 1.0\n",
      "- Val Loss : 5.879142150888583e-08 Score : 1.0\n",
      "[483/1000]\n",
      "- Train Loss : 6.246161823720966e-08 Score : 1.0\n",
      "- Val Loss : 5.8561521854016974e-08 Score : 1.0\n",
      "[484/1000]\n",
      "- Train Loss : 6.232875319648088e-08 Score : 1.0\n",
      "- Val Loss : 5.836033523110018e-08 Score : 1.0\n",
      "[485/1000]\n",
      "- Train Loss : 6.221257655410053e-08 Score : 1.0\n",
      "- Val Loss : 5.8140045666732476e-08 Score : 1.0\n",
      "[486/1000]\n",
      "- Train Loss : 6.20767635420294e-08 Score : 1.0\n",
      "- Val Loss : 5.7875318759670336e-08 Score : 1.0\n",
      "[487/1000]\n",
      "- Train Loss : 6.192531821507128e-08 Score : 1.0\n",
      "- Val Loss : 5.758145604772835e-08 Score : 1.0\n",
      "[488/1000]\n",
      "- Train Loss : 6.173276788985e-08 Score : 1.0\n",
      "- Val Loss : 5.7166499090044454e-08 Score : 1.0\n",
      "[489/1000]\n",
      "- Train Loss : 6.149739238141425e-08 Score : 1.0\n",
      "- Val Loss : 5.6932396574893573e-08 Score : 1.0\n",
      "[490/1000]\n",
      "- Train Loss : 6.136547755590065e-08 Score : 1.0\n",
      "- Val Loss : 5.677406633708415e-08 Score : 1.0\n",
      "[491/1000]\n",
      "- Train Loss : 6.127698763140775e-08 Score : 1.0\n",
      "- Val Loss : 5.664895041945783e-08 Score : 1.0\n",
      "[492/1000]\n",
      "- Train Loss : 6.120758472360426e-08 Score : 1.0\n",
      "- Val Loss : 5.650471734952589e-08 Score : 1.0\n",
      "[493/1000]\n",
      "- Train Loss : 6.111857978515396e-08 Score : 1.0\n",
      "- Val Loss : 5.631406452266674e-08 Score : 1.0\n",
      "[494/1000]\n",
      "- Train Loss : 6.101260719334108e-08 Score : 1.0\n",
      "- Val Loss : 5.615327225427791e-08 Score : 1.0\n",
      "[495/1000]\n",
      "- Train Loss : 6.091482269756272e-08 Score : 1.0\n",
      "- Val Loss : 5.586849383121262e-08 Score : 1.0\n",
      "[496/1000]\n",
      "- Train Loss : 6.073122247303288e-08 Score : 1.0\n",
      "- Val Loss : 5.550356263483991e-08 Score : 1.0\n",
      "[497/1000]\n",
      "- Train Loss : 5.9203247474961774e-08 Score : 1.0\n",
      "- Val Loss : 5.532128710683537e-08 Score : 1.0\n",
      "[498/1000]\n",
      "- Train Loss : 5.9101109174058355e-08 Score : 1.0\n",
      "- Val Loss : 5.518901247114627e-08 Score : 1.0\n",
      "[499/1000]\n",
      "- Train Loss : 5.9027535478696956e-08 Score : 1.0\n",
      "- Val Loss : 5.5046093905275484e-08 Score : 1.0\n",
      "[500/1000]\n",
      "- Train Loss : 5.893991859928165e-08 Score : 1.0\n",
      "- Val Loss : 5.486302612212057e-08 Score : 1.0\n",
      "[501/1000]\n",
      "- Train Loss : 5.883810557869018e-08 Score : 1.0\n",
      "- Val Loss : 5.4742063326784773e-08 Score : 1.0\n",
      "[502/1000]\n",
      "- Train Loss : 5.877193582382959e-08 Score : 1.0\n",
      "- Val Loss : 5.4523589199106937e-08 Score : 1.0\n",
      "[503/1000]\n",
      "- Train Loss : 5.861894065791061e-08 Score : 1.0\n",
      "- Val Loss : 5.412339731947213e-08 Score : 1.0\n",
      "[504/1000]\n",
      "- Train Loss : 5.839519969927993e-08 Score : 1.0\n",
      "- Val Loss : 5.393831870037502e-08 Score : 1.0\n",
      "[505/1000]\n",
      "- Train Loss : 5.829415543566011e-08 Score : 1.0\n",
      "- Val Loss : 5.3840807368032984e-08 Score : 1.0\n",
      "[506/1000]\n",
      "- Train Loss : 5.8243021507935355e-08 Score : 1.0\n",
      "- Val Loss : 5.374672085167731e-08 Score : 1.0\n",
      "[507/1000]\n",
      "- Train Loss : 5.8185727413312405e-08 Score : 1.0\n",
      "- Val Loss : 5.36164925790672e-08 Score : 1.0\n",
      "[508/1000]\n",
      "- Train Loss : 5.811638413759214e-08 Score : 1.0\n",
      "- Val Loss : 5.342409536979176e-08 Score : 1.0\n",
      "[509/1000]\n",
      "- Train Loss : 5.798061034067992e-08 Score : 1.0\n",
      "- Val Loss : 5.306316452902138e-08 Score : 1.0\n",
      "[510/1000]\n",
      "- Train Loss : 5.778124343305207e-08 Score : 1.0\n",
      "- Val Loss : 5.292037386084303e-08 Score : 1.0\n",
      "[511/1000]\n",
      "- Train Loss : 5.638159750303203e-08 Score : 1.0\n",
      "- Val Loss : 5.285405180188718e-08 Score : 1.0\n",
      "[512/1000]\n",
      "- Train Loss : 5.634725700615073e-08 Score : 1.0\n",
      "- Val Loss : 5.2763262203825434e-08 Score : 1.0\n",
      "[513/1000]\n",
      "- Train Loss : 5.629121032111082e-08 Score : 1.0\n",
      "- Val Loss : 5.254191393078145e-08 Score : 1.0\n",
      "[514/1000]\n",
      "- Train Loss : 5.6147539593018635e-08 Score : 1.0\n",
      "- Val Loss : 5.224051591312673e-08 Score : 1.0\n",
      "[515/1000]\n",
      "- Train Loss : 5.5980806187387135e-08 Score : 1.0\n",
      "- Val Loss : 5.210594622440112e-08 Score : 1.0\n",
      "[516/1000]\n",
      "- Train Loss : 5.590902055400816e-08 Score : 1.0\n",
      "- Val Loss : 5.2008083173404884e-08 Score : 1.0\n",
      "[517/1000]\n",
      "- Train Loss : 5.585050475917508e-08 Score : 1.0\n",
      "- Val Loss : 5.188953622337067e-08 Score : 1.0\n",
      "[518/1000]\n",
      "- Train Loss : 5.57882326083904e-08 Score : 1.0\n",
      "- Val Loss : 5.171754224875258e-08 Score : 1.0\n",
      "[519/1000]\n",
      "- Train Loss : 5.566651932040818e-08 Score : 1.0\n",
      "- Val Loss : 5.1388816757480527e-08 Score : 1.0\n",
      "[520/1000]\n",
      "- Train Loss : 5.548580407941126e-08 Score : 1.0\n",
      "- Val Loss : 5.126508284547526e-08 Score : 1.0\n",
      "[521/1000]\n",
      "- Train Loss : 5.542177363295702e-08 Score : 1.0\n",
      "- Val Loss : 4.696622113442572e-08 Score : 1.0\n",
      "[522/1000]\n",
      "- Train Loss : 5.540455731585051e-08 Score : 1.0\n",
      "- Val Loss : 4.684114429664987e-08 Score : 1.0\n",
      "[523/1000]\n",
      "- Train Loss : 5.531066935787137e-08 Score : 1.0\n",
      "- Val Loss : 4.654620511246321e-08 Score : 1.0\n",
      "[524/1000]\n",
      "- Train Loss : 5.515024937656025e-08 Score : 1.0\n",
      "- Val Loss : 4.645034579198182e-08 Score : 1.0\n",
      "[525/1000]\n",
      "- Train Loss : 5.37784936679226e-08 Score : 1.0\n",
      "- Val Loss : 4.6393786590215313e-08 Score : 1.0\n",
      "[526/1000]\n",
      "- Train Loss : 5.374364895862873e-08 Score : 1.0\n",
      "- Val Loss : 4.626513216976491e-08 Score : 1.0\n",
      "[527/1000]\n",
      "- Train Loss : 5.366801422866817e-08 Score : 1.0\n",
      "- Val Loss : 4.6140513632053626e-08 Score : 1.0\n",
      "[528/1000]\n",
      "- Train Loss : 5.360216077827477e-08 Score : 1.0\n",
      "- Val Loss : 4.1717587606626694e-08 Score : 1.0\n",
      "[529/1000]\n",
      "- Train Loss : 5.3485960724451875e-08 Score : 1.0\n",
      "- Val Loss : 4.140840559330172e-08 Score : 1.0\n",
      "[530/1000]\n",
      "- Train Loss : 5.3316311420202135e-08 Score : 1.0\n",
      "- Val Loss : 4.129381991901937e-08 Score : 1.0\n",
      "[531/1000]\n",
      "- Train Loss : 5.32572913929108e-08 Score : 1.0\n",
      "- Val Loss : 4.125697472545653e-08 Score : 1.0\n",
      "[532/1000]\n",
      "- Train Loss : 5.324250654132704e-08 Score : 1.0\n",
      "- Val Loss : 4.114158969059645e-08 Score : 1.0\n",
      "[533/1000]\n",
      "- Train Loss : 5.315597726491323e-08 Score : 1.0\n",
      "- Val Loss : 4.0866442674314385e-08 Score : 1.0\n",
      "[534/1000]\n",
      "- Train Loss : 5.3006515217622154e-08 Score : 1.0\n",
      "- Val Loss : 4.077830340065702e-08 Score : 1.0\n",
      "[535/1000]\n",
      "- Train Loss : 5.296347665031336e-08 Score : 1.0\n",
      "- Val Loss : 4.0735852024909036e-08 Score : 1.0\n",
      "[536/1000]\n",
      "- Train Loss : 5.293919577323589e-08 Score : 1.0\n",
      "- Val Loss : 4.05676701120683e-08 Score : 1.0\n",
      "[537/1000]\n",
      "- Train Loss : 5.282371536751953e-08 Score : 1.0\n",
      "- Val Loss : 4.028670019806668e-08 Score : 1.0\n",
      "[538/1000]\n",
      "- Train Loss : 5.2672144800079856e-08 Score : 1.0\n",
      "- Val Loss : 4.0211080687413414e-08 Score : 1.0\n",
      "[539/1000]\n",
      "- Train Loss : 5.1312597008603934e-08 Score : 1.0\n",
      "- Val Loss : 3.5948026777532505e-08 Score : 1.0\n",
      "[540/1000]\n",
      "- Train Loss : 5.131467456663695e-08 Score : 1.0\n",
      "- Val Loss : 3.584240104714809e-08 Score : 1.0\n",
      "[541/1000]\n",
      "- Train Loss : 5.1234468159149104e-08 Score : 1.0\n",
      "- Val Loss : 3.557594041581069e-08 Score : 1.0\n",
      "[542/1000]\n",
      "- Train Loss : 5.1089150268712127e-08 Score : 1.0\n",
      "- Val Loss : 3.548170113276683e-08 Score : 1.0\n",
      "[543/1000]\n",
      "- Train Loss : 5.104172846303231e-08 Score : 1.0\n",
      "- Val Loss : 3.54292524207267e-08 Score : 1.0\n",
      "[544/1000]\n",
      "- Train Loss : 4.968677822074959e-08 Score : 1.0\n",
      "- Val Loss : 3.533017434165231e-08 Score : 1.0\n",
      "[545/1000]\n",
      "- Train Loss : 4.962936828938829e-08 Score : 1.0\n",
      "- Val Loss : 3.516175439699509e-08 Score : 1.0\n",
      "[546/1000]\n",
      "- Train Loss : 4.952078751847062e-08 Score : 1.0\n",
      "- Val Loss : 3.493915912144985e-08 Score : 1.0\n",
      "[547/1000]\n",
      "- Train Loss : 4.940080865482095e-08 Score : 1.0\n",
      "- Val Loss : 3.486885802317374e-08 Score : 1.0\n",
      "[548/1000]\n",
      "- Train Loss : 4.936732699601746e-08 Score : 1.0\n",
      "- Val Loss : 3.483202704046562e-08 Score : 1.0\n",
      "[549/1000]\n",
      "- Train Loss : 4.93461938236449e-08 Score : 1.0\n",
      "- Val Loss : 3.46730253397709e-08 Score : 1.0\n",
      "[550/1000]\n",
      "- Train Loss : 4.923717291406097e-08 Score : 1.0\n",
      "- Val Loss : 3.440646167973682e-08 Score : 1.0\n",
      "[551/1000]\n",
      "- Train Loss : 4.9093175950787946e-08 Score : 1.0\n",
      "- Val Loss : 3.43325936569272e-08 Score : 1.0\n",
      "[552/1000]\n",
      "- Train Loss : 4.905868551250476e-08 Score : 1.0\n",
      "- Val Loss : 3.4335279508468375e-08 Score : 1.0\n",
      "[553/1000]\n",
      "- Train Loss : 4.774296097585321e-08 Score : 1.0\n",
      "- Val Loss : 3.4259603154396245e-08 Score : 1.0\n",
      "[554/1000]\n",
      "- Train Loss : 4.768093404750657e-08 Score : 1.0\n",
      "- Val Loss : 3.401417458803735e-08 Score : 1.0\n",
      "[555/1000]\n",
      "- Train Loss : 4.754680728187114e-08 Score : 1.0\n",
      "- Val Loss : 3.389730096614585e-08 Score : 1.0\n",
      "[556/1000]\n",
      "- Train Loss : 4.74796433882115e-08 Score : 1.0\n",
      "- Val Loss : 3.38023724566483e-08 Score : 1.0\n",
      "[557/1000]\n",
      "- Train Loss : 4.74312790336378e-08 Score : 1.0\n",
      "- Val Loss : 3.367807011045443e-08 Score : 1.0\n",
      "[558/1000]\n",
      "- Train Loss : 4.734275718136659e-08 Score : 1.0\n",
      "- Val Loss : 3.3429131462980877e-08 Score : 1.0\n",
      "[559/1000]\n",
      "- Train Loss : 4.72077194688731e-08 Score : 1.0\n",
      "- Val Loss : 3.3351064132602914e-08 Score : 1.0\n",
      "[560/1000]\n",
      "- Train Loss : 4.7169820969333864e-08 Score : 1.0\n",
      "- Val Loss : 3.334085718620372e-08 Score : 1.0\n",
      "[561/1000]\n",
      "- Train Loss : 4.717040990409902e-08 Score : 1.0\n",
      "- Val Loss : 3.3260754150887806e-08 Score : 1.0\n",
      "[562/1000]\n",
      "- Train Loss : 4.71076787909709e-08 Score : 1.0\n",
      "- Val Loss : 3.3038737967672205e-08 Score : 1.0\n",
      "[563/1000]\n",
      "- Train Loss : 4.698867598529024e-08 Score : 1.0\n",
      "- Val Loss : 3.29556506528661e-08 Score : 1.0\n",
      "[564/1000]\n",
      "- Train Loss : 4.561769930087426e-08 Score : 1.0\n",
      "- Val Loss : 3.289299499442677e-08 Score : 1.0\n",
      "[565/1000]\n",
      "- Train Loss : 4.5588543570767826e-08 Score : 1.0\n",
      "- Val Loss : 3.2793671778108546e-08 Score : 1.0\n",
      "[566/1000]\n",
      "- Train Loss : 4.551505781914051e-08 Score : 1.0\n",
      "- Val Loss : 3.2566145335977126e-08 Score : 1.0\n",
      "[567/1000]\n",
      "- Train Loss : 4.5392921964469856e-08 Score : 1.0\n",
      "- Val Loss : 3.2508037151046665e-08 Score : 1.0\n",
      "[568/1000]\n",
      "- Train Loss : 4.404248121488923e-08 Score : 1.0\n",
      "- Val Loss : 3.241621371330439e-08 Score : 1.0\n",
      "[569/1000]\n",
      "- Train Loss : 4.397291820829966e-08 Score : 1.0\n",
      "- Val Loss : 3.218068300725463e-08 Score : 1.0\n",
      "[570/1000]\n",
      "- Train Loss : 4.3844162624558557e-08 Score : 1.0\n",
      "- Val Loss : 3.209357402056412e-08 Score : 1.0\n",
      "[571/1000]\n",
      "- Train Loss : 4.3799797171696166e-08 Score : 1.0\n",
      "- Val Loss : 3.204248599786297e-08 Score : 1.0\n",
      "[572/1000]\n",
      "- Train Loss : 4.377012373591973e-08 Score : 1.0\n",
      "- Val Loss : 3.195434672420561e-08 Score : 1.0\n",
      "[573/1000]\n",
      "- Train Loss : 4.3719595838274485e-08 Score : 1.0\n",
      "- Val Loss : 3.1877608108743516e-08 Score : 1.0\n",
      "[574/1000]\n",
      "- Train Loss : 4.3681365992866305e-08 Score : 1.0\n",
      "- Val Loss : 3.1769506136924974e-08 Score : 1.0\n",
      "[575/1000]\n",
      "- Train Loss : 4.360370539616286e-08 Score : 1.0\n",
      "- Val Loss : 3.154347538725233e-08 Score : 1.0\n",
      "[576/1000]\n",
      "- Train Loss : 4.348148498104714e-08 Score : 1.0\n",
      "- Val Loss : 3.147594185293201e-08 Score : 1.0\n",
      "[577/1000]\n",
      "- Train Loss : 4.3449343815160945e-08 Score : 1.0\n",
      "- Val Loss : 3.140541693369414e-08 Score : 1.0\n",
      "[578/1000]\n",
      "- Train Loss : 4.339940873035155e-08 Score : 1.0\n",
      "- Val Loss : 3.1255066090807304e-08 Score : 1.0\n",
      "[579/1000]\n",
      "- Train Loss : 4.332064733248493e-08 Score : 1.0\n",
      "- Val Loss : 3.120060299011129e-08 Score : 1.0\n",
      "[580/1000]\n",
      "- Train Loss : 4.3290551315371283e-08 Score : 1.0\n",
      "- Val Loss : 3.1132238120790134e-08 Score : 1.0\n",
      "[581/1000]\n",
      "- Train Loss : 4.1928153569248084e-08 Score : 1.0\n",
      "- Val Loss : 3.098839229664918e-08 Score : 1.0\n",
      "[582/1000]\n",
      "- Train Loss : 4.0506169352722e-08 Score : 1.0\n",
      "- Val Loss : 3.075091470350344e-08 Score : 1.0\n",
      "[583/1000]\n",
      "- Train Loss : 4.037728935923912e-08 Score : 1.0\n",
      "- Val Loss : 3.067936660272608e-08 Score : 1.0\n",
      "[584/1000]\n",
      "- Train Loss : 4.0342704165348254e-08 Score : 1.0\n",
      "- Val Loss : 3.0673621864707457e-08 Score : 1.0\n",
      "[585/1000]\n",
      "- Train Loss : 3.7696042572143984e-08 Score : 1.0\n",
      "- Val Loss : 2.6333323077665227e-08 Score : 1.0\n",
      "[586/1000]\n",
      "- Train Loss : 3.763143607625783e-08 Score : 1.0\n",
      "- Val Loss : 2.609501059680497e-08 Score : 1.0\n",
      "[587/1000]\n",
      "- Train Loss : 3.7499221791212965e-08 Score : 1.0\n",
      "- Val Loss : 2.5986386376075643e-08 Score : 1.0\n",
      "[588/1000]\n",
      "- Train Loss : 3.744076107905441e-08 Score : 1.0\n",
      "- Val Loss : 2.59303618577178e-08 Score : 1.0\n",
      "[589/1000]\n",
      "- Train Loss : 3.741225926105762e-08 Score : 1.0\n",
      "- Val Loss : 2.5813344350922307e-08 Score : 1.0\n",
      "[590/1000]\n",
      "- Train Loss : 3.732931473481027e-08 Score : 1.0\n",
      "- Val Loss : 2.557596978647325e-08 Score : 1.0\n",
      "[591/1000]\n",
      "- Train Loss : 3.719860303312407e-08 Score : 1.0\n",
      "- Val Loss : 2.5480940024635856e-08 Score : 1.0\n",
      "[592/1000]\n",
      "- Train Loss : 3.7148760619362985e-08 Score : 1.0\n",
      "- Val Loss : 2.542163102248196e-08 Score : 1.0\n",
      "[593/1000]\n",
      "- Train Loss : 3.7113995851138317e-08 Score : 1.0\n",
      "- Val Loss : 2.529042397725334e-08 Score : 1.0\n",
      "[594/1000]\n",
      "- Train Loss : 3.702927519761331e-08 Score : 1.0\n",
      "- Val Loss : 2.5109205381568245e-08 Score : 1.0\n",
      "[595/1000]\n",
      "- Train Loss : 3.693129217930937e-08 Score : 1.0\n",
      "- Val Loss : 2.504752671939059e-08 Score : 1.0\n",
      "[596/1000]\n",
      "- Train Loss : 3.690109031939207e-08 Score : 1.0\n",
      "- Val Loss : 2.4995028269358954e-08 Score : 1.0\n",
      "[597/1000]\n",
      "- Train Loss : 3.6866833648396824e-08 Score : 1.0\n",
      "- Val Loss : 2.4801675380103916e-08 Score : 1.0\n",
      "[598/1000]\n",
      "- Train Loss : 3.673863610727246e-08 Score : 1.0\n",
      "- Val Loss : 2.4525405706299352e-08 Score : 1.0\n",
      "[599/1000]\n",
      "- Train Loss : 3.65878784852228e-08 Score : 1.0\n",
      "- Val Loss : 2.4441970225552723e-08 Score : 1.0\n",
      "[600/1000]\n",
      "- Train Loss : 3.65464068905459e-08 Score : 1.0\n",
      "- Val Loss : 2.443483637648569e-08 Score : 1.0\n",
      "[601/1000]\n",
      "- Train Loss : 3.654810773063196e-08 Score : 1.0\n",
      "- Val Loss : 2.4358772776622573e-08 Score : 1.0\n",
      "[602/1000]\n",
      "- Train Loss : 3.648724875016107e-08 Score : 1.0\n",
      "- Val Loss : 2.4138728349498706e-08 Score : 1.0\n",
      "[603/1000]\n",
      "- Train Loss : 3.636894006767298e-08 Score : 1.0\n",
      "- Val Loss : 2.4086517669275054e-08 Score : 1.0\n",
      "[604/1000]\n",
      "- Train Loss : 3.634571562659129e-08 Score : 1.0\n",
      "- Val Loss : 2.410035904176766e-08 Score : 1.0\n",
      "[605/1000]\n",
      "- Train Loss : 3.6359890822137975e-08 Score : 1.0\n",
      "- Val Loss : 2.4040494039923033e-08 Score : 1.0\n",
      "[606/1000]\n",
      "- Train Loss : 3.630874731488468e-08 Score : 1.0\n",
      "- Val Loss : 1.957637074667673e-08 Score : 1.0\n",
      "[607/1000]\n",
      "- Train Loss : 3.4873966534588553e-08 Score : 1.0\n",
      "- Val Loss : 1.950629346936239e-08 Score : 1.0\n",
      "[608/1000]\n",
      "- Train Loss : 3.4833775356656514e-08 Score : 1.0\n",
      "- Val Loss : 1.935203819414255e-08 Score : 1.0\n",
      "[609/1000]\n",
      "- Train Loss : 3.4729419049881536e-08 Score : 1.0\n",
      "- Val Loss : 1.9104245296830413e-08 Score : 1.0\n",
      "[610/1000]\n",
      "- Train Loss : 3.459417817717562e-08 Score : 1.0\n",
      "- Val Loss : 1.9025454989218815e-08 Score : 1.0\n",
      "[611/1000]\n",
      "- Train Loss : 3.4554623108271685e-08 Score : 1.0\n",
      "- Val Loss : 1.901380031199551e-08 Score : 1.0\n",
      "[612/1000]\n",
      "- Train Loss : 3.455303763250095e-08 Score : 1.0\n",
      "- Val Loss : 1.895779355720606e-08 Score : 1.0\n",
      "[613/1000]\n",
      "- Train Loss : 3.450969420334651e-08 Score : 1.0\n",
      "- Val Loss : 1.880043321023095e-08 Score : 1.0\n",
      "[614/1000]\n",
      "- Train Loss : 3.442608700771991e-08 Score : 1.0\n",
      "- Val Loss : 1.8744374941093156e-08 Score : 1.0\n",
      "[615/1000]\n",
      "- Train Loss : 3.4395207148093766e-08 Score : 1.0\n",
      "- Val Loss : 1.867031507174488e-08 Score : 1.0\n",
      "[616/1000]\n",
      "- Train Loss : 3.435055859433526e-08 Score : 1.0\n",
      "- Val Loss : 1.8591844508364375e-08 Score : 1.0\n",
      "[617/1000]\n",
      "- Train Loss : 3.4311925812754995e-08 Score : 1.0\n",
      "- Val Loss : 1.8520292854873333e-08 Score : 1.0\n",
      "[618/1000]\n",
      "- Train Loss : 3.426047607129704e-08 Score : 1.0\n",
      "- Val Loss : 1.836770202601201e-08 Score : 1.0\n",
      "[619/1000]\n",
      "- Train Loss : 3.418030481913692e-08 Score : 1.0\n",
      "- Val Loss : 1.826919593383991e-08 Score : 1.0\n",
      "[620/1000]\n",
      "- Train Loss : 3.4113150911123845e-08 Score : 1.0\n",
      "- Val Loss : 1.8103815335734907e-08 Score : 1.0\n",
      "[621/1000]\n",
      "- Train Loss : 3.4026180487012956e-08 Score : 1.0\n",
      "- Val Loss : 1.8081289354654473e-08 Score : 1.0\n",
      "[622/1000]\n",
      "- Train Loss : 3.4020055203476396e-08 Score : 1.0\n",
      "- Val Loss : 1.8024866932364603e-08 Score : 1.0\n",
      "[623/1000]\n",
      "- Train Loss : 3.397346379527575e-08 Score : 1.0\n",
      "- Val Loss : 1.7847769484546916e-08 Score : 1.0\n",
      "[624/1000]\n",
      "- Train Loss : 3.388031321408432e-08 Score : 1.0\n",
      "- Val Loss : 1.7827121112645727e-08 Score : 1.0\n",
      "[625/1000]\n",
      "- Train Loss : 3.3875625485869455e-08 Score : 1.0\n",
      "- Val Loss : 1.7777546545971745e-08 Score : 1.0\n",
      "[626/1000]\n",
      "- Train Loss : 3.383340144428377e-08 Score : 1.0\n",
      "- Val Loss : 1.760905909975463e-08 Score : 1.0\n",
      "[627/1000]\n",
      "- Train Loss : 3.374550465316508e-08 Score : 1.0\n",
      "- Val Loss : 1.7562971521556392e-08 Score : 1.0\n",
      "[628/1000]\n",
      "- Train Loss : 3.3718620101481446e-08 Score : 1.0\n",
      "- Val Loss : 1.7523090534155017e-08 Score : 1.0\n",
      "[629/1000]\n",
      "- Train Loss : 3.2379000185327654e-08 Score : 1.0\n",
      "- Val Loss : 1.7470725310886337e-08 Score : 1.0\n",
      "[630/1000]\n",
      "- Train Loss : 3.2334977234919136e-08 Score : 1.0\n",
      "- Val Loss : 1.7291204912339708e-08 Score : 1.0\n",
      "[631/1000]\n",
      "- Train Loss : 3.223877357144226e-08 Score : 1.0\n",
      "- Val Loss : 1.7248186878759952e-08 Score : 1.0\n",
      "[632/1000]\n",
      "- Train Loss : 3.2219781930813307e-08 Score : 1.0\n",
      "- Val Loss : 1.7177123723399745e-08 Score : 1.0\n",
      "[633/1000]\n",
      "- Train Loss : 3.216573209309823e-08 Score : 1.0\n",
      "- Val Loss : 1.6998074059415558e-08 Score : 1.0\n",
      "[634/1000]\n",
      "- Train Loss : 3.207028507629569e-08 Score : 1.0\n",
      "- Val Loss : 1.6962532711772838e-08 Score : 1.0\n",
      "[635/1000]\n",
      "- Train Loss : 3.205600035628389e-08 Score : 1.0\n",
      "- Val Loss : 1.6951640091633635e-08 Score : 1.0\n",
      "[636/1000]\n",
      "- Train Loss : 3.2048187942180906e-08 Score : 1.0\n",
      "- Val Loss : 1.684411010671738e-08 Score : 1.0\n",
      "[637/1000]\n",
      "- Train Loss : 3.1977360231200745e-08 Score : 1.0\n",
      "- Val Loss : 1.6689897464061687e-08 Score : 1.0\n",
      "[638/1000]\n",
      "- Train Loss : 3.1896439983075055e-08 Score : 1.0\n",
      "- Val Loss : 1.667146776185291e-08 Score : 1.0\n",
      "[639/1000]\n",
      "- Train Loss : 3.1892313942130967e-08 Score : 1.0\n",
      "- Val Loss : 1.6624172261003878e-08 Score : 1.0\n",
      "[640/1000]\n",
      "- Train Loss : 3.1852747727628694e-08 Score : 1.0\n",
      "- Val Loss : 1.6467796015717795e-08 Score : 1.0\n",
      "[641/1000]\n",
      "- Train Loss : 3.1770904470148876e-08 Score : 1.0\n",
      "- Val Loss : 1.6454020368428246e-08 Score : 1.0\n",
      "[642/1000]\n",
      "- Train Loss : 3.176979455397756e-08 Score : 1.0\n",
      "- Val Loss : 1.6413853387575728e-08 Score : 1.0\n",
      "[643/1000]\n",
      "- Train Loss : 3.1734520877829376e-08 Score : 1.0\n",
      "- Val Loss : 1.6264948499156162e-08 Score : 1.0\n",
      "[644/1000]\n",
      "- Train Loss : 3.165729867138367e-08 Score : 1.0\n",
      "- Val Loss : 1.622754908225943e-08 Score : 1.0\n",
      "[645/1000]\n",
      "- Train Loss : 3.163573308603223e-08 Score : 1.0\n",
      "- Val Loss : 1.6131473046243627e-08 Score : 1.0\n",
      "[646/1000]\n",
      "- Train Loss : 3.157350101063654e-08 Score : 1.0\n",
      "- Val Loss : 1.6007499326065044e-08 Score : 1.0\n",
      "[647/1000]\n",
      "- Train Loss : 3.151083499300686e-08 Score : 1.0\n",
      "- Val Loss : 1.6017647652688538e-08 Score : 1.0\n",
      "[648/1000]\n",
      "- Train Loss : 3.152397249585943e-08 Score : 1.0\n",
      "- Val Loss : 1.5997549951407564e-08 Score : 1.0\n",
      "[649/1000]\n",
      "- Train Loss : 3.150071239512735e-08 Score : 1.0\n",
      "- Val Loss : 1.5848414136598876e-08 Score : 1.0\n",
      "[650/1000]\n",
      "- Train Loss : 3.1419879059924514e-08 Score : 1.0\n",
      "- Val Loss : 1.581366326774969e-08 Score : 1.0\n",
      "[651/1000]\n",
      "- Train Loss : 3.008334349512143e-08 Score : 1.0\n",
      "- Val Loss : 1.5778143236389042e-08 Score : 1.0\n",
      "[652/1000]\n",
      "- Train Loss : 3.005112002026849e-08 Score : 1.0\n",
      "- Val Loss : 1.5628801364186984e-08 Score : 1.0\n",
      "[653/1000]\n",
      "- Train Loss : 2.997208886711893e-08 Score : 1.0\n",
      "- Val Loss : 1.5603266234620605e-08 Score : 1.0\n",
      "[654/1000]\n",
      "- Train Loss : 2.863853961287811e-08 Score : 1.0\n",
      "- Val Loss : 1.554893103161703e-08 Score : 1.0\n",
      "[655/1000]\n",
      "- Train Loss : 2.7270402628580703e-08 Score : 1.0\n",
      "- Val Loss : 1.5379297835238503e-08 Score : 1.0\n",
      "[656/1000]\n",
      "- Train Loss : 2.7178877619172937e-08 Score : 1.0\n",
      "- Val Loss : 1.5332073388663048e-08 Score : 1.0\n",
      "[657/1000]\n",
      "- Train Loss : 2.7156516995596426e-08 Score : 1.0\n",
      "- Val Loss : 1.525918058575826e-08 Score : 1.0\n",
      "[658/1000]\n",
      "- Train Loss : 2.71026150972196e-08 Score : 1.0\n",
      "- Val Loss : 1.5089128169165633e-08 Score : 1.0\n",
      "[659/1000]\n",
      "- Train Loss : 2.7011285816854375e-08 Score : 1.0\n",
      "- Val Loss : 1.5048863488686948e-08 Score : 1.0\n",
      "[660/1000]\n",
      "- Train Loss : 2.69933896770374e-08 Score : 1.0\n",
      "- Val Loss : 1.503117275092336e-08 Score : 1.0\n",
      "[661/1000]\n",
      "- Train Loss : 2.6981594051796947e-08 Score : 1.0\n",
      "- Val Loss : 1.492677270675813e-08 Score : 1.0\n",
      "[662/1000]\n",
      "- Train Loss : 2.6913340678165015e-08 Score : 1.0\n",
      "- Val Loss : 1.4780569657091291e-08 Score : 1.0\n",
      "[663/1000]\n",
      "- Train Loss : 2.6836006648745586e-08 Score : 1.0\n",
      "- Val Loss : 1.4757140398558022e-08 Score : 1.0\n",
      "[664/1000]\n",
      "- Train Loss : 2.6828054371502246e-08 Score : 1.0\n",
      "- Val Loss : 1.4708095186222181e-08 Score : 1.0\n",
      "[665/1000]\n",
      "- Train Loss : 2.67886244235767e-08 Score : 1.0\n",
      "- Val Loss : 1.4561007510849322e-08 Score : 1.0\n",
      "[666/1000]\n",
      "- Train Loss : 2.6711107511898553e-08 Score : 1.0\n",
      "- Val Loss : 1.4542614223955752e-08 Score : 1.0\n",
      "[667/1000]\n",
      "- Train Loss : 2.6706375335827803e-08 Score : 1.0\n",
      "- Val Loss : 1.4500753486856865e-08 Score : 1.0\n",
      "[668/1000]\n",
      "- Train Loss : 2.667133821476455e-08 Score : 1.0\n",
      "- Val Loss : 1.4361477340685269e-08 Score : 1.0\n",
      "[669/1000]\n",
      "- Train Loss : 2.659860146806553e-08 Score : 1.0\n",
      "- Val Loss : 1.4338617404519027e-08 Score : 1.0\n",
      "[670/1000]\n",
      "- Train Loss : 2.6588746608467407e-08 Score : 1.0\n",
      "- Val Loss : 1.4272449000429788e-08 Score : 1.0\n",
      "[671/1000]\n",
      "- Train Loss : 2.6540369397568913e-08 Score : 1.0\n",
      "- Val Loss : 1.4129583725264183e-08 Score : 1.0\n",
      "[672/1000]\n",
      "- Train Loss : 2.6466015025661722e-08 Score : 1.0\n",
      "- Val Loss : 1.4123452629632993e-08 Score : 1.0\n",
      "[673/1000]\n",
      "- Train Loss : 2.6468809750761213e-08 Score : 1.0\n",
      "- Val Loss : 1.4096720235556859e-08 Score : 1.0\n",
      "[674/1000]\n",
      "- Train Loss : 2.6443030736421347e-08 Score : 1.0\n",
      "- Val Loss : 1.3973143531131882e-08 Score : 1.0\n",
      "[675/1000]\n",
      "- Train Loss : 2.6379755498748425e-08 Score : 1.0\n",
      "- Val Loss : 1.3904764450956009e-08 Score : 1.0\n",
      "[676/1000]\n",
      "- Train Loss : 2.633083141887621e-08 Score : 1.0\n",
      "- Val Loss : 1.3770359963416468e-08 Score : 1.0\n",
      "[677/1000]\n",
      "- Train Loss : 2.626178390676245e-08 Score : 1.0\n",
      "- Val Loss : 1.3776295659795323e-08 Score : 1.0\n",
      "[678/1000]\n",
      "- Train Loss : 2.494741007081574e-08 Score : 1.0\n",
      "- Val Loss : 1.376007663367318e-08 Score : 1.0\n",
      "[679/1000]\n",
      "- Train Loss : 2.3602710814935345e-08 Score : 1.0\n",
      "- Val Loss : 1.3623639993909364e-08 Score : 1.0\n",
      "[680/1000]\n",
      "- Train Loss : 2.3529427370325613e-08 Score : 1.0\n",
      "- Val Loss : 1.3532707399122046e-08 Score : 1.0\n",
      "[681/1000]\n",
      "- Train Loss : 2.2144349708579402e-08 Score : 1.0\n",
      "- Val Loss : 1.3386878272569902e-08 Score : 1.0\n",
      "[682/1000]\n",
      "- Train Loss : 2.206552285516284e-08 Score : 1.0\n",
      "- Val Loss : 1.3343075977445551e-08 Score : 1.0\n",
      "[683/1000]\n",
      "- Train Loss : 2.204431179158599e-08 Score : 1.0\n",
      "- Val Loss : 1.3314699565114552e-08 Score : 1.0\n",
      "[684/1000]\n",
      "- Train Loss : 2.2026074368247506e-08 Score : 1.0\n",
      "- Val Loss : 1.321148257460436e-08 Score : 1.0\n",
      "[685/1000]\n",
      "- Train Loss : 2.195986343808687e-08 Score : 1.0\n",
      "- Val Loss : 1.3075983851251749e-08 Score : 1.0\n",
      "[686/1000]\n",
      "- Train Loss : 2.1887363290611855e-08 Score : 1.0\n",
      "- Val Loss : 1.3043809587998112e-08 Score : 1.0\n",
      "[687/1000]\n",
      "- Train Loss : 2.1873163862712936e-08 Score : 1.0\n",
      "- Val Loss : 1.2987632302952079e-08 Score : 1.0\n",
      "[688/1000]\n",
      "- Train Loss : 2.1830681859940114e-08 Score : 1.0\n",
      "- Val Loss : 1.2845116081905417e-08 Score : 1.0\n",
      "[689/1000]\n",
      "- Train Loss : 2.1754603162874957e-08 Score : 1.0\n",
      "- Val Loss : 1.2815794647735856e-08 Score : 1.0\n",
      "[690/1000]\n",
      "- Train Loss : 2.1742393181874153e-08 Score : 1.0\n",
      "- Val Loss : 1.2766433243882602e-08 Score : 1.0\n",
      "[691/1000]\n",
      "- Train Loss : 2.1704179464961115e-08 Score : 1.0\n",
      "- Val Loss : 1.2632318302507883e-08 Score : 1.0\n",
      "[692/1000]\n",
      "- Train Loss : 2.163317990051585e-08 Score : 1.0\n",
      "- Val Loss : 1.2611421240649179e-08 Score : 1.0\n",
      "[693/1000]\n",
      "- Train Loss : 2.1625985066088538e-08 Score : 1.0\n",
      "- Val Loss : 1.257027726353499e-08 Score : 1.0\n",
      "[694/1000]\n",
      "- Train Loss : 2.159275605443594e-08 Score : 1.0\n",
      "- Val Loss : 1.2444180796933324e-08 Score : 1.0\n",
      "[695/1000]\n",
      "- Train Loss : 2.152657074527368e-08 Score : 1.0\n",
      "- Val Loss : 1.2380568570335981e-08 Score : 1.0\n",
      "[696/1000]\n",
      "- Train Loss : 2.1484245957020038e-08 Score : 1.0\n",
      "- Val Loss : 1.2281583749995661e-08 Score : 1.0\n",
      "[697/1000]\n",
      "- Train Loss : 2.143356679177681e-08 Score : 1.0\n",
      "- Val Loss : 1.2258003501131043e-08 Score : 1.0\n",
      "[698/1000]\n",
      "- Train Loss : 2.141977020001425e-08 Score : 1.0\n",
      "- Val Loss : 1.2180812802853325e-08 Score : 1.0\n",
      "[699/1000]\n",
      "- Train Loss : 2.13694353005713e-08 Score : 1.0\n",
      "- Val Loss : 1.2075095590091678e-08 Score : 1.0\n",
      "[700/1000]\n",
      "- Train Loss : 2.131529063178807e-08 Score : 1.0\n",
      "- Val Loss : 1.2028357865290218e-08 Score : 1.0\n",
      "[701/1000]\n",
      "- Train Loss : 2.1283076741956952e-08 Score : 1.0\n",
      "- Val Loss : 1.1944749189751747e-08 Score : 1.0\n",
      "[702/1000]\n",
      "- Train Loss : 2.1241580169285845e-08 Score : 1.0\n",
      "- Val Loss : 1.1935184396349996e-08 Score : 1.0\n",
      "[703/1000]\n",
      "- Train Loss : 2.1236196633461923e-08 Score : 1.0\n",
      "- Val Loss : 1.1860636917049305e-08 Score : 1.0\n",
      "[704/1000]\n",
      "- Train Loss : 1.986022788161907e-08 Score : 1.0\n",
      "- Val Loss : 1.1730265647713622e-08 Score : 1.0\n",
      "[705/1000]\n",
      "- Train Loss : 1.9792057687110448e-08 Score : 1.0\n",
      "- Val Loss : 1.1722694814864099e-08 Score : 1.0\n",
      "[706/1000]\n",
      "- Train Loss : 1.9793004207557346e-08 Score : 1.0\n",
      "- Val Loss : 1.1698794821768388e-08 Score : 1.0\n",
      "[707/1000]\n",
      "- Train Loss : 1.977034669280407e-08 Score : 1.0\n",
      "- Val Loss : 1.1552724110686086e-08 Score : 1.0\n",
      "[708/1000]\n",
      "- Train Loss : 1.968474320631337e-08 Score : 1.0\n",
      "- Val Loss : 1.1456938509013526e-08 Score : 1.0\n",
      "[709/1000]\n",
      "- Train Loss : 1.9636473639728863e-08 Score : 1.0\n",
      "- Val Loss : 1.1444697634033218e-08 Score : 1.0\n",
      "[710/1000]\n",
      "- Train Loss : 1.962966153772755e-08 Score : 1.0\n",
      "- Val Loss : 1.1371838581908378e-08 Score : 1.0\n",
      "[711/1000]\n",
      "- Train Loss : 1.9580055084233994e-08 Score : 1.0\n",
      "- Val Loss : 1.125471893459462e-08 Score : 1.0\n",
      "[712/1000]\n",
      "- Train Loss : 1.9520046611216924e-08 Score : 1.0\n",
      "- Val Loss : 1.1263385779614055e-08 Score : 1.0\n",
      "[713/1000]\n",
      "- Train Loss : 1.8205469586723977e-08 Score : 1.0\n",
      "- Val Loss : 1.1230125274153124e-08 Score : 1.0\n",
      "[714/1000]\n",
      "- Train Loss : 1.817590451565364e-08 Score : 1.0\n",
      "- Val Loss : 1.1096318530690041e-08 Score : 1.0\n",
      "[715/1000]\n",
      "- Train Loss : 1.8104314657865617e-08 Score : 1.0\n",
      "- Val Loss : 1.1028411961433449e-08 Score : 1.0\n",
      "[716/1000]\n",
      "- Train Loss : 1.8061186033158556e-08 Score : 1.0\n",
      "- Val Loss : 1.0940553352156712e-08 Score : 1.0\n",
      "[717/1000]\n",
      "- Train Loss : 1.8015682151149693e-08 Score : 1.0\n",
      "- Val Loss : 1.090848478213502e-08 Score : 1.0\n",
      "[718/1000]\n",
      "- Train Loss : 1.667161200179697e-08 Score : 1.0\n",
      "- Val Loss : 1.084699352560392e-08 Score : 1.0\n",
      "[719/1000]\n",
      "- Train Loss : 1.663598361835516e-08 Score : 1.0\n",
      "- Val Loss : 1.0757645441117347e-08 Score : 1.0\n",
      "[720/1000]\n",
      "- Train Loss : 1.6579968110054676e-08 Score : 1.0\n",
      "- Val Loss : 1.06452375803201e-08 Score : 1.0\n",
      "[721/1000]\n",
      "- Train Loss : 1.6518761906947343e-08 Score : 1.0\n",
      "- Val Loss : 1.0601143962674087e-08 Score : 1.0\n",
      "[722/1000]\n",
      "- Train Loss : 1.6496016508404315e-08 Score : 1.0\n",
      "- Val Loss : 1.056348875039248e-08 Score : 1.0\n",
      "[723/1000]\n",
      "- Train Loss : 1.6472160270431585e-08 Score : 1.0\n",
      "- Val Loss : 1.0470174061083526e-08 Score : 1.0\n",
      "[724/1000]\n",
      "- Train Loss : 1.6414450589165607e-08 Score : 1.0\n",
      "- Val Loss : 1.0363637059640496e-08 Score : 1.0\n",
      "[725/1000]\n",
      "- Train Loss : 1.6356982573576294e-08 Score : 1.0\n",
      "- Val Loss : 1.0328461641506692e-08 Score : 1.0\n",
      "[726/1000]\n",
      "- Train Loss : 1.6339601524815544e-08 Score : 1.0\n",
      "- Val Loss : 1.0267487304815859e-08 Score : 1.0\n",
      "[727/1000]\n",
      "- Train Loss : 1.6295753786875016e-08 Score : 1.0\n",
      "- Val Loss : 1.0132678696095354e-08 Score : 1.0\n",
      "[728/1000]\n",
      "- Train Loss : 1.622285097683236e-08 Score : 1.0\n",
      "- Val Loss : 1.009211469948923e-08 Score : 1.0\n",
      "[729/1000]\n",
      "- Train Loss : 1.6202779687198176e-08 Score : 1.0\n",
      "- Val Loss : 1.0077143564046764e-08 Score : 1.0\n",
      "[730/1000]\n",
      "- Train Loss : 1.6194492074800127e-08 Score : 1.0\n",
      "- Val Loss : 1.0037547681918113e-08 Score : 1.0\n",
      "[731/1000]\n",
      "- Train Loss : 1.6170419267925312e-08 Score : 1.0\n",
      "- Val Loss : 9.947626722350833e-09 Score : 1.0\n",
      "[732/1000]\n",
      "- Train Loss : 1.6113504007130097e-08 Score : 1.0\n",
      "- Val Loss : 9.835434688909572e-09 Score : 1.0\n",
      "[733/1000]\n",
      "- Train Loss : 1.605373353745113e-08 Score : 1.0\n",
      "- Val Loss : 9.810348089445142e-09 Score : 1.0\n",
      "[734/1000]\n",
      "- Train Loss : 1.6042740706769903e-08 Score : 1.0\n",
      "- Val Loss : 9.774942633100636e-09 Score : 1.0\n",
      "[735/1000]\n",
      "- Train Loss : 1.6016139937825086e-08 Score : 1.0\n",
      "- Val Loss : 9.682497470464568e-09 Score : 1.0\n",
      "[736/1000]\n",
      "- Train Loss : 1.596760903712458e-08 Score : 1.0\n",
      "- Val Loss : 9.659385291627132e-09 Score : 1.0\n",
      "[737/1000]\n",
      "- Train Loss : 1.5955926859667314e-08 Score : 1.0\n",
      "- Val Loss : 9.599221861833485e-09 Score : 1.0\n",
      "[738/1000]\n",
      "- Train Loss : 1.5913774740728863e-08 Score : 1.0\n",
      "- Val Loss : 9.48171141601506e-09 Score : 1.0\n",
      "[739/1000]\n",
      "- Train Loss : 1.585153784954806e-08 Score : 1.0\n",
      "- Val Loss : 9.46251788036534e-09 Score : 1.0\n",
      "[740/1000]\n",
      "- Train Loss : 1.584434035058549e-08 Score : 1.0\n",
      "- Val Loss : 9.437592041194875e-09 Score : 1.0\n",
      "[741/1000]\n",
      "- Train Loss : 1.5824170240934705e-08 Score : 1.0\n",
      "- Val Loss : 9.357338015547612e-09 Score : 1.0\n",
      "[742/1000]\n",
      "- Train Loss : 1.5782950600242106e-08 Score : 1.0\n",
      "- Val Loss : 9.334905293201246e-09 Score : 1.0\n",
      "[743/1000]\n",
      "- Train Loss : 1.576957851646811e-08 Score : 1.0\n",
      "- Val Loss : 9.257512090243836e-09 Score : 1.0\n",
      "[744/1000]\n",
      "- Train Loss : 1.571838990245313e-08 Score : 1.0\n",
      "- Val Loss : 9.141865930928361e-09 Score : 1.0\n",
      "[745/1000]\n",
      "- Train Loss : 1.5657677692936045e-08 Score : 1.0\n",
      "- Val Loss : 9.130986633465454e-09 Score : 1.0\n",
      "[746/1000]\n",
      "- Train Loss : 1.5655561072255236e-08 Score : 1.0\n",
      "- Val Loss : 9.11696229621839e-09 Score : 1.0\n",
      "[747/1000]\n",
      "- Train Loss : 1.5642019006830955e-08 Score : 1.0\n",
      "- Val Loss : 9.048521043553137e-09 Score : 1.0\n",
      "[748/1000]\n",
      "- Train Loss : 1.5607851150545537e-08 Score : 1.0\n",
      "- Val Loss : 9.037735004824299e-09 Score : 1.0\n",
      "[749/1000]\n",
      "- Train Loss : 1.5601449622800145e-08 Score : 1.0\n",
      "- Val Loss : 8.971786868983145e-09 Score : 1.0\n",
      "[750/1000]\n",
      "- Train Loss : 1.5557122426083692e-08 Score : 1.0\n",
      "- Val Loss : 8.86721807091817e-09 Score : 1.0\n",
      "[751/1000]\n",
      "- Train Loss : 1.5502994817149368e-08 Score : 1.0\n",
      "- Val Loss : 8.866741119106791e-09 Score : 1.0\n",
      "[752/1000]\n",
      "- Train Loss : 1.5507048906100485e-08 Score : 1.0\n",
      "- Val Loss : 8.853757726967615e-09 Score : 1.0\n",
      "[753/1000]\n",
      "- Train Loss : 1.5492526421997866e-08 Score : 1.0\n",
      "- Val Loss : 8.772047976890462e-09 Score : 1.0\n",
      "[754/1000]\n",
      "- Train Loss : 1.545130735082462e-08 Score : 1.0\n",
      "- Val Loss : 8.729894140913075e-09 Score : 1.0\n",
      "[755/1000]\n",
      "- Train Loss : 1.5420694489282998e-08 Score : 1.0\n",
      "- Val Loss : 8.640151705208154e-09 Score : 1.0\n",
      "[756/1000]\n",
      "- Train Loss : 1.5375172913191053e-08 Score : 1.0\n",
      "- Val Loss : 8.650415495026209e-09 Score : 1.0\n",
      "[757/1000]\n",
      "- Train Loss : 1.5385527667234072e-08 Score : 1.0\n",
      "- Val Loss : 8.646511062693207e-09 Score : 1.0\n",
      "[758/1000]\n",
      "- Train Loss : 1.5376379364603588e-08 Score : 1.0\n",
      "- Val Loss : 8.561512387927905e-09 Score : 1.0\n",
      "[759/1000]\n",
      "- Train Loss : 1.533115692678174e-08 Score : 1.0\n",
      "- Val Loss : 8.497863746015355e-09 Score : 1.0\n",
      "[760/1000]\n",
      "- Train Loss : 1.528888553317213e-08 Score : 1.0\n",
      "- Val Loss : 8.405222295948533e-09 Score : 1.0\n",
      "[761/1000]\n",
      "- Train Loss : 1.52421074886266e-08 Score : 1.0\n",
      "- Val Loss : 8.419573482854048e-09 Score : 1.0\n",
      "[762/1000]\n",
      "- Train Loss : 1.5255010990967294e-08 Score : 1.0\n",
      "- Val Loss : 8.422551545095303e-09 Score : 1.0\n",
      "[763/1000]\n",
      "- Train Loss : 1.5250060525341514e-08 Score : 1.0\n",
      "- Val Loss : 8.345507396256835e-09 Score : 1.0\n",
      "[764/1000]\n",
      "- Train Loss : 1.5209573679935556e-08 Score : 1.0\n",
      "- Val Loss : 8.301030973711931e-09 Score : 1.0\n",
      "[765/1000]\n",
      "- Train Loss : 1.5180775782853522e-08 Score : 1.0\n",
      "- Val Loss : 8.24499579721305e-09 Score : 1.0\n",
      "[766/1000]\n",
      "- Train Loss : 1.5154634329991173e-08 Score : 1.0\n",
      "- Val Loss : 8.224611214302513e-09 Score : 1.0\n",
      "[767/1000]\n",
      "- Train Loss : 1.5136965674871943e-08 Score : 1.0\n",
      "- Val Loss : 8.154673381000066e-09 Score : 1.0\n",
      "[768/1000]\n",
      "- Train Loss : 1.510315129470835e-08 Score : 1.0\n",
      "- Val Loss : 8.163270059924344e-09 Score : 1.0\n",
      "[769/1000]\n",
      "- Train Loss : 1.5108820527490263e-08 Score : 1.0\n",
      "- Val Loss : 8.12489453494436e-09 Score : 1.0\n",
      "[770/1000]\n",
      "- Train Loss : 1.5081211331262796e-08 Score : 1.0\n",
      "- Val Loss : 8.050052180408329e-09 Score : 1.0\n",
      "[771/1000]\n",
      "- Train Loss : 1.3720279091193302e-08 Score : 1.0\n",
      "- Val Loss : 8.031810772024528e-09 Score : 1.0\n",
      "[772/1000]\n",
      "- Train Loss : 1.3705170632679326e-08 Score : 1.0\n",
      "- Val Loss : 7.967996040747494e-09 Score : 1.0\n",
      "[773/1000]\n",
      "- Train Loss : 1.3672835581327961e-08 Score : 1.0\n",
      "- Val Loss : 7.953658176518275e-09 Score : 1.0\n",
      "[774/1000]\n",
      "- Train Loss : 1.3664379023487788e-08 Score : 1.0\n",
      "- Val Loss : 7.90026888353168e-09 Score : 1.0\n",
      "[775/1000]\n",
      "- Train Loss : 1.3629844971124275e-08 Score : 1.0\n",
      "- Val Loss : 7.826026049428947e-09 Score : 1.0\n",
      "[776/1000]\n",
      "- Train Loss : 1.3591878694059556e-08 Score : 1.0\n",
      "- Val Loss : 7.828023562694852e-09 Score : 1.0\n",
      "[777/1000]\n",
      "- Train Loss : 1.3596475807596234e-08 Score : 1.0\n",
      "- Val Loss : 7.814453084620254e-09 Score : 1.0\n",
      "[778/1000]\n",
      "- Train Loss : 1.3582621303728781e-08 Score : 1.0\n",
      "- Val Loss : 7.732824158779295e-09 Score : 1.0\n",
      "[779/1000]\n",
      "- Train Loss : 1.353902089027746e-08 Score : 1.0\n",
      "- Val Loss : 7.680037938939677e-09 Score : 1.0\n",
      "[780/1000]\n",
      "- Train Loss : 1.3505168024165259e-08 Score : 1.0\n",
      "- Val Loss : 7.611117069927786e-09 Score : 1.0\n",
      "[781/1000]\n",
      "- Train Loss : 1.3470488996093996e-08 Score : 1.0\n",
      "- Val Loss : 7.619911812639657e-09 Score : 1.0\n",
      "[782/1000]\n",
      "- Train Loss : 1.3479133885682131e-08 Score : 1.0\n",
      "- Val Loss : 7.613762953440073e-09 Score : 1.0\n",
      "[783/1000]\n",
      "- Train Loss : 1.3469716095446193e-08 Score : 1.0\n",
      "- Val Loss : 7.530006840283932e-09 Score : 1.0\n",
      "[784/1000]\n",
      "- Train Loss : 1.3423150144987168e-08 Score : 1.0\n",
      "- Val Loss : 7.496876008872277e-09 Score : 1.0\n",
      "[785/1000]\n",
      "- Train Loss : 1.340856820246642e-08 Score : 1.0\n",
      "- Val Loss : 7.477213515016956e-09 Score : 1.0\n",
      "[786/1000]\n",
      "- Train Loss : 1.3391825410947867e-08 Score : 1.0\n",
      "- Val Loss : 7.408762048299877e-09 Score : 1.0\n",
      "[787/1000]\n",
      "- Train Loss : 1.3357665065340483e-08 Score : 1.0\n",
      "- Val Loss : 7.385075218024895e-09 Score : 1.0\n",
      "[788/1000]\n",
      "- Train Loss : 1.334049888728391e-08 Score : 1.0\n",
      "- Val Loss : 7.3339161410501674e-09 Score : 1.0\n",
      "[789/1000]\n",
      "- Train Loss : 1.3316069618442669e-08 Score : 1.0\n",
      "- Val Loss : 7.310970939755634e-09 Score : 1.0\n",
      "[790/1000]\n",
      "- Train Loss : 1.3297784182343367e-08 Score : 1.0\n",
      "- Val Loss : 7.245127164878795e-09 Score : 1.0\n",
      "[791/1000]\n",
      "- Train Loss : 1.3265296443330105e-08 Score : 1.0\n",
      "- Val Loss : 7.246051758613703e-09 Score : 1.0\n",
      "[792/1000]\n",
      "- Train Loss : 1.3266338702153718e-08 Score : 1.0\n",
      "- Val Loss : 7.207188623681304e-09 Score : 1.0\n",
      "[793/1000]\n",
      "- Train Loss : 1.3239302954525816e-08 Score : 1.0\n",
      "- Val Loss : 7.137525237510545e-09 Score : 1.0\n",
      "[794/1000]\n",
      "- Train Loss : 1.3204834866998136e-08 Score : 1.0\n",
      "- Val Loss : 7.1565291470676584e-09 Score : 1.0\n",
      "[795/1000]\n",
      "- Train Loss : 1.321977974595519e-08 Score : 1.0\n",
      "- Val Loss : 7.164696391726011e-09 Score : 1.0\n",
      "[796/1000]\n",
      "- Train Loss : 1.3218983919876542e-08 Score : 1.0\n",
      "- Val Loss : 7.0687282693882025e-09 Score : 1.0\n",
      "[797/1000]\n",
      "- Train Loss : 1.3160345472913881e-08 Score : 1.0\n",
      "- Val Loss : 6.980361177966188e-09 Score : 1.0\n",
      "[798/1000]\n",
      "- Train Loss : 1.3115687236121676e-08 Score : 1.0\n",
      "- Val Loss : 6.995529933107036e-09 Score : 1.0\n",
      "[799/1000]\n",
      "- Train Loss : 1.3128672729878344e-08 Score : 1.0\n",
      "- Val Loss : 7.005837687756866e-09 Score : 1.0\n",
      "[800/1000]\n",
      "- Train Loss : 1.3129314179059925e-08 Score : 1.0\n",
      "- Val Loss : 6.949645303677698e-09 Score : 1.0\n",
      "[801/1000]\n",
      "- Train Loss : 1.3100540213114667e-08 Score : 1.0\n",
      "- Val Loss : 6.911943462029058e-09 Score : 1.0\n",
      "[802/1000]\n",
      "- Train Loss : 1.3074589095172213e-08 Score : 1.0\n",
      "- Val Loss : 6.8491710081275414e-09 Score : 1.0\n",
      "[803/1000]\n",
      "- Train Loss : 1.304432885670215e-08 Score : 1.0\n",
      "- Val Loss : 6.877317826337048e-09 Score : 1.0\n",
      "[804/1000]\n",
      "- Train Loss : 1.3064726104346919e-08 Score : 1.0\n",
      "- Val Loss : 6.895306547960445e-09 Score : 1.0\n",
      "[805/1000]\n",
      "- Train Loss : 1.3069782471853327e-08 Score : 1.0\n",
      "- Val Loss : 6.80856881984937e-09 Score : 1.0\n",
      "[806/1000]\n",
      "- Train Loss : 1.3016639905406394e-08 Score : 1.0\n",
      "- Val Loss : 6.729067525412802e-09 Score : 1.0\n",
      "[807/1000]\n",
      "- Train Loss : 1.2977240329790455e-08 Score : 1.0\n",
      "- Val Loss : 6.743877012382882e-09 Score : 1.0\n",
      "[808/1000]\n",
      "- Train Loss : 1.2988316992284702e-08 Score : 1.0\n",
      "- Val Loss : 6.738204660905467e-09 Score : 1.0\n",
      "[809/1000]\n",
      "- Train Loss : 1.2980394574981113e-08 Score : 1.0\n",
      "- Val Loss : 6.683559483633417e-09 Score : 1.0\n",
      "[810/1000]\n",
      "- Train Loss : 1.295328533473304e-08 Score : 1.0\n",
      "- Val Loss : 6.656881712530094e-09 Score : 1.0\n",
      "[811/1000]\n",
      "- Train Loss : 1.2933800052662703e-08 Score : 1.0\n",
      "- Val Loss : 6.603645186231688e-09 Score : 1.0\n",
      "[812/1000]\n",
      "- Train Loss : 1.2909207334706108e-08 Score : 1.0\n",
      "- Val Loss : 6.603751767642052e-09 Score : 1.0\n",
      "[813/1000]\n",
      "- Train Loss : 1.2906346169649302e-08 Score : 1.0\n",
      "- Val Loss : 6.56913501373424e-09 Score : 1.0\n",
      "[814/1000]\n",
      "- Train Loss : 1.2890109301221592e-08 Score : 1.0\n",
      "- Val Loss : 6.548153574925664e-09 Score : 1.0\n",
      "[815/1000]\n",
      "- Train Loss : 1.2873932716073023e-08 Score : 1.0\n",
      "- Val Loss : 6.499387694702818e-09 Score : 1.0\n",
      "[816/1000]\n",
      "- Train Loss : 1.2851958398982927e-08 Score : 1.0\n",
      "- Val Loss : 6.495959326002776e-09 Score : 1.0\n",
      "[817/1000]\n",
      "- Train Loss : 1.284563438764518e-08 Score : 1.0\n",
      "- Val Loss : 6.455034284869043e-09 Score : 1.0\n",
      "[818/1000]\n",
      "- Train Loss : 1.150355892482646e-08 Score : 1.0\n",
      "- Val Loss : 6.453044765208915e-09 Score : 1.0\n",
      "[819/1000]\n",
      "- Train Loss : 1.1497539234389753e-08 Score : 1.0\n",
      "- Val Loss : 6.3913425663031376e-09 Score : 1.0\n",
      "[820/1000]\n",
      "- Train Loss : 1.1464986410720077e-08 Score : 1.0\n",
      "- Val Loss : 6.375863836893814e-09 Score : 1.0\n",
      "[821/1000]\n",
      "- Train Loss : 1.1457847993547892e-08 Score : 1.0\n",
      "- Val Loss : 6.345068470636761e-09 Score : 1.0\n",
      "[822/1000]\n",
      "- Train Loss : 1.143592971522999e-08 Score : 1.0\n",
      "- Val Loss : 6.281168918320645e-09 Score : 1.0\n",
      "[823/1000]\n",
      "- Train Loss : 1.1403648556412386e-08 Score : 1.0\n",
      "- Val Loss : 6.289453846619608e-09 Score : 1.0\n",
      "[824/1000]\n",
      "- Train Loss : 1.1411590129424052e-08 Score : 1.0\n",
      "- Val Loss : 6.28773078048539e-09 Score : 1.0\n",
      "[825/1000]\n",
      "- Train Loss : 1.1405901660510356e-08 Score : 1.0\n",
      "- Val Loss : 6.2046465743037515e-09 Score : 1.0\n",
      "[826/1000]\n",
      "- Train Loss : 1.1356424927180808e-08 Score : 1.0\n",
      "- Val Loss : 6.138050512305426e-09 Score : 1.0\n",
      "[827/1000]\n",
      "- Train Loss : 1.1322846432691636e-08 Score : 1.0\n",
      "- Val Loss : 6.148385356397057e-09 Score : 1.0\n",
      "[828/1000]\n",
      "- Train Loss : 1.1332107004408892e-08 Score : 1.0\n",
      "- Val Loss : 6.150701725715635e-09 Score : 1.0\n",
      "[829/1000]\n",
      "- Train Loss : 1.1328883795599662e-08 Score : 1.0\n",
      "- Val Loss : 6.089305060186234e-09 Score : 1.0\n",
      "[830/1000]\n",
      "- Train Loss : 1.1295200465916144e-08 Score : 1.0\n",
      "- Val Loss : 6.070950409053921e-09 Score : 1.0\n",
      "[831/1000]\n",
      "- Train Loss : 1.1288558648852603e-08 Score : 1.0\n",
      "- Val Loss : 6.063807678202693e-09 Score : 1.0\n",
      "[832/1000]\n",
      "- Train Loss : 1.1280209169782468e-08 Score : 1.0\n",
      "- Val Loss : 6.015131504000237e-09 Score : 1.0\n",
      "[833/1000]\n",
      "- Train Loss : 1.1256741571017361e-08 Score : 1.0\n",
      "- Val Loss : 5.997498053744721e-09 Score : 1.0\n",
      "[834/1000]\n",
      "- Train Loss : 1.124263701697644e-08 Score : 1.0\n",
      "- Val Loss : 5.94631988093397e-09 Score : 1.0\n",
      "[835/1000]\n",
      "- Train Loss : 1.1217896522528764e-08 Score : 1.0\n",
      "- Val Loss : 5.929357005385327e-09 Score : 1.0\n",
      "[836/1000]\n",
      "- Train Loss : 1.1204270347832082e-08 Score : 1.0\n",
      "- Val Loss : 5.880054665396983e-09 Score : 1.0\n",
      "[837/1000]\n",
      "- Train Loss : 1.1180639513828569e-08 Score : 1.0\n",
      "- Val Loss : 5.894929433480911e-09 Score : 1.0\n",
      "[838/1000]\n",
      "- Train Loss : 1.1191057279750289e-08 Score : 1.0\n",
      "- Val Loss : 5.886630294327233e-09 Score : 1.0\n",
      "[839/1000]\n",
      "- Train Loss : 1.1182277606292134e-08 Score : 1.0\n",
      "- Val Loss : 5.811589431914399e-09 Score : 1.0\n",
      "[840/1000]\n",
      "- Train Loss : 1.11381865602773e-08 Score : 1.0\n",
      "- Val Loss : 5.760514731889543e-09 Score : 1.0\n",
      "[841/1000]\n",
      "- Train Loss : 1.1113741374215778e-08 Score : 1.0\n",
      "- Val Loss : 5.777345268853651e-09 Score : 1.0\n",
      "[842/1000]\n",
      "- Train Loss : 1.1125395308977435e-08 Score : 1.0\n",
      "- Val Loss : 5.7724593993668805e-09 Score : 1.0\n",
      "[843/1000]\n",
      "- Train Loss : 1.1118686178571494e-08 Score : 1.0\n",
      "- Val Loss : 5.722966101018301e-09 Score : 1.0\n",
      "[844/1000]\n",
      "- Train Loss : 1.1093448063419528e-08 Score : 1.0\n",
      "- Val Loss : 5.693689963948145e-09 Score : 1.0\n",
      "[845/1000]\n",
      "- Train Loss : 1.1073266353576488e-08 Score : 1.0\n",
      "- Val Loss : 5.644732681275855e-09 Score : 1.0\n",
      "[846/1000]\n",
      "- Train Loss : 1.105016563415368e-08 Score : 1.0\n",
      "- Val Loss : 5.666903835077619e-09 Score : 1.0\n",
      "[847/1000]\n",
      "- Train Loss : 9.740743427928572e-09 Score : 1.0\n",
      "- Val Loss : 5.665412139421733e-09 Score : 1.0\n",
      "[848/1000]\n",
      "- Train Loss : 9.735490896204965e-09 Score : 1.0\n",
      "- Val Loss : 5.587177387411657e-09 Score : 1.0\n",
      "[849/1000]\n",
      "- Train Loss : 9.688965465320518e-09 Score : 1.0\n",
      "- Val Loss : 5.527351021328286e-09 Score : 1.0\n",
      "[850/1000]\n",
      "- Train Loss : 9.659364748646235e-09 Score : 1.0\n",
      "- Val Loss : 5.545516490457203e-09 Score : 1.0\n",
      "[851/1000]\n",
      "- Train Loss : 9.673172719929768e-09 Score : 1.0\n",
      "- Val Loss : 5.555643944887834e-09 Score : 1.0\n",
      "[852/1000]\n",
      "- Train Loss : 9.674527326725907e-09 Score : 1.0\n",
      "- Val Loss : 5.477990505653452e-09 Score : 1.0\n",
      "[853/1000]\n",
      "- Train Loss : 9.627171185871126e-09 Score : 1.0\n",
      "- Val Loss : 5.406849634681521e-09 Score : 1.0\n",
      "[854/1000]\n",
      "- Train Loss : 9.591424246873317e-09 Score : 1.0\n",
      "- Val Loss : 5.423263615966789e-09 Score : 1.0\n",
      "[855/1000]\n",
      "- Train Loss : 9.604392152750529e-09 Score : 1.0\n",
      "- Val Loss : 5.435510264106824e-09 Score : 1.0\n",
      "[856/1000]\n",
      "- Train Loss : 9.607109744551235e-09 Score : 1.0\n",
      "- Val Loss : 5.384966250687739e-09 Score : 1.0\n",
      "[857/1000]\n",
      "- Train Loss : 9.579835429535072e-09 Score : 1.0\n",
      "- Val Loss : 5.34003419261353e-09 Score : 1.0\n",
      "[858/1000]\n",
      "- Train Loss : 9.550888151535486e-09 Score : 1.0\n",
      "- Val Loss : 5.284517268222544e-09 Score : 1.0\n",
      "[859/1000]\n",
      "- Train Loss : 9.523978721893361e-09 Score : 1.0\n",
      "- Val Loss : 5.301636907262264e-09 Score : 1.0\n",
      "[860/1000]\n",
      "- Train Loss : 9.535823289947349e-09 Score : 1.0\n",
      "- Val Loss : 5.298902205908007e-09 Score : 1.0\n",
      "[861/1000]\n",
      "- Train Loss : 9.530378357633682e-09 Score : 1.0\n",
      "- Val Loss : 5.236786559947859e-09 Score : 1.0\n",
      "[862/1000]\n",
      "- Train Loss : 9.495014561950401e-09 Score : 1.0\n",
      "- Val Loss : 5.208820930135971e-09 Score : 1.0\n",
      "[863/1000]\n",
      "- Train Loss : 9.48355322818755e-09 Score : 1.0\n",
      "- Val Loss : 5.20861709318865e-09 Score : 1.0\n",
      "[864/1000]\n",
      "- Train Loss : 9.479664952249828e-09 Score : 1.0\n",
      "- Val Loss : 5.158603766375336e-09 Score : 1.0\n",
      "[865/1000]\n",
      "- Train Loss : 9.453013155016551e-09 Score : 1.0\n",
      "- Val Loss : 5.142455794526768e-09 Score : 1.0\n",
      "[866/1000]\n",
      "- Train Loss : 9.445187763808646e-09 Score : 1.0\n",
      "- Val Loss : 5.1306985326959875e-09 Score : 1.0\n",
      "[867/1000]\n",
      "- Train Loss : 9.437962158904504e-09 Score : 1.0\n",
      "- Val Loss : 5.092942512163745e-09 Score : 1.0\n",
      "[868/1000]\n",
      "- Train Loss : 9.41468161144459e-09 Score : 1.0\n",
      "- Val Loss : 5.059650920458125e-09 Score : 1.0\n",
      "[869/1000]\n",
      "- Train Loss : 9.400438778909112e-09 Score : 1.0\n",
      "- Val Loss : 5.060713181848087e-09 Score : 1.0\n",
      "[870/1000]\n",
      "- Train Loss : 9.3974605617956e-09 Score : 1.0\n",
      "- Val Loss : 5.022561477829868e-09 Score : 1.0\n",
      "[871/1000]\n",
      "- Train Loss : 9.379237573211627e-09 Score : 1.0\n",
      "- Val Loss : 5.008458980881869e-09 Score : 1.0\n",
      "[872/1000]\n",
      "- Train Loss : 9.367898636068045e-09 Score : 1.0\n",
      "- Val Loss : 4.96394081395124e-09 Score : 1.0\n",
      "[873/1000]\n",
      "- Train Loss : 8.021316851284438e-09 Score : 1.0\n",
      "- Val Loss : 4.942365627869094e-09 Score : 1.0\n",
      "[874/1000]\n",
      "- Train Loss : 9.329927755938675e-09 Score : 1.0\n",
      "- Val Loss : 4.898949246268103e-09 Score : 1.0\n",
      "[875/1000]\n",
      "- Train Loss : 7.985676975130943e-09 Score : 1.0\n",
      "- Val Loss : 4.897067196196758e-09 Score : 1.0\n",
      "[876/1000]\n",
      "- Train Loss : 7.980759745264987e-09 Score : 1.0\n",
      "- Val Loss : 4.852122259535463e-09 Score : 1.0\n",
      "[877/1000]\n",
      "- Train Loss : 7.958175922432332e-09 Score : 1.0\n",
      "- Val Loss : 4.839768585895854e-09 Score : 1.0\n",
      "[878/1000]\n",
      "- Train Loss : 7.949867904361569e-09 Score : 1.0\n",
      "- Val Loss : 4.806055553530086e-09 Score : 1.0\n",
      "[879/1000]\n",
      "- Train Loss : 7.931240041413431e-09 Score : 1.0\n",
      "- Val Loss : 4.7922736889915996e-09 Score : 1.0\n",
      "[880/1000]\n",
      "- Train Loss : 7.926035184324126e-09 Score : 1.0\n",
      "- Val Loss : 4.781749662896573e-09 Score : 1.0\n",
      "[881/1000]\n",
      "- Train Loss : 7.91646426496326e-09 Score : 1.0\n",
      "- Val Loss : 4.7295927174673125e-09 Score : 1.0\n",
      "[882/1000]\n",
      "- Train Loss : 7.888715039519755e-09 Score : 1.0\n",
      "- Val Loss : 4.698000655167789e-09 Score : 1.0\n",
      "[883/1000]\n",
      "- Train Loss : 7.868885058498321e-09 Score : 1.0\n",
      "- Val Loss : 4.660489771879384e-09 Score : 1.0\n",
      "[884/1000]\n",
      "- Train Loss : 7.850669571870581e-09 Score : 1.0\n",
      "- Val Loss : 4.643845752383413e-09 Score : 1.0\n",
      "[885/1000]\n",
      "- Train Loss : 7.837936614603897e-09 Score : 1.0\n",
      "- Val Loss : 4.599269853855503e-09 Score : 1.0\n",
      "[886/1000]\n",
      "- Train Loss : 7.815863804131176e-09 Score : 1.0\n",
      "- Val Loss : 4.6047397006532265e-09 Score : 1.0\n",
      "[887/1000]\n",
      "- Train Loss : 7.820287109675358e-09 Score : 1.0\n",
      "- Val Loss : 4.591762525762988e-09 Score : 1.0\n",
      "[888/1000]\n",
      "- Train Loss : 7.809635658889163e-09 Score : 1.0\n",
      "- Val Loss : 4.549625121086365e-09 Score : 1.0\n",
      "[889/1000]\n",
      "- Train Loss : 7.788990843604781e-09 Score : 1.0\n",
      "- Val Loss : 4.533680986185118e-09 Score : 1.0\n",
      "[890/1000]\n",
      "- Train Loss : 7.776773753104838e-09 Score : 1.0\n",
      "- Val Loss : 4.4918020414286275e-09 Score : 1.0\n",
      "[891/1000]\n",
      "- Train Loss : 7.756335566299252e-09 Score : 1.0\n",
      "- Val Loss : 4.4772088259037446e-09 Score : 1.0\n",
      "[892/1000]\n",
      "- Train Loss : 7.744978368637185e-09 Score : 1.0\n",
      "- Val Loss : 4.4372479024445965e-09 Score : 1.0\n",
      "[893/1000]\n",
      "- Train Loss : 7.725677612789275e-09 Score : 1.0\n",
      "- Val Loss : 4.42462111394093e-09 Score : 1.0\n",
      "[894/1000]\n",
      "- Train Loss : 7.715480855071577e-09 Score : 1.0\n",
      "- Val Loss : 4.386661256461366e-09 Score : 1.0\n",
      "[895/1000]\n",
      "- Train Loss : 7.697339258797548e-09 Score : 1.0\n",
      "- Val Loss : 4.394308028565774e-09 Score : 1.0\n",
      "[896/1000]\n",
      "- Train Loss : 7.702194698439718e-09 Score : 1.0\n",
      "- Val Loss : 4.375785067622928e-09 Score : 1.0\n",
      "[897/1000]\n",
      "- Train Loss : 7.68878035817517e-09 Score : 1.0\n",
      "- Val Loss : 4.336531578275071e-09 Score : 1.0\n",
      "[898/1000]\n",
      "- Train Loss : 7.669984040374342e-09 Score : 1.0\n",
      "- Val Loss : 4.331357050801898e-09 Score : 1.0\n",
      "[899/1000]\n",
      "- Train Loss : 7.665064043407601e-09 Score : 1.0\n",
      "- Val Loss : 4.290884980662213e-09 Score : 1.0\n",
      "[900/1000]\n",
      "- Train Loss : 7.64191827271016e-09 Score : 1.0\n",
      "- Val Loss : 4.270186870769521e-09 Score : 1.0\n",
      "[901/1000]\n",
      "- Train Loss : 7.633504783307499e-09 Score : 1.0\n",
      "- Val Loss : 4.268602804557986e-09 Score : 1.0\n",
      "[902/1000]\n",
      "- Train Loss : 7.629650396469829e-09 Score : 1.0\n",
      "- Val Loss : 4.226967664777703e-09 Score : 1.0\n",
      "[903/1000]\n",
      "- Train Loss : 7.60736221383738e-09 Score : 1.0\n",
      "- Val Loss : 4.2116155007931866e-09 Score : 1.0\n",
      "[904/1000]\n",
      "- Train Loss : 7.599676204263603e-09 Score : 1.0\n",
      "- Val Loss : 4.188306590435786e-09 Score : 1.0\n",
      "[905/1000]\n",
      "- Train Loss : 7.583865903350808e-09 Score : 1.0\n",
      "- Val Loss : 4.1514667259434646e-09 Score : 1.0\n",
      "[906/1000]\n",
      "- Train Loss : 7.566640027797481e-09 Score : 1.0\n",
      "- Val Loss : 4.1575978215746545e-09 Score : 1.0\n",
      "[907/1000]\n",
      "- Train Loss : 7.569495551027583e-09 Score : 1.0\n",
      "- Val Loss : 4.1280507900864905e-09 Score : 1.0\n",
      "[908/1000]\n",
      "- Train Loss : 7.550341652050474e-09 Score : 1.0\n",
      "- Val Loss : 4.090172645021539e-09 Score : 1.0\n",
      "[909/1000]\n",
      "- Train Loss : 7.532599139079501e-09 Score : 1.0\n",
      "- Val Loss : 4.104315998176844e-09 Score : 1.0\n",
      "[910/1000]\n",
      "- Train Loss : 7.541418186206911e-09 Score : 1.0\n",
      "- Val Loss : 4.0947685242542775e-09 Score : 1.0\n",
      "[911/1000]\n",
      "- Train Loss : 7.533352308693367e-09 Score : 1.0\n",
      "- Val Loss : 4.058771541082251e-09 Score : 1.0\n",
      "[912/1000]\n",
      "- Train Loss : 7.515416208027331e-09 Score : 1.0\n",
      "- Val Loss : 4.041894374751109e-09 Score : 1.0\n",
      "[913/1000]\n",
      "- Train Loss : 7.5033746459078e-09 Score : 1.0\n",
      "- Val Loss : 4.0106287180208255e-09 Score : 1.0\n",
      "[914/1000]\n",
      "- Train Loss : 7.489400120522546e-09 Score : 1.0\n",
      "- Val Loss : 4.010698440026772e-09 Score : 1.0\n",
      "[915/1000]\n",
      "- Train Loss : 7.486793080456466e-09 Score : 1.0\n",
      "- Val Loss : 3.986618590801072e-09 Score : 1.0\n",
      "[916/1000]\n",
      "- Train Loss : 7.476842760832466e-09 Score : 1.0\n",
      "- Val Loss : 3.990012764631956e-09 Score : 1.0\n",
      "[917/1000]\n",
      "- Train Loss : 7.476086262211967e-09 Score : 1.0\n",
      "- Val Loss : 3.949048643647757e-09 Score : 1.0\n",
      "[918/1000]\n",
      "- Train Loss : 7.453108091437665e-09 Score : 1.0\n",
      "- Val Loss : 3.9356411463131735e-09 Score : 1.0\n",
      "[919/1000]\n",
      "- Train Loss : 7.449145874068262e-09 Score : 1.0\n",
      "- Val Loss : 3.9443301957931e-09 Score : 1.0\n",
      "[920/1000]\n",
      "- Train Loss : 7.45138402717488e-09 Score : 1.0\n",
      "- Val Loss : 3.91343890626672e-09 Score : 1.0\n",
      "[921/1000]\n",
      "- Train Loss : 7.43545388786603e-09 Score : 1.0\n",
      "- Val Loss : 3.89057364102996e-09 Score : 1.0\n",
      "[922/1000]\n",
      "- Train Loss : 7.4202726987935e-09 Score : 1.0\n",
      "- Val Loss : 3.8601917218272774e-09 Score : 1.0\n",
      "[923/1000]\n",
      "- Train Loss : 7.406977660256978e-09 Score : 1.0\n",
      "- Val Loss : 3.863964259664954e-09 Score : 1.0\n",
      "[924/1000]\n",
      "- Train Loss : 7.406602174240759e-09 Score : 1.0\n",
      "- Val Loss : 3.838041440218376e-09 Score : 1.0\n",
      "[925/1000]\n",
      "- Train Loss : 7.394524614488884e-09 Score : 1.0\n",
      "- Val Loss : 3.8298799687197516e-09 Score : 1.0\n",
      "[926/1000]\n",
      "- Train Loss : 7.387565534800509e-09 Score : 1.0\n",
      "- Val Loss : 3.806413850782064e-09 Score : 1.0\n",
      "[927/1000]\n",
      "- Train Loss : 7.378177813658969e-09 Score : 1.0\n",
      "- Val Loss : 3.814105031807458e-09 Score : 1.0\n",
      "[928/1000]\n",
      "- Train Loss : 7.380032837208379e-09 Score : 1.0\n",
      "- Val Loss : 3.766933875937184e-09 Score : 1.0\n",
      "[929/1000]\n",
      "- Train Loss : 7.351538446731319e-09 Score : 1.0\n",
      "- Val Loss : 3.729996311818695e-09 Score : 1.0\n",
      "[930/1000]\n",
      "- Train Loss : 7.334769230182103e-09 Score : 1.0\n",
      "- Val Loss : 3.734054843107515e-09 Score : 1.0\n",
      "[931/1000]\n",
      "- Train Loss : 7.334728096033547e-09 Score : 1.0\n",
      "- Val Loss : 3.7175029721225883e-09 Score : 1.0\n",
      "[932/1000]\n",
      "- Train Loss : 7.329277813591746e-09 Score : 1.0\n",
      "- Val Loss : 3.729828002008162e-09 Score : 1.0\n",
      "[933/1000]\n",
      "- Train Loss : 7.333803049776746e-09 Score : 1.0\n",
      "- Val Loss : 3.6862368713030946e-09 Score : 1.0\n",
      "[934/1000]\n",
      "- Train Loss : 7.307384185730578e-09 Score : 1.0\n",
      "- Val Loss : 3.652361302286522e-09 Score : 1.0\n",
      "[935/1000]\n",
      "- Train Loss : 7.29240466263508e-09 Score : 1.0\n",
      "- Val Loss : 3.6780625212173845e-09 Score : 1.0\n",
      "[936/1000]\n",
      "- Train Loss : 7.3081665101793385e-09 Score : 1.0\n",
      "- Val Loss : 3.682261606741122e-09 Score : 1.0\n",
      "[937/1000]\n",
      "- Train Loss : 7.308231483994151e-09 Score : 1.0\n",
      "- Val Loss : 3.641316581592946e-09 Score : 1.0\n",
      "[938/1000]\n",
      "- Train Loss : 7.2843221600979366e-09 Score : 1.0\n",
      "- Val Loss : 3.620456157094054e-09 Score : 1.0\n",
      "[939/1000]\n",
      "- Train Loss : 7.2766119191115655e-09 Score : 1.0\n",
      "- Val Loss : 3.633366718602815e-09 Score : 1.0\n",
      "[940/1000]\n",
      "- Train Loss : 7.2815829819765365e-09 Score : 1.0\n",
      "- Val Loss : 3.5917662177809007e-09 Score : 1.0\n",
      "[941/1000]\n",
      "- Train Loss : 7.25637242919536e-09 Score : 1.0\n",
      "- Val Loss : 3.560544525882392e-09 Score : 1.0\n",
      "[942/1000]\n",
      "- Train Loss : 7.24297489790738e-09 Score : 1.0\n",
      "- Val Loss : 3.570774564920498e-09 Score : 1.0\n",
      "[943/1000]\n",
      "- Train Loss : 7.246543482607402e-09 Score : 1.0\n",
      "- Val Loss : 3.55344220714926e-09 Score : 1.0\n",
      "[944/1000]\n",
      "- Train Loss : 7.239599106844797e-09 Score : 1.0\n",
      "- Val Loss : 3.554982086484415e-09 Score : 1.0\n",
      "[945/1000]\n",
      "- Train Loss : 7.2383452733036565e-09 Score : 1.0\n",
      "- Val Loss : 3.535884030014813e-09 Score : 1.0\n",
      "[946/1000]\n",
      "- Train Loss : 5.906167318584976e-09 Score : 1.0\n",
      "- Val Loss : 3.537558024291343e-09 Score : 1.0\n",
      "[947/1000]\n",
      "- Train Loss : 5.90447007970053e-09 Score : 1.0\n",
      "- Val Loss : 3.488832112097384e-09 Score : 1.0\n",
      "[948/1000]\n",
      "- Train Loss : 5.875648050265839e-09 Score : 1.0\n",
      "- Val Loss : 3.449623919848932e-09 Score : 1.0\n",
      "[949/1000]\n",
      "- Train Loss : 5.855927775397711e-09 Score : 1.0\n",
      "- Val Loss : 3.456824604342046e-09 Score : 1.0\n",
      "[950/1000]\n",
      "- Train Loss : 5.861949508953179e-09 Score : 1.0\n",
      "- Val Loss : 3.4590683650748133e-09 Score : 1.0\n",
      "[951/1000]\n",
      "- Train Loss : 5.860670590539588e-09 Score : 1.0\n",
      "- Val Loss : 3.4227916057005814e-09 Score : 1.0\n",
      "[952/1000]\n",
      "- Train Loss : 5.84087954692855e-09 Score : 1.0\n",
      "- Val Loss : 3.3936669030509847e-09 Score : 1.0\n",
      "[953/1000]\n",
      "- Train Loss : 5.823058560500581e-09 Score : 1.0\n",
      "- Val Loss : 3.3641416319341033e-09 Score : 1.0\n",
      "[954/1000]\n",
      "- Train Loss : 5.8088070333744355e-09 Score : 1.0\n",
      "- Val Loss : 3.373164858544442e-09 Score : 1.0\n",
      "[955/1000]\n",
      "- Train Loss : 5.815174595378728e-09 Score : 1.0\n",
      "- Val Loss : 3.36951289092724e-09 Score : 1.0\n",
      "[956/1000]\n",
      "- Train Loss : 5.810734895059348e-09 Score : 1.0\n",
      "- Val Loss : 3.322797814675482e-09 Score : 1.0\n",
      "[957/1000]\n",
      "- Train Loss : 5.783268333602583e-09 Score : 1.0\n",
      "- Val Loss : 3.288189054373447e-09 Score : 1.0\n",
      "[958/1000]\n",
      "- Train Loss : 5.766290511700564e-09 Score : 1.0\n",
      "- Val Loss : 3.30066463050116e-09 Score : 1.0\n",
      "[959/1000]\n",
      "- Train Loss : 5.775410913135032e-09 Score : 1.0\n",
      "- Val Loss : 3.308507690036322e-09 Score : 1.0\n",
      "[960/1000]\n",
      "- Train Loss : 5.7774420348251406e-09 Score : 1.0\n",
      "- Val Loss : 3.263727288427276e-09 Score : 1.0\n",
      "[961/1000]\n",
      "- Train Loss : 5.75037106639575e-09 Score : 1.0\n",
      "- Val Loss : 3.2227736035395083e-09 Score : 1.0\n",
      "[962/1000]\n",
      "- Train Loss : 5.729950696537443e-09 Score : 1.0\n",
      "- Val Loss : 3.225640865522905e-09 Score : 1.0\n",
      "[963/1000]\n",
      "- Train Loss : 5.732116984832987e-09 Score : 1.0\n",
      "- Val Loss : 3.2122993154359847e-09 Score : 1.0\n",
      "[964/1000]\n",
      "- Train Loss : 5.722540036440681e-09 Score : 1.0\n",
      "- Val Loss : 3.184527752608801e-09 Score : 1.0\n",
      "[965/1000]\n",
      "- Train Loss : 5.709465154792635e-09 Score : 1.0\n",
      "- Val Loss : 3.1831015601113677e-09 Score : 1.0\n",
      "[966/1000]\n",
      "- Train Loss : 5.707281422936991e-09 Score : 1.0\n",
      "- Val Loss : 3.1689262325329537e-09 Score : 1.0\n",
      "[967/1000]\n",
      "- Train Loss : 5.7017779097343416e-09 Score : 1.0\n",
      "- Val Loss : 3.1693563329326935e-09 Score : 1.0\n",
      "[968/1000]\n",
      "- Train Loss : 5.699875728871673e-09 Score : 1.0\n",
      "- Val Loss : 3.1390752219806473e-09 Score : 1.0\n",
      "[969/1000]\n",
      "- Train Loss : 5.6837834621019645e-09 Score : 1.0\n",
      "- Val Loss : 3.119669855777829e-09 Score : 1.0\n",
      "[970/1000]\n",
      "- Train Loss : 5.671906122820597e-09 Score : 1.0\n",
      "- Val Loss : 3.1022300284178073e-09 Score : 1.0\n",
      "[971/1000]\n",
      "- Train Loss : 5.6646602945944994e-09 Score : 1.0\n",
      "- Val Loss : 3.10285863669435e-09 Score : 1.0\n",
      "[972/1000]\n",
      "- Train Loss : 5.66296354298665e-09 Score : 1.0\n",
      "- Val Loss : 3.077988308675117e-09 Score : 1.0\n",
      "[973/1000]\n",
      "- Train Loss : 5.650687011295348e-09 Score : 1.0\n",
      "- Val Loss : 3.065608211727522e-09 Score : 1.0\n",
      "[974/1000]\n",
      "- Train Loss : 5.641829715286155e-09 Score : 1.0\n",
      "- Val Loss : 3.0414251117605318e-09 Score : 1.0\n",
      "[975/1000]\n",
      "- Train Loss : 5.6309097567241465e-09 Score : 1.0\n",
      "- Val Loss : 3.0407647511054847e-09 Score : 1.0\n",
      "[976/1000]\n",
      "- Train Loss : 5.628586190760668e-09 Score : 1.0\n",
      "- Val Loss : 3.021604078057294e-09 Score : 1.0\n",
      "[977/1000]\n",
      "- Train Loss : 5.6204837521428655e-09 Score : 1.0\n",
      "- Val Loss : 3.0233464620721406e-09 Score : 1.0\n",
      "[978/1000]\n",
      "- Train Loss : 5.619504525364112e-09 Score : 1.0\n",
      "- Val Loss : 2.9827849079566704e-09 Score : 1.0\n",
      "[979/1000]\n",
      "- Train Loss : 5.595158163716231e-09 Score : 1.0\n",
      "- Val Loss : 2.9497309039783204e-09 Score : 1.0\n",
      "[980/1000]\n",
      "- Train Loss : 5.579434863835779e-09 Score : 1.0\n",
      "- Val Loss : 2.9654556588099013e-09 Score : 1.0\n",
      "[981/1000]\n",
      "- Train Loss : 5.589783674366815e-09 Score : 1.0\n",
      "- Val Loss : 2.9713345117698964e-09 Score : 1.0\n",
      "[982/1000]\n",
      "- Train Loss : 5.591152807653015e-09 Score : 1.0\n",
      "- Val Loss : 2.933445042430094e-09 Score : 1.0\n",
      "[983/1000]\n",
      "- Train Loss : 5.568346246554931e-09 Score : 1.0\n",
      "- Val Loss : 2.9024487258055842e-09 Score : 1.0\n",
      "[984/1000]\n",
      "- Train Loss : 5.553807137211871e-09 Score : 1.0\n",
      "- Val Loss : 2.9249096478167758e-09 Score : 1.0\n",
      "[985/1000]\n",
      "- Train Loss : 5.568896747750486e-09 Score : 1.0\n",
      "- Val Loss : 2.945132804299533e-09 Score : 1.0\n",
      "[986/1000]\n",
      "- Train Loss : 5.578242267659644e-09 Score : 1.0\n",
      "- Val Loss : 2.9128905953967887e-09 Score : 1.0\n",
      "[987/1000]\n",
      "- Train Loss : 5.55854513285825e-09 Score : 1.0\n",
      "- Val Loss : 2.8756317327349734e-09 Score : 1.0\n",
      "[988/1000]\n",
      "- Train Loss : 5.5389030500484675e-09 Score : 1.0\n",
      "- Val Loss : 2.863175030398679e-09 Score : 1.0\n",
      "[989/1000]\n",
      "- Train Loss : 5.531943829474044e-09 Score : 1.0\n",
      "- Val Loss : 2.857654113341823e-09 Score : 1.0\n",
      "[990/1000]\n",
      "- Train Loss : 5.530642866447501e-09 Score : 1.0\n",
      "- Val Loss : 2.857628134123047e-09 Score : 1.0\n",
      "[991/1000]\n",
      "- Train Loss : 5.52888783737067e-09 Score : 1.0\n",
      "- Val Loss : 2.824629863340533e-09 Score : 1.0\n",
      "[992/1000]\n",
      "- Train Loss : 5.509847289658662e-09 Score : 1.0\n",
      "- Val Loss : 2.8084539138717446e-09 Score : 1.0\n",
      "[993/1000]\n",
      "- Train Loss : 5.503647090293838e-09 Score : 1.0\n",
      "- Val Loss : 2.816121114079806e-09 Score : 1.0\n",
      "[994/1000]\n",
      "- Train Loss : 5.506209453351876e-09 Score : 1.0\n",
      "- Val Loss : 2.7963589221968732e-09 Score : 1.0\n",
      "[995/1000]\n",
      "- Train Loss : 5.496349387583575e-09 Score : 1.0\n",
      "- Val Loss : 2.7837412375220083e-09 Score : 1.0\n",
      "[996/1000]\n",
      "- Train Loss : 5.487704785730652e-09 Score : 1.0\n",
      "- Val Loss : 2.765551121441945e-09 Score : 1.0\n",
      "[997/1000]\n",
      "- Train Loss : 5.48043697190834e-09 Score : 1.0\n",
      "- Val Loss : 2.773455687332671e-09 Score : 1.0\n",
      "[998/1000]\n",
      "- Train Loss : 5.48317794793811e-09 Score : 1.0\n",
      "- Val Loss : 2.744228400075599e-09 Score : 1.0\n",
      "[999/1000]\n",
      "- Train Loss : 5.466119245626179e-09 Score : 1.0\n",
      "- Val Loss : 2.728986814304335e-09 Score : 1.0\n"
     ]
    }
   ],
   "source": [
    "## 학습의 효과 확인 - 손실값과 성능평가값 저장 필요\n",
    "LOSS_HISTORY, SCORE_HISTORY = [[],[]], [[],[]]\n",
    "CNT = len(trainDL)\n",
    "print(f'CNT =>{CNT}')\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # 학습 모드로 모델 성정\n",
    "    model.train()\n",
    "\n",
    "    # 배치크기만큼 데이터 로딩해서 학습 진행\n",
    "    loss_total, score_total = 0,0\n",
    "    for featureTS, targetTS in trainDL :\n",
    "        # 학습 진행\n",
    "        pre_y = model(featureTS)\n",
    "\n",
    "        # 손실계산\n",
    "        loss = reqLoss(pre_y, targetTS)\n",
    "        loss_total += loss.item()\n",
    "\n",
    "        # 성능평가 계산\n",
    "        score = BinaryF1Score()(pre_y, targetTS)\n",
    "        # 방법2 : score = F1Score(task='binary')(pre_y, targetTS)\n",
    "        score_total += score.item()\n",
    "\n",
    "        # 최적화 진행\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 에포크 당 검증기능\n",
    "    # 모델 검증 모드 설정\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 검증용 데이터셋 생성\n",
    "        val_feaure_TS = torch.FloatTensor(valDS.featureDF.values)\n",
    "        val_target_TS = torch.FloatTensor(valDS.targetDF.values)\n",
    "        # 평가\n",
    "        pre_val = model(val_feaure_TS)\n",
    "        # 손실 계산\n",
    "        loss_val = reqLoss(pre_val, val_target_TS)\n",
    "        # 성능 평가\n",
    "        score_val = BinaryF1Score()(pre_val, val_target_TS)\n",
    "\n",
    "    # 에포크 당 손실과 성능평가값 저장\n",
    "    LOSS_HISTORY[0].append(loss_total/CNT)\n",
    "    SCORE_HISTORY[0].append(score_total/CNT)\n",
    "    LOSS_HISTORY[1].append(loss_val)\n",
    "    SCORE_HISTORY[1].append(score_val)\n",
    "\n",
    "    print(f'[{epoch}/{EPOCH}]\\n- Train Loss : {LOSS_HISTORY[0][-1]} Score : {SCORE_HISTORY[0][-1]}')\n",
    "    print(f'- Val Loss : {LOSS_HISTORY[1][-1]} Score : {SCORE_HISTORY[1][-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 결과 체크 (시각화) => 학습과 검증의 Loss 변화, 성능 변화"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
