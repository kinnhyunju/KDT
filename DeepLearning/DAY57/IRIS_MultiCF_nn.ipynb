{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DNN 기반 분류 모델 구현\n",
    "- 데이터 : iris.csv\n",
    "- 피처/속성 : 4개 Sepal.Length, Sepal.Width, Petal.Length, Petal.Width\n",
    "- 타겟/라벨 : 3개\n",
    "- 학습-방법 : 지도학습 > 분류> 다중분류\n",
    "- 학습 알고리즘 : 인공신경망(ANN) -> 심층 신경망 (MLP, DNN) : 은닉층이 많은 구성\n",
    "- 프레임워크 : Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] 모듈 로딩 및 데이터 준비 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 로딩\n",
    "# 모델 관련 모듈\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchmetrics.classification import F1Score, MulticlassF1Score\n",
    "from torchinfo import summary\n",
    "\n",
    "# 데이터 및 시각화 관련 모듈\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch v.2.4.1\n",
      "Pandas v.2.0.3\n"
     ]
    }
   ],
   "source": [
    "# 활용 패키지 버전 체크 ==> 사용자 정의 함수로 구현하기~~~\n",
    "print(f'Pytorch v.{torch.__version__}')\n",
    "print(f'Pandas v.{pd.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28adca74e70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.length  sepal.width  petal.length  petal.width variety\n",
       "0           5.1          3.5           1.4          0.2  Setosa"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 데이터 로딩\n",
    "DATA_FILE = '../../ML/Data/iris.csv'\n",
    "\n",
    "### CSV => DataFrame\n",
    "irisDF = pd.read_csv(DATA_FILE)\n",
    "\n",
    "### 확인\n",
    "irisDF.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Setosa', 'Versicolor', 'Virginica'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 타겟 변경 => 정수화\n",
    "irisDF['variety'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels => {'Setosa': 0, 'Versicolor': 1, 'Virginica': 2}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.length  sepal.width  petal.length  petal.width  variety\n",
       "0           5.1          3.5           1.4          0.2        0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dict(zip(irisDF['variety'].unique().tolist(),range(3)))\n",
    "print(f'labels => {labels}')\n",
    "\n",
    "irisDF['variety'] = irisDF['variety'].replace(labels)\n",
    "irisDF.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] 모델 클래스 설계 및 정의 <hr>\n",
    "- 클래스 목적 : iris 데이터를 학습 및 추론 목적\n",
    "- 클래스 이름 : IrisMCFModel\n",
    "- 부모 클래스 : nn.Module\n",
    "- 매 개 변 수 : 층별 입출력 개수 고정하므로 필요 없음\n",
    "- 속성 / 필드 :\n",
    "- 기능 / 역할 : __init__() : 모델 구조 설정, forward() : 순방향 학습 <=오버라이딩(상속관계에서만 가능)\n",
    "- 클래스 구조\n",
    "    * 입력층 : 입력 4개(피처 개수) / 출력 10개(퍼셉트론/뉴런 개수 10개)\n",
    "    * 은닉층 : 입력 10개          / 출력 5개\n",
    "    * 출력층 : 입력 5개          / 출력 3개(다중 분류 : 'Setosa', 'Versicolor', 'Virginica')\n",
    "- - -\n",
    "- 손실함수 / 활성화 함수\n",
    "    * 클래스 형태 ==> nn.MESLoss, nn.ReLU ==> __init__() 메서드\n",
    "    * 함수 형태 ==> torch.nn.fuctional 아래에 ==> forward() 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisMCFModel(nn.Module):\n",
    "    # 모델 구조 구성 및 인스턴스 생성 메서드\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.in_layer = nn.Linear(4,10)\n",
    "        self.hidden_layer = nn.Linear(10,5)\n",
    "        self.out_layer = nn.Linear(5,3)\n",
    "\n",
    "    #순방향 학습 진행 메서드\n",
    "    def forward(self,x) : \n",
    "        # 입력층\n",
    "        y = self.in_layer(x)    # \n",
    "        y=F.relu(y)             # relu 값의 범위 : 0<=y / 시그모이드 : 0~1\n",
    "        # 은닉층 : 10개 숫자의 값(>=0)\n",
    "        y = self.hidden_layer(y)\n",
    "        y = F.relu(y)\n",
    "        # 출력층 : 5개 숫자값 / 다중 분류이므로 소프트맥스 함수 적용 => 손실함수 CrossEntropy는 소프트맥스 자동으로 진행 => 그대로 반환하기\n",
    "        return self.out_layer(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IrisMCFModel(\n",
      "  (in_layer): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (hidden_layer): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (out_layer): Linear(in_features=5, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = IrisMCFModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "IrisMCFModel                             [50000, 3]                --\n",
       "├─Linear: 1-1                            [50000, 10]               50\n",
       "├─Linear: 1-2                            [50000, 5]                55\n",
       "├─Linear: 1-3                            [50000, 3]                18\n",
       "==========================================================================================\n",
       "Total params: 123\n",
       "Trainable params: 123\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 6.15\n",
       "==========================================================================================\n",
       "Input size (MB): 0.80\n",
       "Forward/backward pass size (MB): 7.20\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 8.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### [테스트] 모델 사용 메모리 정보 확인\n",
    "summary(model, input_size=(50000,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] 데이터셋 클래스 설계 및 정의 <hr>\n",
    "- 데이터셋 : iris.csv\n",
    "- 피처 개수 : 4개\n",
    "- 타겟 개수 : 1개\n",
    "- 클래스 이름 : IrisDataset\n",
    "- 부모 클래스 : utils.data.Dataset\n",
    "- 속성 / 필드 : featureDF, targetDF, n_rows, n_features\n",
    "- 필수 메서드\n",
    "    * _ _init_ _(self) : 데이터셋 저장 및 전처리, 개발자가 필요한 속성 설정\n",
    "    * _ _len_ _(self) : 데이터의 개수 반환\n",
    "    * _ _getItem_ _(self, index) : 특정 인덱스의 피처와 타겟 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisDataset(Dataset):\n",
    "\n",
    "    def __init__(self, featureDF, targetDF):\n",
    "        self.featureDF = featureDF\n",
    "        self.targetDF = targetDF\n",
    "        self.n_rows = featureDF.shape[0]\n",
    "        self.n_features = featureDF.shape[1]\n",
    "\n",
    "    def __len__(self) : \n",
    "        return self.n_rows\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 텐서화\n",
    "        feaureTS = torch.FloatTensor(self.featureDF.iloc[index].values)\n",
    "        targetTS = torch.FloatTensor(self.targetDF.iloc[index].values)\n",
    "        \n",
    "        # 피처와 타겟 반환\n",
    "        return feaureTS, targetTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4]) torch.Size([1, 1]) tensor([[5.1000, 3.5000, 1.4000, 0.2000]]) tensor([[0.]])\n"
     ]
    }
   ],
   "source": [
    "## [테스트] 데이터셋 인스턴스 생성\n",
    "# 피처와 타겟 데이터 추출\n",
    "featureDF = irisDF[irisDF.columns[:-1]]     # 2D (150,4)\n",
    "targetDF = irisDF[irisDF.columns[-1:]]      # 2D (150,1)\n",
    "\n",
    "# 커스텀데이터셋 인스턴스 생성\n",
    "irisDS = IrisDataset(featureDF, targetDF)\n",
    "\n",
    "# 데이터로더 인스턴스 생성\n",
    "irisDL = DataLoader(irisDS)\n",
    "for feature, label in irisDL:\n",
    "    print(feature.shape, label.shape, feature, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] 학습 준비 <hr>\n",
    "- 학습 횟수 : EPOCH   <- 처음부터 끝까지 공부하는 단위\n",
    "- 배치 크기 : BATCH_SIZE   <- 한번에 학습할 데이터셋 양\n",
    "- 위치 지정 : DEVICE  <- 텐서 저장 및 실행 위치 (GPU/CPU)\n",
    "- 학 습 율  : LR 가중치와 절편 업데이트 시 경사 하강법으로 업데이트 간격 설정 0.001~0.1 (하이퍼파라미터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 학습 진행 관련 설정\n",
    "EPOCH = 1000\n",
    "BATCH_SIZE = 10\n",
    "BATCH_CNT = irisDF.shape[0]//BATCH_SIZE # 선택사항 - 코드에 넣을 수도 있음\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 인스턴스/객체 : 모델, 데이터셋, 최적화 (, 손실함수, 성능지표)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스\n",
    "model = IrisMCFModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 4) (38, 4) (28, 4)\n",
      "(84, 1) (38, 1) (28, 1)\n"
     ]
    }
   ],
   "source": [
    "### DS과 DL 인스턴스\n",
    "\n",
    "# 학습/검증/테스트용 데이터 분리\n",
    "X_train,X_test, y_train, y_test = train_test_split(featureDF, targetDF, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=1)\n",
    "print(f'{X_train.shape} {X_test.shape} {X_val.shape}')\n",
    "print(f'{y_train.shape} {y_test.shape} {y_val.shape}')\n",
    "\n",
    "## 학습/검증/테스트용 데이터셋\n",
    "# irisDS = IrisDataset(X_train, y_train)\n",
    "trainDS = IrisDataset(X_train, y_train)\n",
    "valDS = IrisDataset(X_val, y_val)\n",
    "testDS = IrisDataset(X_test, y_test)\n",
    "\n",
    "# 학습용 데이터로더 인스턴스 (검증용은 필요 없음, 테스트는 양이 많을 때 개발자가 선택하여 인스턴스 생성)\n",
    "# irisDL = DataLoader(irisDS, batch_size = BATCH_SIZE)\n",
    "trainDL = DataLoader(trainDS, batch_size = BATCH_SIZE, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적화, 손실함수 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화 인스턴스 => W,b 텐서, 즉 model.parameters() 전달 - 최적화하는 이유 : 오차를 줄이기 위해서!\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# 손실함수 인스턴스 => 분류 => 다중분류 : CrossEntropyLoss\n",
    "#                            예측값은 확률값으로 전달 ==> sigmoid() AF 처리 후 전달\n",
    "crossLoss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainDL), trainDL.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNT =>8\n",
      "[0/1000]\n",
      "- Train Loss : 1.1644433736801147 Score : 0.14747752528637648\n",
      "- Val Loss : 1.0627670288085938 Score : 0.20000000298023224\n",
      "[1/1000]\n",
      "- Train Loss : 1.1466117948293686 Score : 0.14747752528637648\n",
      "- Val Loss : 1.056637167930603 Score : 0.20000000298023224\n",
      "[2/1000]\n",
      "- Train Loss : 1.1305239647626877 Score : 0.14747752528637648\n",
      "- Val Loss : 1.0484484434127808 Score : 0.20000000298023224\n",
      "[3/1000]\n",
      "- Train Loss : 1.114012822508812 Score : 0.14747752528637648\n",
      "- Val Loss : 1.0389717817306519 Score : 0.20000000298023224\n",
      "[4/1000]\n",
      "- Train Loss : 1.0971026867628098 Score : 0.14747752528637648\n",
      "- Val Loss : 1.0284874439239502 Score : 0.20000000298023224\n",
      "[5/1000]\n",
      "- Train Loss : 1.0797532051801682 Score : 0.14747752528637648\n",
      "- Val Loss : 1.016173243522644 Score : 0.20000000298023224\n",
      "[6/1000]\n",
      "- Train Loss : 1.0620037764310837 Score : 0.18864053022116423\n",
      "- Val Loss : 1.002556324005127 Score : 0.31309041380882263\n",
      "[7/1000]\n",
      "- Train Loss : 1.0438390001654625 Score : 0.2687257193028927\n",
      "- Val Loss : 0.986355185508728 Score : 0.5206349492073059\n",
      "[8/1000]\n",
      "- Train Loss : 1.0252382531762123 Score : 0.42991199530661106\n",
      "- Val Loss : 0.9680952429771423 Score : 0.6010256409645081\n",
      "[9/1000]\n",
      "- Train Loss : 1.0067457109689713 Score : 0.48953478038311005\n",
      "- Val Loss : 0.9501009583473206 Score : 0.6020512580871582\n",
      "[10/1000]\n",
      "- Train Loss : 0.9883095324039459 Score : 0.5325397029519081\n",
      "- Val Loss : 0.9325204491615295 Score : 0.6020512580871582\n",
      "[11/1000]\n",
      "- Train Loss : 0.9699213355779648 Score : 0.5487561300396919\n",
      "- Val Loss : 0.9150017499923706 Score : 0.604938268661499\n",
      "[12/1000]\n",
      "- Train Loss : 0.9515349343419075 Score : 0.5487561300396919\n",
      "- Val Loss : 0.8959922790527344 Score : 0.604938268661499\n",
      "[13/1000]\n",
      "- Train Loss : 0.9338186606764793 Score : 0.5487561300396919\n",
      "- Val Loss : 0.8763360977172852 Score : 0.604938268661499\n",
      "[14/1000]\n",
      "- Train Loss : 0.916620746254921 Score : 0.5487561300396919\n",
      "- Val Loss : 0.8568593263626099 Score : 0.604938268661499\n",
      "[15/1000]\n",
      "- Train Loss : 0.8991401121020317 Score : 0.5487561300396919\n",
      "- Val Loss : 0.8364143371582031 Score : 0.604938268661499\n",
      "[16/1000]\n",
      "- Train Loss : 0.8811757415533066 Score : 0.5487561300396919\n",
      "- Val Loss : 0.8145627379417419 Score : 0.604938268661499\n",
      "[17/1000]\n",
      "- Train Loss : 0.862736165523529 Score : 0.5487561300396919\n",
      "- Val Loss : 0.7919805645942688 Score : 0.604938268661499\n",
      "[18/1000]\n",
      "- Train Loss : 0.8437392339110374 Score : 0.5487561300396919\n",
      "- Val Loss : 0.7685145735740662 Score : 0.604938268661499\n",
      "[19/1000]\n",
      "- Train Loss : 0.8241662159562111 Score : 0.5647817701101303\n",
      "- Val Loss : 0.7436363101005554 Score : 0.604938268661499\n",
      "[20/1000]\n",
      "- Train Loss : 0.8043141812086105 Score : 0.5647817701101303\n",
      "- Val Loss : 0.7186701893806458 Score : 0.7264957427978516\n",
      "[21/1000]\n",
      "- Train Loss : 0.7841441109776497 Score : 0.5647817701101303\n",
      "- Val Loss : 0.6937354803085327 Score : 0.7264957427978516\n",
      "[22/1000]\n",
      "- Train Loss : 0.7637737467885017 Score : 0.5647817701101303\n",
      "- Val Loss : 0.6686610579490662 Score : 0.7264957427978516\n",
      "[23/1000]\n",
      "- Train Loss : 0.743466891348362 Score : 0.5647817701101303\n",
      "- Val Loss : 0.644112765789032 Score : 0.7264957427978516\n",
      "[24/1000]\n",
      "- Train Loss : 0.7238001301884651 Score : 0.5647817701101303\n",
      "- Val Loss : 0.620583176612854 Score : 0.7264957427978516\n",
      "[25/1000]\n",
      "- Train Loss : 0.7047093361616135 Score : 0.5647817701101303\n",
      "- Val Loss : 0.5980215072631836 Score : 0.7264957427978516\n",
      "[26/1000]\n",
      "- Train Loss : 0.6865642294287682 Score : 0.5647817701101303\n",
      "- Val Loss : 0.5765706300735474 Score : 0.7264957427978516\n",
      "[27/1000]\n",
      "- Train Loss : 0.6691428646445274 Score : 0.5647817701101303\n",
      "- Val Loss : 0.5560252070426941 Score : 0.7264957427978516\n",
      "[28/1000]\n",
      "- Train Loss : 0.652643695473671 Score : 0.5647817701101303\n",
      "- Val Loss : 0.5371325612068176 Score : 0.7264957427978516\n",
      "[29/1000]\n",
      "- Train Loss : 0.6369597092270851 Score : 0.5647817701101303\n",
      "- Val Loss : 0.520036518573761 Score : 0.7264957427978516\n",
      "[30/1000]\n",
      "- Train Loss : 0.622002974152565 Score : 0.5647817701101303\n",
      "- Val Loss : 0.5045433640480042 Score : 0.7264957427978516\n",
      "[31/1000]\n",
      "- Train Loss : 0.6076078116893768 Score : 0.5647817701101303\n",
      "- Val Loss : 0.4886818826198578 Score : 0.7264957427978516\n",
      "[32/1000]\n",
      "- Train Loss : 0.5941803865134716 Score : 0.5781746283173561\n",
      "- Val Loss : 0.4740113914012909 Score : 0.7264957427978516\n",
      "[33/1000]\n",
      "- Train Loss : 0.581579677760601 Score : 0.5781746283173561\n",
      "- Val Loss : 0.4606740176677704 Score : 0.7264957427978516\n",
      "[34/1000]\n",
      "- Train Loss : 0.5694967545568943 Score : 0.5781746283173561\n",
      "- Val Loss : 0.447814404964447 Score : 0.7264957427978516\n",
      "[35/1000]\n",
      "- Train Loss : 0.5584172904491425 Score : 0.5906205177307129\n",
      "- Val Loss : 0.43668463826179504 Score : 0.7264957427978516\n",
      "[36/1000]\n",
      "- Train Loss : 0.5477670729160309 Score : 0.5906205177307129\n",
      "- Val Loss : 0.4259498417377472 Score : 0.7264957427978516\n",
      "[37/1000]\n",
      "- Train Loss : 0.5377850010991096 Score : 0.6015332117676735\n",
      "- Val Loss : 0.41628217697143555 Score : 0.8171428442001343\n",
      "[38/1000]\n",
      "- Train Loss : 0.5285245664417744 Score : 0.6676046326756477\n",
      "- Val Loss : 0.4082573354244232 Score : 0.8171428442001343\n",
      "[39/1000]\n",
      "- Train Loss : 0.5193819254636765 Score : 0.6856601908802986\n",
      "- Val Loss : 0.3999623656272888 Score : 0.8171428442001343\n",
      "[40/1000]\n",
      "- Train Loss : 0.5107389576733112 Score : 0.6856601908802986\n",
      "- Val Loss : 0.3918282985687256 Score : 0.8171428442001343\n",
      "[41/1000]\n",
      "- Train Loss : 0.5025356598198414 Score : 0.6961309686303139\n",
      "- Val Loss : 0.3843608796596527 Score : 0.8171428442001343\n",
      "[42/1000]\n",
      "- Train Loss : 0.4948042146861553 Score : 0.7508267313241959\n",
      "- Val Loss : 0.3780226707458496 Score : 0.8171428442001343\n",
      "[43/1000]\n",
      "- Train Loss : 0.4870508797466755 Score : 0.8091295436024666\n",
      "- Val Loss : 0.37127378582954407 Score : 0.8171428442001343\n",
      "[44/1000]\n",
      "- Train Loss : 0.4797573983669281 Score : 0.8503332510590553\n",
      "- Val Loss : 0.3652891218662262 Score : 0.888888955116272\n",
      "[45/1000]\n",
      "- Train Loss : 0.4724092110991478 Score : 0.8503332510590553\n",
      "- Val Loss : 0.3587947487831116 Score : 0.888888955116272\n",
      "[46/1000]\n",
      "- Train Loss : 0.4654421769082546 Score : 0.8598239868879318\n",
      "- Val Loss : 0.35315370559692383 Score : 0.888888955116272\n",
      "[47/1000]\n",
      "- Train Loss : 0.4585227854549885 Score : 0.8926282152533531\n",
      "- Val Loss : 0.34764260053634644 Score : 0.888888955116272\n",
      "[48/1000]\n",
      "- Train Loss : 0.451609518378973 Score : 0.904268428683281\n",
      "- Val Loss : 0.34113809466362 Score : 0.9484702348709106\n",
      "[49/1000]\n",
      "- Train Loss : 0.44504106789827347 Score : 0.904268428683281\n",
      "- Val Loss : 0.33603718876838684 Score : 0.9484702348709106\n",
      "[50/1000]\n",
      "- Train Loss : 0.4385416842997074 Score : 0.9135276898741722\n",
      "- Val Loss : 0.3315877914428711 Score : 0.9484702348709106\n",
      "[51/1000]\n",
      "- Train Loss : 0.4318801797926426 Score : 0.9135276898741722\n",
      "- Val Loss : 0.32566753029823303 Score : 0.9484702348709106\n",
      "[52/1000]\n",
      "- Train Loss : 0.42522941529750824 Score : 0.9135276898741722\n",
      "- Val Loss : 0.3188970685005188 Score : 0.9484702348709106\n",
      "[53/1000]\n",
      "- Train Loss : 0.419071052223444 Score : 0.9135276898741722\n",
      "- Val Loss : 0.31429213285446167 Score : 0.9484702348709106\n",
      "[54/1000]\n",
      "- Train Loss : 0.4127235449850559 Score : 0.9365560412406921\n",
      "- Val Loss : 0.3096241354942322 Score : 0.9484702348709106\n",
      "[55/1000]\n",
      "- Train Loss : 0.4062637463212013 Score : 0.9542328119277954\n",
      "- Val Loss : 0.304232120513916 Score : 0.9484702348709106\n",
      "[56/1000]\n",
      "- Train Loss : 0.3999073877930641 Score : 0.9542328119277954\n",
      "- Val Loss : 0.29885610938072205 Score : 1.0\n",
      "[57/1000]\n",
      "- Train Loss : 0.3936161994934082 Score : 0.9542328119277954\n",
      "- Val Loss : 0.2937704622745514 Score : 1.0\n",
      "[58/1000]\n",
      "- Train Loss : 0.38729023933410645 Score : 0.9542328119277954\n",
      "- Val Loss : 0.2886688709259033 Score : 1.0\n",
      "[59/1000]\n",
      "- Train Loss : 0.3809661194682121 Score : 0.9542328119277954\n",
      "- Val Loss : 0.2834279537200928 Score : 1.0\n",
      "[60/1000]\n",
      "- Train Loss : 0.37464262172579765 Score : 0.9542328119277954\n",
      "- Val Loss : 0.2781265377998352 Score : 1.0\n",
      "[61/1000]\n",
      "- Train Loss : 0.36836036667227745 Score : 0.9542328119277954\n",
      "- Val Loss : 0.2730597257614136 Score : 1.0\n",
      "[62/1000]\n",
      "- Train Loss : 0.36203404143452644 Score : 0.9542328119277954\n",
      "- Val Loss : 0.26750075817108154 Score : 1.0\n",
      "[63/1000]\n",
      "- Train Loss : 0.3556867875158787 Score : 0.9542328119277954\n",
      "- Val Loss : 0.26194825768470764 Score : 1.0\n",
      "[64/1000]\n",
      "- Train Loss : 0.3495844639837742 Score : 0.9542328119277954\n",
      "- Val Loss : 0.2575295567512512 Score : 1.0\n",
      "[65/1000]\n",
      "- Train Loss : 0.34328586608171463 Score : 0.9542328119277954\n",
      "- Val Loss : 0.25274544954299927 Score : 1.0\n",
      "[66/1000]\n",
      "- Train Loss : 0.33699729293584824 Score : 0.9542328119277954\n",
      "- Val Loss : 0.2477244883775711 Score : 1.0\n",
      "[67/1000]\n",
      "- Train Loss : 0.3308236375451088 Score : 0.9542328119277954\n",
      "- Val Loss : 0.24297764897346497 Score : 1.0\n",
      "[68/1000]\n",
      "- Train Loss : 0.3246496357023716 Score : 0.9542328119277954\n",
      "- Val Loss : 0.2379758656024933 Score : 1.0\n",
      "[69/1000]\n",
      "- Train Loss : 0.318482480943203 Score : 0.9542328119277954\n",
      "- Val Loss : 0.232662171125412 Score : 1.0\n",
      "[70/1000]\n",
      "- Train Loss : 0.31246729381382465 Score : 0.9542328119277954\n",
      "- Val Loss : 0.22791244089603424 Score : 1.0\n",
      "[71/1000]\n",
      "- Train Loss : 0.3065419476479292 Score : 0.9542328119277954\n",
      "- Val Loss : 0.2236403375864029 Score : 1.0\n",
      "[72/1000]\n",
      "- Train Loss : 0.3005610294640064 Score : 0.9542328119277954\n",
      "- Val Loss : 0.21892718970775604 Score : 1.0\n",
      "[73/1000]\n",
      "- Train Loss : 0.2946639135479927 Score : 0.9542328119277954\n",
      "- Val Loss : 0.21418429911136627 Score : 1.0\n",
      "[74/1000]\n",
      "- Train Loss : 0.2889625374227762 Score : 0.9542328119277954\n",
      "- Val Loss : 0.20997275412082672 Score : 1.0\n",
      "[75/1000]\n",
      "- Train Loss : 0.28324116207659245 Score : 0.9542328119277954\n",
      "- Val Loss : 0.20548208057880402 Score : 1.0\n",
      "[76/1000]\n",
      "- Train Loss : 0.2775931376963854 Score : 0.9542328119277954\n",
      "- Val Loss : 0.2008516788482666 Score : 1.0\n",
      "[77/1000]\n",
      "- Train Loss : 0.2721284907311201 Score : 0.9542328119277954\n",
      "- Val Loss : 0.1966874897480011 Score : 1.0\n",
      "[78/1000]\n",
      "- Train Loss : 0.26664525642991066 Score : 0.9542328119277954\n",
      "- Val Loss : 0.19210655987262726 Score : 1.0\n",
      "[79/1000]\n",
      "- Train Loss : 0.2614658586680889 Score : 0.9542328119277954\n",
      "- Val Loss : 0.1882905513048172 Score : 1.0\n",
      "[80/1000]\n",
      "- Train Loss : 0.25623765401542187 Score : 0.9542328119277954\n",
      "- Val Loss : 0.18432794511318207 Score : 1.0\n",
      "[81/1000]\n",
      "- Train Loss : 0.25116389989852905 Score : 0.9542328119277954\n",
      "- Val Loss : 0.1804903894662857 Score : 1.0\n",
      "[82/1000]\n",
      "- Train Loss : 0.24615903198719025 Score : 0.9542328119277954\n",
      "- Val Loss : 0.17644940316677094 Score : 1.0\n",
      "[83/1000]\n",
      "- Train Loss : 0.2412182316184044 Score : 0.9542328119277954\n",
      "- Val Loss : 0.17213596403598785 Score : 1.0\n",
      "[84/1000]\n",
      "- Train Loss : 0.23667436838150024 Score : 0.9542328119277954\n",
      "- Val Loss : 0.16923941671848297 Score : 1.0\n",
      "[85/1000]\n",
      "- Train Loss : 0.23197486996650696 Score : 0.9542328119277954\n",
      "- Val Loss : 0.1656884253025055 Score : 1.0\n",
      "[86/1000]\n",
      "- Train Loss : 0.22733648866415024 Score : 0.9542328119277954\n",
      "- Val Loss : 0.16149261593818665 Score : 1.0\n",
      "[87/1000]\n",
      "- Train Loss : 0.22308982722461224 Score : 0.9542328119277954\n",
      "- Val Loss : 0.15863797068595886 Score : 1.0\n",
      "[88/1000]\n",
      "- Train Loss : 0.21874499879777431 Score : 0.9542328119277954\n",
      "- Val Loss : 0.15518687665462494 Score : 1.0\n",
      "[89/1000]\n",
      "- Train Loss : 0.2145998403429985 Score : 0.9542328119277954\n",
      "- Val Loss : 0.15199710428714752 Score : 1.0\n",
      "[90/1000]\n",
      "- Train Loss : 0.21056096069514751 Score : 0.9542328119277954\n",
      "- Val Loss : 0.14909057319164276 Score : 1.0\n",
      "[91/1000]\n",
      "- Train Loss : 0.20656827744096518 Score : 0.9542328119277954\n",
      "- Val Loss : 0.14598426222801208 Score : 1.0\n",
      "[92/1000]\n",
      "- Train Loss : 0.20274483691900969 Score : 0.9542328119277954\n",
      "- Val Loss : 0.14287376403808594 Score : 1.0\n",
      "[93/1000]\n",
      "- Train Loss : 0.19908312521874905 Score : 0.9708994776010513\n",
      "- Val Loss : 0.14021626114845276 Score : 1.0\n",
      "[94/1000]\n",
      "- Train Loss : 0.1954294927418232 Score : 0.9708994776010513\n",
      "- Val Loss : 0.13738779723644257 Score : 1.0\n",
      "[95/1000]\n",
      "- Train Loss : 0.19191282521933317 Score : 0.9708994776010513\n",
      "- Val Loss : 0.13471104204654694 Score : 1.0\n",
      "[96/1000]\n",
      "- Train Loss : 0.18850128259509802 Score : 0.9708994776010513\n",
      "- Val Loss : 0.1320308893918991 Score : 1.0\n",
      "[97/1000]\n",
      "- Train Loss : 0.1851972471922636 Score : 0.9708994776010513\n",
      "- Val Loss : 0.12957797944545746 Score : 1.0\n",
      "[98/1000]\n",
      "- Train Loss : 0.18198311980813742 Score : 0.9708994776010513\n",
      "- Val Loss : 0.1271294206380844 Score : 1.0\n",
      "[99/1000]\n",
      "- Train Loss : 0.17886198312044144 Score : 0.9708994776010513\n",
      "- Val Loss : 0.12459087371826172 Score : 1.0\n",
      "[100/1000]\n",
      "- Train Loss : 0.1758573893457651 Score : 0.9708994776010513\n",
      "- Val Loss : 0.12245947122573853 Score : 1.0\n",
      "[101/1000]\n",
      "- Train Loss : 0.17294012755155563 Score : 0.9708994776010513\n",
      "- Val Loss : 0.12019599229097366 Score : 1.0\n",
      "[102/1000]\n",
      "- Train Loss : 0.17010106053203344 Score : 0.9708994776010513\n",
      "- Val Loss : 0.11811310797929764 Score : 1.0\n",
      "[103/1000]\n",
      "- Train Loss : 0.16736592166125774 Score : 0.9708994776010513\n",
      "- Val Loss : 0.11597733199596405 Score : 1.0\n",
      "[104/1000]\n",
      "- Train Loss : 0.16470195166766644 Score : 0.9708994776010513\n",
      "- Val Loss : 0.11388932913541794 Score : 1.0\n",
      "[105/1000]\n",
      "- Train Loss : 0.16214354801923037 Score : 0.9708994776010513\n",
      "- Val Loss : 0.11213337630033493 Score : 1.0\n",
      "[106/1000]\n",
      "- Train Loss : 0.1596516016870737 Score : 0.9708994776010513\n",
      "- Val Loss : 0.11019786447286606 Score : 1.0\n",
      "[107/1000]\n",
      "- Train Loss : 0.15723105985671282 Score : 0.9708994776010513\n",
      "- Val Loss : 0.10826737433671951 Score : 1.0\n",
      "[108/1000]\n",
      "- Train Loss : 0.15490588545799255 Score : 0.9708994776010513\n",
      "- Val Loss : 0.10663693398237228 Score : 1.0\n",
      "[109/1000]\n",
      "- Train Loss : 0.1526439916342497 Score : 0.9708994776010513\n",
      "- Val Loss : 0.10490643978118896 Score : 1.0\n",
      "[110/1000]\n",
      "- Train Loss : 0.15045294957235456 Score : 0.9708994776010513\n",
      "- Val Loss : 0.10329734534025192 Score : 1.0\n",
      "[111/1000]\n",
      "- Train Loss : 0.14833102840930223 Score : 0.9708994776010513\n",
      "- Val Loss : 0.10164129734039307 Score : 1.0\n",
      "[112/1000]\n",
      "- Train Loss : 0.146281145978719 Score : 0.9708994776010513\n",
      "- Val Loss : 0.10009769350290298 Score : 1.0\n",
      "[113/1000]\n",
      "- Train Loss : 0.1442945757880807 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09870224446058273 Score : 1.0\n",
      "[114/1000]\n",
      "- Train Loss : 0.1423661676235497 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09720444679260254 Score : 1.0\n",
      "[115/1000]\n",
      "- Train Loss : 0.1404997557401657 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09587807208299637 Score : 1.0\n",
      "[116/1000]\n",
      "- Train Loss : 0.1386972600594163 Score : 0.9708994776010513\n",
      "- Val Loss : 0.094471275806427 Score : 1.0\n",
      "[117/1000]\n",
      "- Train Loss : 0.13694148045033216 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09321055561304092 Score : 1.0\n",
      "[118/1000]\n",
      "- Train Loss : 0.1352606574073434 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0920005589723587 Score : 1.0\n",
      "[119/1000]\n",
      "- Train Loss : 0.1336098969914019 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09067366272211075 Score : 1.0\n",
      "[120/1000]\n",
      "- Train Loss : 0.13201897451654077 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08949407190084457 Score : 1.0\n",
      "[121/1000]\n",
      "- Train Loss : 0.1304813390597701 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08844801038503647 Score : 1.0\n",
      "[122/1000]\n",
      "- Train Loss : 0.12898323033005 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08735543489456177 Score : 1.0\n",
      "[123/1000]\n",
      "- Train Loss : 0.12753076385706663 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08620273321866989 Score : 1.0\n",
      "[124/1000]\n",
      "- Train Loss : 0.12612761463969946 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08523578941822052 Score : 1.0\n",
      "[125/1000]\n",
      "- Train Loss : 0.12476635910570621 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08420100063085556 Score : 1.0\n",
      "[126/1000]\n",
      "- Train Loss : 0.12343914667144418 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08328133821487427 Score : 1.0\n",
      "[127/1000]\n",
      "- Train Loss : 0.12216152343899012 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08232029527425766 Score : 1.0\n",
      "[128/1000]\n",
      "- Train Loss : 0.12090739328414202 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08143415302038193 Score : 1.0\n",
      "[129/1000]\n",
      "- Train Loss : 0.11970789451152086 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08053883165121078 Score : 1.0\n",
      "[130/1000]\n",
      "- Train Loss : 0.11853019427508116 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07971523702144623 Score : 1.0\n",
      "[131/1000]\n",
      "- Train Loss : 0.11738966964185238 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07892080396413803 Score : 1.0\n",
      "[132/1000]\n",
      "- Train Loss : 0.11628276063129306 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07807188481092453 Score : 1.0\n",
      "[133/1000]\n",
      "- Train Loss : 0.1152066788636148 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07732980698347092 Score : 1.0\n",
      "[134/1000]\n",
      "- Train Loss : 0.11416592914611101 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07655692845582962 Score : 1.0\n",
      "[135/1000]\n",
      "- Train Loss : 0.11314570577815175 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07585730403661728 Score : 1.0\n",
      "[136/1000]\n",
      "- Train Loss : 0.11215718393214047 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07517770677804947 Score : 1.0\n",
      "[137/1000]\n",
      "- Train Loss : 0.11120224744081497 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07445444166660309 Score : 1.0\n",
      "[138/1000]\n",
      "- Train Loss : 0.11026665521785617 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0738195851445198 Score : 1.0\n",
      "[139/1000]\n",
      "- Train Loss : 0.10936080850660801 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07315486669540405 Score : 1.0\n",
      "[140/1000]\n",
      "- Train Loss : 0.10847327462397516 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07246451079845428 Score : 1.0\n",
      "[141/1000]\n",
      "- Train Loss : 0.10761251067742705 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07194085419178009 Score : 1.0\n",
      "[142/1000]\n",
      "- Train Loss : 0.1067774766124785 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0713687613606453 Score : 1.0\n",
      "[143/1000]\n",
      "- Train Loss : 0.10596126201562583 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07074368745088577 Score : 1.0\n",
      "[144/1000]\n",
      "- Train Loss : 0.1051615965552628 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07022999972105026 Score : 1.0\n",
      "[145/1000]\n",
      "- Train Loss : 0.10439255833625793 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06967956572771072 Score : 1.0\n",
      "[146/1000]\n",
      "- Train Loss : 0.10363528179004788 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06917836517095566 Score : 1.0\n",
      "[147/1000]\n",
      "- Train Loss : 0.10290343035012484 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0686887577176094 Score : 1.0\n",
      "[148/1000]\n",
      "- Train Loss : 0.1021897834725678 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0681474432349205 Score : 1.0\n",
      "[149/1000]\n",
      "- Train Loss : 0.10148681653663516 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06770393997430801 Score : 1.0\n",
      "[150/1000]\n",
      "- Train Loss : 0.10080979741178453 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06726617366075516 Score : 1.0\n",
      "[151/1000]\n",
      "- Train Loss : 0.10014906828291714 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06676014512777328 Score : 1.0\n",
      "[152/1000]\n",
      "- Train Loss : 0.09949714387767017 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06634484231472015 Score : 1.0\n",
      "[153/1000]\n",
      "- Train Loss : 0.09886980406008661 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06588821858167648 Score : 1.0\n",
      "[154/1000]\n",
      "- Train Loss : 0.09825516724959016 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06542156636714935 Score : 1.0\n",
      "[155/1000]\n",
      "- Train Loss : 0.09764815657399595 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06511795520782471 Score : 1.0\n",
      "[156/1000]\n",
      "- Train Loss : 0.09707334940321743 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0646805390715599 Score : 1.0\n",
      "[157/1000]\n",
      "- Train Loss : 0.09649531380273402 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06426137685775757 Score : 1.0\n",
      "[158/1000]\n",
      "- Train Loss : 0.09593265526928008 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06398921459913254 Score : 1.0\n",
      "[159/1000]\n",
      "- Train Loss : 0.0953979806508869 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06357783824205399 Score : 1.0\n",
      "[160/1000]\n",
      "- Train Loss : 0.09485716954804957 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06321651488542557 Score : 1.0\n",
      "[161/1000]\n",
      "- Train Loss : 0.09433687198907137 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06291813403367996 Score : 1.0\n",
      "[162/1000]\n",
      "- Train Loss : 0.09382654586806893 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06261415779590607 Score : 1.0\n",
      "[163/1000]\n",
      "- Train Loss : 0.09332820354029536 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06228315830230713 Score : 1.0\n",
      "[164/1000]\n",
      "- Train Loss : 0.09284056234173477 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06194809079170227 Score : 1.0\n",
      "[165/1000]\n",
      "- Train Loss : 0.09235856425948441 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06170053035020828 Score : 1.0\n",
      "[166/1000]\n",
      "- Train Loss : 0.09189090505242348 Score : 0.9708994776010513\n",
      "- Val Loss : 0.061435993760824203 Score : 1.0\n",
      "[167/1000]\n",
      "- Train Loss : 0.09143243357539177 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06111212819814682 Score : 1.0\n",
      "[168/1000]\n",
      "- Train Loss : 0.09097547922283411 Score : 0.9708994776010513\n",
      "- Val Loss : 0.060853421688079834 Score : 1.0\n",
      "[169/1000]\n",
      "- Train Loss : 0.09053231228608638 Score : 0.9708994776010513\n",
      "- Val Loss : 0.060616862028837204 Score : 1.0\n",
      "[170/1000]\n",
      "- Train Loss : 0.09009959478862584 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06035514920949936 Score : 1.0\n",
      "[171/1000]\n",
      "- Train Loss : 0.08967085601761937 Score : 0.9708994776010513\n",
      "- Val Loss : 0.060117267072200775 Score : 1.0\n",
      "[172/1000]\n",
      "- Train Loss : 0.089250557590276 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0598808228969574 Score : 1.0\n",
      "[173/1000]\n",
      "- Train Loss : 0.08883892744779587 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05964120849967003 Score : 1.0\n",
      "[174/1000]\n",
      "- Train Loss : 0.088436683639884 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05939142033457756 Score : 1.0\n",
      "[175/1000]\n",
      "- Train Loss : 0.08803742832969874 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05918853357434273 Score : 1.0\n",
      "[176/1000]\n",
      "- Train Loss : 0.08765090256929398 Score : 0.9708994776010513\n",
      "- Val Loss : 0.058973755687475204 Score : 1.0\n",
      "[177/1000]\n",
      "- Train Loss : 0.0872729227412492 Score : 0.9708994776010513\n",
      "- Val Loss : 0.058748457580804825 Score : 1.0\n",
      "[178/1000]\n",
      "- Train Loss : 0.08690085588023067 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05854012444615364 Score : 1.0\n",
      "[179/1000]\n",
      "- Train Loss : 0.08653715730179101 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05829974263906479 Score : 1.0\n",
      "[180/1000]\n",
      "- Train Loss : 0.08617917087394744 Score : 0.9708994776010513\n",
      "- Val Loss : 0.058087389916181564 Score : 1.0\n",
      "[181/1000]\n",
      "- Train Loss : 0.08583152841310948 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05789974704384804 Score : 1.0\n",
      "[182/1000]\n",
      "- Train Loss : 0.08549321885220706 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05766814574599266 Score : 1.0\n",
      "[183/1000]\n",
      "- Train Loss : 0.08515930909197778 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0574738048017025 Score : 1.0\n",
      "[184/1000]\n",
      "- Train Loss : 0.0848300204379484 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05733413249254227 Score : 1.0\n",
      "[185/1000]\n",
      "- Train Loss : 0.08451005793176591 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0571756586432457 Score : 1.0\n",
      "[186/1000]\n",
      "- Train Loss : 0.08419281535316259 Score : 0.9708994776010513\n",
      "- Val Loss : 0.057003770023584366 Score : 1.0\n",
      "[187/1000]\n",
      "- Train Loss : 0.0838811444118619 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05683817341923714 Score : 1.0\n",
      "[188/1000]\n",
      "- Train Loss : 0.08357635291758925 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05668211728334427 Score : 1.0\n",
      "[189/1000]\n",
      "- Train Loss : 0.08327543258201331 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05655842646956444 Score : 1.0\n",
      "[190/1000]\n",
      "- Train Loss : 0.08297777606640011 Score : 0.9708994776010513\n",
      "- Val Loss : 0.056426119059324265 Score : 1.0\n",
      "[191/1000]\n",
      "- Train Loss : 0.08268237416632473 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05628339573740959 Score : 1.0\n",
      "[192/1000]\n",
      "- Train Loss : 0.08239214913919568 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05613962188363075 Score : 1.0\n",
      "[193/1000]\n",
      "- Train Loss : 0.08210676827002317 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05599948763847351 Score : 1.0\n",
      "[194/1000]\n",
      "- Train Loss : 0.08182643842883408 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05586513876914978 Score : 1.0\n",
      "[195/1000]\n",
      "- Train Loss : 0.08155176555737853 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05572685971856117 Score : 1.0\n",
      "[196/1000]\n",
      "- Train Loss : 0.08127850922755897 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05561358854174614 Score : 1.0\n",
      "[197/1000]\n",
      "- Train Loss : 0.08101278496906161 Score : 0.9708994776010513\n",
      "- Val Loss : 0.055496398359537125 Score : 1.0\n",
      "[198/1000]\n",
      "- Train Loss : 0.0807515800697729 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05537144094705582 Score : 1.0\n",
      "[199/1000]\n",
      "- Train Loss : 0.08049519394990057 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05524318665266037 Score : 1.0\n",
      "[200/1000]\n",
      "- Train Loss : 0.08023988327477127 Score : 0.9708994776010513\n",
      "- Val Loss : 0.055141981691122055 Score : 1.0\n",
      "[201/1000]\n",
      "- Train Loss : 0.07999182504136115 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05504129081964493 Score : 1.0\n",
      "[202/1000]\n",
      "- Train Loss : 0.07974834542255849 Score : 0.9708994776010513\n",
      "- Val Loss : 0.054931171238422394 Score : 1.0\n",
      "[203/1000]\n",
      "- Train Loss : 0.07950892858207226 Score : 0.9708994776010513\n",
      "- Val Loss : 0.054818086326122284 Score : 1.0\n",
      "[204/1000]\n",
      "- Train Loss : 0.0792696593562141 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05473381280899048 Score : 0.9538239240646362\n",
      "[205/1000]\n",
      "- Train Loss : 0.07903741113841534 Score : 0.9708994776010513\n",
      "- Val Loss : 0.054646026343107224 Score : 0.9538239240646362\n",
      "[206/1000]\n",
      "- Train Loss : 0.07881075004115701 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05451596528291702 Score : 0.9538239240646362\n",
      "[207/1000]\n",
      "- Train Loss : 0.07858393026981503 Score : 0.9708994776010513\n",
      "- Val Loss : 0.054405327886343 Score : 0.9538239240646362\n",
      "[208/1000]\n",
      "- Train Loss : 0.07836164243053645 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0543268620967865 Score : 0.9538239240646362\n",
      "[209/1000]\n",
      "- Train Loss : 0.07814481930108741 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0542462095618248 Score : 0.9538239240646362\n",
      "[210/1000]\n",
      "- Train Loss : 0.07793064403813332 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05416734889149666 Score : 0.9538239240646362\n",
      "[211/1000]\n",
      "- Train Loss : 0.07772065734025091 Score : 0.9708994776010513\n",
      "- Val Loss : 0.054086361080408096 Score : 0.9538239240646362\n",
      "[212/1000]\n",
      "- Train Loss : 0.07751473726239055 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05399796739220619 Score : 0.9538239240646362\n",
      "[213/1000]\n",
      "- Train Loss : 0.07730879221344367 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05392992123961449 Score : 0.9538239240646362\n",
      "[214/1000]\n",
      "- Train Loss : 0.07710918819066137 Score : 0.9708994776010513\n",
      "- Val Loss : 0.053856633603572845 Score : 0.9538239240646362\n",
      "[215/1000]\n",
      "- Train Loss : 0.07691034401068464 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05379074066877365 Score : 0.9538239240646362\n",
      "[216/1000]\n",
      "- Train Loss : 0.07671652326826006 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05371933430433273 Score : 0.9538239240646362\n",
      "[217/1000]\n",
      "- Train Loss : 0.07652306120144203 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05366038158535957 Score : 0.9538239240646362\n",
      "[218/1000]\n",
      "- Train Loss : 0.0763349526678212 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05359581485390663 Score : 0.9538239240646362\n",
      "[219/1000]\n",
      "- Train Loss : 0.07614720572018996 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05354113504290581 Score : 0.9538239240646362\n",
      "[220/1000]\n",
      "- Train Loss : 0.07596437499159947 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05348106473684311 Score : 0.9538239240646362\n",
      "[221/1000]\n",
      "- Train Loss : 0.07578394399024546 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05340772122144699 Score : 0.9538239240646362\n",
      "[222/1000]\n",
      "- Train Loss : 0.07560449850279838 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05333896353840828 Score : 0.9538239240646362\n",
      "[223/1000]\n",
      "- Train Loss : 0.07542729278793558 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05329131335020065 Score : 0.9538239240646362\n",
      "[224/1000]\n",
      "- Train Loss : 0.07525394455296919 Score : 0.9708994776010513\n",
      "- Val Loss : 0.053250301629304886 Score : 0.9538239240646362\n",
      "[225/1000]\n",
      "- Train Loss : 0.07508400606457144 Score : 0.9708994776010513\n",
      "- Val Loss : 0.053205907344818115 Score : 0.9538239240646362\n",
      "[226/1000]\n",
      "- Train Loss : 0.07491576002212241 Score : 0.9708994776010513\n",
      "- Val Loss : 0.053162287920713425 Score : 0.9538239240646362\n",
      "[227/1000]\n",
      "- Train Loss : 0.07474841829389334 Score : 0.9708994776010513\n",
      "- Val Loss : 0.053130414336919785 Score : 0.9538239240646362\n",
      "[228/1000]\n",
      "- Train Loss : 0.07458574010524899 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05308917537331581 Score : 0.9538239240646362\n",
      "[229/1000]\n",
      "- Train Loss : 0.0744237995822914 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05304888263344765 Score : 0.9538239240646362\n",
      "[230/1000]\n",
      "- Train Loss : 0.07426249561831355 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05302422121167183 Score : 0.9538239240646362\n",
      "[231/1000]\n",
      "- Train Loss : 0.07410611014347523 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05299045890569687 Score : 0.9538239240646362\n",
      "[232/1000]\n",
      "- Train Loss : 0.07395039050607011 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05295536667108536 Score : 0.9538239240646362\n",
      "[233/1000]\n",
      "- Train Loss : 0.07379603246226907 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05292879790067673 Score : 0.9538239240646362\n",
      "[234/1000]\n",
      "- Train Loss : 0.07364404539112002 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05290547385811806 Score : 0.9538239240646362\n",
      "[235/1000]\n",
      "- Train Loss : 0.07349425344727933 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05288100615143776 Score : 0.9538239240646362\n",
      "[236/1000]\n",
      "- Train Loss : 0.07334605243522674 Score : 0.9708994776010513\n",
      "- Val Loss : 0.052857596427202225 Score : 0.9538239240646362\n",
      "[237/1000]\n",
      "- Train Loss : 0.07319869770435616 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0528433583676815 Score : 0.9538239240646362\n",
      "[238/1000]\n",
      "- Train Loss : 0.07305525382980704 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05282025411725044 Score : 0.9538239240646362\n",
      "[239/1000]\n",
      "- Train Loss : 0.07291240920312703 Score : 0.9708994776010513\n",
      "- Val Loss : 0.052797965705394745 Score : 0.9538239240646362\n",
      "[240/1000]\n",
      "- Train Loss : 0.07277089555282146 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05278393253684044 Score : 0.9538239240646362\n",
      "[241/1000]\n",
      "- Train Loss : 0.0726314039202407 Score : 0.9708994776010513\n",
      "- Val Loss : 0.052772071212530136 Score : 0.9538239240646362\n",
      "[242/1000]\n",
      "- Train Loss : 0.07249373372178525 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05275912210345268 Score : 0.9538239240646362\n",
      "[243/1000]\n",
      "- Train Loss : 0.07235730614047498 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05274709686636925 Score : 0.9538239240646362\n",
      "[244/1000]\n",
      "- Train Loss : 0.07222244032891467 Score : 0.9708994776010513\n",
      "- Val Loss : 0.052737705409526825 Score : 0.9538239240646362\n",
      "[245/1000]\n",
      "- Train Loss : 0.0720893518300727 Score : 0.9708994776010513\n",
      "- Val Loss : 0.052730023860931396 Score : 0.9538239240646362\n",
      "[246/1000]\n",
      "- Train Loss : 0.07195768458768725 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05272335931658745 Score : 0.9538239240646362\n",
      "[247/1000]\n",
      "- Train Loss : 0.0718274392420426 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05271803215146065 Score : 0.9538239240646362\n",
      "[248/1000]\n",
      "- Train Loss : 0.07169842207804322 Score : 0.9708994776010513\n",
      "- Val Loss : 0.052714236080646515 Score : 0.9538239240646362\n",
      "[249/1000]\n",
      "- Train Loss : 0.07157098746392876 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05271169915795326 Score : 0.9538239240646362\n",
      "[250/1000]\n",
      "- Train Loss : 0.07144503976451233 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05271049961447716 Score : 0.9538239240646362\n",
      "[251/1000]\n",
      "- Train Loss : 0.07132039044518024 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05271073058247566 Score : 0.9538239240646362\n",
      "[252/1000]\n",
      "- Train Loss : 0.07119706424418837 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05271225422620773 Score : 0.9538239240646362\n",
      "[253/1000]\n",
      "- Train Loss : 0.07107489078771323 Score : 0.9708994776010513\n",
      "- Val Loss : 0.052715104073286057 Score : 0.9538239240646362\n",
      "[254/1000]\n",
      "- Train Loss : 0.0709540699608624 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0527188703417778 Score : 0.9538239240646362\n",
      "[255/1000]\n",
      "- Train Loss : 0.0708346466999501 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05272401496767998 Score : 0.9538239240646362\n",
      "[256/1000]\n",
      "- Train Loss : 0.0707163818879053 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05273055285215378 Score : 0.9538239240646362\n",
      "[257/1000]\n",
      "- Train Loss : 0.07059920282335952 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05273811146616936 Score : 0.9538239240646362\n",
      "[258/1000]\n",
      "- Train Loss : 0.07048328773817047 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05274680256843567 Score : 0.9538239240646362\n",
      "[259/1000]\n",
      "- Train Loss : 0.07036868709838018 Score : 0.9708994776010513\n",
      "- Val Loss : 0.052756667137145996 Score : 0.9538239240646362\n",
      "[260/1000]\n",
      "- Train Loss : 0.07025508262449875 Score : 0.9708994776010513\n",
      "- Val Loss : 0.052767686545848846 Score : 0.9538239240646362\n",
      "[261/1000]\n",
      "- Train Loss : 0.07014254876412451 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0527799017727375 Score : 0.9538239240646362\n",
      "[262/1000]\n",
      "- Train Loss : 0.07003112346865237 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05279296636581421 Score : 0.9538239240646362\n",
      "[263/1000]\n",
      "- Train Loss : 0.06992095411987975 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05280720070004463 Score : 0.9538239240646362\n",
      "[264/1000]\n",
      "- Train Loss : 0.06981165453908034 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05282258987426758 Score : 0.9538239240646362\n",
      "[265/1000]\n",
      "- Train Loss : 0.06970347237074748 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05283896252512932 Score : 0.9538239240646362\n",
      "[266/1000]\n",
      "- Train Loss : 0.0695963844191283 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05285622552037239 Score : 0.9538239240646362\n",
      "[267/1000]\n",
      "- Train Loss : 0.06949013465782627 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05287451669573784 Score : 0.9538239240646362\n",
      "[268/1000]\n",
      "- Train Loss : 0.0693849949457217 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05289395526051521 Score : 0.9538239240646362\n",
      "[269/1000]\n",
      "- Train Loss : 0.06928089371649548 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0529143288731575 Score : 0.9538239240646362\n",
      "[270/1000]\n",
      "- Train Loss : 0.06917762517696247 Score : 0.9708994776010513\n",
      "- Val Loss : 0.052935607731342316 Score : 0.9538239240646362\n",
      "[271/1000]\n",
      "- Train Loss : 0.0690753074013628 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05295786261558533 Score : 0.9538239240646362\n",
      "[272/1000]\n",
      "- Train Loss : 0.06897403870243579 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0529809407889843 Score : 0.9538239240646362\n",
      "[273/1000]\n",
      "- Train Loss : 0.06887455505784601 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05298498645424843 Score : 0.9538239240646362\n",
      "[274/1000]\n",
      "- Train Loss : 0.06877439891104586 Score : 0.9708994776010513\n",
      "- Val Loss : 0.052995625883340836 Score : 0.9538239240646362\n",
      "[275/1000]\n",
      "- Train Loss : 0.0686741670651827 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05302897468209267 Score : 0.9538239240646362\n",
      "[276/1000]\n",
      "- Train Loss : 0.06857676757499576 Score : 0.9708994776010513\n",
      "- Val Loss : 0.053062472492456436 Score : 0.9538239240646362\n",
      "[277/1000]\n",
      "- Train Loss : 0.06848087103571743 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05308738350868225 Score : 0.9538239240646362\n",
      "[278/1000]\n",
      "- Train Loss : 0.06838500883895904 Score : 0.9708994776010513\n",
      "- Val Loss : 0.053112614899873734 Score : 0.9538239240646362\n",
      "[279/1000]\n",
      "- Train Loss : 0.06828962705913 Score : 0.9708994776010513\n",
      "- Val Loss : 0.053142767399549484 Score : 0.9538239240646362\n",
      "[280/1000]\n",
      "- Train Loss : 0.06819541048025712 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05317431315779686 Score : 0.9538239240646362\n",
      "[281/1000]\n",
      "- Train Loss : 0.06810214451979846 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05320483446121216 Score : 0.9538239240646362\n",
      "[282/1000]\n",
      "- Train Loss : 0.06800953211495653 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05323587730526924 Score : 0.9538239240646362\n",
      "[283/1000]\n",
      "- Train Loss : 0.06791756342863664 Score : 0.9708994776010513\n",
      "- Val Loss : 0.053268544375896454 Score : 0.9538239240646362\n",
      "[284/1000]\n",
      "- Train Loss : 0.06782645345083438 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0533020980656147 Score : 0.9538239240646362\n",
      "[285/1000]\n",
      "- Train Loss : 0.06773608917137608 Score : 0.9708994776010513\n",
      "- Val Loss : 0.053336162120103836 Score : 0.9538239240646362\n",
      "[286/1000]\n",
      "- Train Loss : 0.06764645967632532 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05337070673704147 Score : 0.9538239240646362\n",
      "[287/1000]\n",
      "- Train Loss : 0.06755748673458584 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0534062422811985 Score : 0.9538239240646362\n",
      "[288/1000]\n",
      "- Train Loss : 0.067469303146936 Score : 0.9708994776010513\n",
      "- Val Loss : 0.053442444652318954 Score : 0.9538239240646362\n",
      "[289/1000]\n",
      "- Train Loss : 0.06738172765471973 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05347949266433716 Score : 0.9538239240646362\n",
      "[290/1000]\n",
      "- Train Loss : 0.0672948912542779 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0535171814262867 Score : 0.9538239240646362\n",
      "[291/1000]\n",
      "- Train Loss : 0.06720873131416738 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05355558171868324 Score : 0.9538239240646362\n",
      "[292/1000]\n",
      "- Train Loss : 0.06712323645479046 Score : 0.9708994776010513\n",
      "- Val Loss : 0.053594619035720825 Score : 0.9538239240646362\n",
      "[293/1000]\n",
      "- Train Loss : 0.06703835623920895 Score : 0.9708994776010513\n",
      "- Val Loss : 0.053634289652109146 Score : 0.9538239240646362\n",
      "[294/1000]\n",
      "- Train Loss : 0.06695414232672192 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05367473512887955 Score : 0.9538239240646362\n",
      "[295/1000]\n",
      "- Train Loss : 0.06687060915282927 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05371566489338875 Score : 0.9538239240646362\n",
      "[296/1000]\n",
      "- Train Loss : 0.06678770115831867 Score : 0.9708994776010513\n",
      "- Val Loss : 0.053757183253765106 Score : 0.9538239240646362\n",
      "[297/1000]\n",
      "- Train Loss : 0.06670533982105553 Score : 0.9708994776010513\n",
      "- Val Loss : 0.053799692541360855 Score : 0.9538239240646362\n",
      "[298/1000]\n",
      "- Train Loss : 0.06662369280820712 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05384274572134018 Score : 0.9538239240646362\n",
      "[299/1000]\n",
      "- Train Loss : 0.0665425397164654 Score : 0.9708994776010513\n",
      "- Val Loss : 0.053886305540800095 Score : 0.9538239240646362\n",
      "[300/1000]\n",
      "- Train Loss : 0.06646201116382144 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05393055081367493 Score : 0.9538239240646362\n",
      "[301/1000]\n",
      "- Train Loss : 0.06638202106114477 Score : 0.9708994776010513\n",
      "- Val Loss : 0.053975433111190796 Score : 0.9538239240646362\n",
      "[302/1000]\n",
      "- Train Loss : 0.0663026635593269 Score : 0.9708994776010513\n",
      "- Val Loss : 0.054020803421735764 Score : 0.9538239240646362\n",
      "[303/1000]\n",
      "- Train Loss : 0.06622380344197154 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0540667362511158 Score : 0.9538239240646362\n",
      "[304/1000]\n",
      "- Train Loss : 0.06614555377746001 Score : 0.9708994776010513\n",
      "- Val Loss : 0.054113440215587616 Score : 0.9538239240646362\n",
      "[305/1000]\n",
      "- Train Loss : 0.06606783793540671 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05416054651141167 Score : 0.9538239240646362\n",
      "[306/1000]\n",
      "- Train Loss : 0.06599063018802553 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05420803651213646 Score : 0.9538239240646362\n",
      "[307/1000]\n",
      "- Train Loss : 0.06591392381233163 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05425647646188736 Score : 0.9538239240646362\n",
      "[308/1000]\n",
      "- Train Loss : 0.06583780207438394 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05430528521537781 Score : 0.9538239240646362\n",
      "[309/1000]\n",
      "- Train Loss : 0.06576215766835958 Score : 0.9708994776010513\n",
      "- Val Loss : 0.054354581981897354 Score : 0.9538239240646362\n",
      "[310/1000]\n",
      "- Train Loss : 0.06568704982055351 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05440440773963928 Score : 0.9538239240646362\n",
      "[311/1000]\n",
      "- Train Loss : 0.065612426988082 Score : 0.9708994776010513\n",
      "- Val Loss : 0.054454684257507324 Score : 0.9538239240646362\n",
      "[312/1000]\n",
      "- Train Loss : 0.06553825084120035 Score : 0.9708994776010513\n",
      "- Val Loss : 0.054505836218595505 Score : 0.9538239240646362\n",
      "[313/1000]\n",
      "- Train Loss : 0.0654646409675479 Score : 0.9708994776010513\n",
      "- Val Loss : 0.054557204246520996 Score : 0.9538239240646362\n",
      "[314/1000]\n",
      "- Train Loss : 0.0653943624929525 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05458742380142212 Score : 0.9538239240646362\n",
      "[315/1000]\n",
      "- Train Loss : 0.06532066833460703 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05463116243481636 Score : 0.9538239240646362\n",
      "[316/1000]\n",
      "- Train Loss : 0.06524731358513236 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05469238758087158 Score : 0.9538239240646362\n",
      "[317/1000]\n",
      "- Train Loss : 0.06517605535918847 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05475085228681564 Score : 0.9538239240646362\n",
      "[318/1000]\n",
      "- Train Loss : 0.06510557926958427 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05480174347758293 Score : 0.9538239240646362\n",
      "[319/1000]\n",
      "- Train Loss : 0.06503488082671538 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05485349893569946 Score : 0.9538239240646362\n",
      "[320/1000]\n",
      "- Train Loss : 0.06496434507425874 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05490944907069206 Score : 0.9538239240646362\n",
      "[321/1000]\n",
      "- Train Loss : 0.06489458412397653 Score : 0.9708994776010513\n",
      "- Val Loss : 0.054965853691101074 Score : 0.9538239240646362\n",
      "[322/1000]\n",
      "- Train Loss : 0.06482532180962153 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05502196028828621 Score : 0.9538239240646362\n",
      "[323/1000]\n",
      "- Train Loss : 0.0647563845559489 Score : 0.9708994776010513\n",
      "- Val Loss : 0.055078137665987015 Score : 0.9538239240646362\n",
      "[324/1000]\n",
      "- Train Loss : 0.06468781785224564 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05513535067439079 Score : 0.9538239240646362\n",
      "[325/1000]\n",
      "- Train Loss : 0.06461970185046084 Score : 0.9708994776010513\n",
      "- Val Loss : 0.055193137377500534 Score : 0.9538239240646362\n",
      "[326/1000]\n",
      "- Train Loss : 0.06455196239403449 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05525143817067146 Score : 0.9538239240646362\n",
      "[327/1000]\n",
      "- Train Loss : 0.06448468586313538 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05530980974435806 Score : 0.9538239240646362\n",
      "[328/1000]\n",
      "- Train Loss : 0.0644177719950676 Score : 0.9708994776010513\n",
      "- Val Loss : 0.055368732661008835 Score : 0.9538239240646362\n",
      "[329/1000]\n",
      "- Train Loss : 0.06435124122072011 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05542809143662453 Score : 0.9538239240646362\n",
      "[330/1000]\n",
      "- Train Loss : 0.06428506851079874 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0554877333343029 Score : 0.9538239240646362\n",
      "[331/1000]\n",
      "- Train Loss : 0.0642192873347085 Score : 0.9708994776010513\n",
      "- Val Loss : 0.055547893047332764 Score : 0.9538239240646362\n",
      "[332/1000]\n",
      "- Train Loss : 0.06415391725022346 Score : 0.9708994776010513\n",
      "- Val Loss : 0.055608492344617844 Score : 0.9538239240646362\n",
      "[333/1000]\n",
      "- Train Loss : 0.06408888270379975 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05566949397325516 Score : 0.9538239240646362\n",
      "[334/1000]\n",
      "- Train Loss : 0.06402421640814282 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05573072284460068 Score : 0.9538239240646362\n",
      "[335/1000]\n",
      "- Train Loss : 0.06395991586032324 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05579235404729843 Score : 0.9538239240646362\n",
      "[336/1000]\n",
      "- Train Loss : 0.06389597442466766 Score : 0.9708994776010513\n",
      "- Val Loss : 0.055854517966508865 Score : 0.9538239240646362\n",
      "[337/1000]\n",
      "- Train Loss : 0.06383240078866947 Score : 0.9708994776010513\n",
      "- Val Loss : 0.055916931480169296 Score : 0.9538239240646362\n",
      "[338/1000]\n",
      "- Train Loss : 0.06376864356570877 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05598599463701248 Score : 0.9538239240646362\n",
      "[339/1000]\n",
      "- Train Loss : 0.06370604560652282 Score : 0.9708994776010513\n",
      "- Val Loss : 0.056050218641757965 Score : 0.9538239240646362\n",
      "[340/1000]\n",
      "- Train Loss : 0.0636435943597462 Score : 0.9708994776010513\n",
      "- Val Loss : 0.056111134588718414 Score : 0.9538239240646362\n",
      "[341/1000]\n",
      "- Train Loss : 0.06358111930603627 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05617425590753555 Score : 0.9538239240646362\n",
      "[342/1000]\n",
      "- Train Loss : 0.06351898339926265 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05623951554298401 Score : 0.9538239240646362\n",
      "[343/1000]\n",
      "- Train Loss : 0.06345730787143111 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05630463734269142 Score : 0.9538239240646362\n",
      "[344/1000]\n",
      "- Train Loss : 0.06339602293155622 Score : 0.9708994776010513\n",
      "- Val Loss : 0.056369032710790634 Score : 0.9538239240646362\n",
      "[345/1000]\n",
      "- Train Loss : 0.06333495108992793 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05643417686223984 Score : 0.9538239240646362\n",
      "[346/1000]\n",
      "- Train Loss : 0.06327418015280273 Score : 0.9708994776010513\n",
      "- Val Loss : 0.056500036269426346 Score : 0.9538239240646362\n",
      "[347/1000]\n",
      "- Train Loss : 0.0632137935172068 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05656605213880539 Score : 0.9538239240646362\n",
      "[348/1000]\n",
      "- Train Loss : 0.06315364330657758 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05663224309682846 Score : 0.9538239240646362\n",
      "[349/1000]\n",
      "- Train Loss : 0.06309381450410001 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05669892951846123 Score : 0.9538239240646362\n",
      "[350/1000]\n",
      "- Train Loss : 0.06303433256107382 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05676579475402832 Score : 0.9538239240646362\n",
      "[351/1000]\n",
      "- Train Loss : 0.06297509952855762 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05683322623372078 Score : 0.9538239240646362\n",
      "[352/1000]\n",
      "- Train Loss : 0.06291621073614806 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05690086632966995 Score : 0.9538239240646362\n",
      "[353/1000]\n",
      "- Train Loss : 0.06285758265585173 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0569685660302639 Score : 0.9538239240646362\n",
      "[354/1000]\n",
      "- Train Loss : 0.06279918918153271 Score : 0.9708994776010513\n",
      "- Val Loss : 0.057036932557821274 Score : 0.9538239240646362\n",
      "[355/1000]\n",
      "- Train Loss : 0.06274121007299982 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05710523575544357 Score : 0.9538239240646362\n",
      "[356/1000]\n",
      "- Train Loss : 0.06268346557044424 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05717405676841736 Score : 0.9538239240646362\n",
      "[357/1000]\n",
      "- Train Loss : 0.06262598669854924 Score : 0.9708994776010513\n",
      "- Val Loss : 0.057243458926677704 Score : 0.9538239240646362\n",
      "[358/1000]\n",
      "- Train Loss : 0.06256877195846755 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05731282755732536 Score : 0.9538239240646362\n",
      "[359/1000]\n",
      "- Train Loss : 0.06251183096901514 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05738244205713272 Score : 0.9538239240646362\n",
      "[360/1000]\n",
      "- Train Loss : 0.06245515897171572 Score : 0.9708994776010513\n",
      "- Val Loss : 0.057452376931905746 Score : 0.9538239240646362\n",
      "[361/1000]\n",
      "- Train Loss : 0.062398729802225716 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05752279609441757 Score : 0.9538239240646362\n",
      "[362/1000]\n",
      "- Train Loss : 0.06234262768703047 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05759333446621895 Score : 0.9538239240646362\n",
      "[363/1000]\n",
      "- Train Loss : 0.06228675090824254 Score : 0.9708994776010513\n",
      "- Val Loss : 0.057664331048727036 Score : 0.9538239240646362\n",
      "[364/1000]\n",
      "- Train Loss : 0.062231105417595245 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05773550644516945 Score : 0.9538239240646362\n",
      "[365/1000]\n",
      "- Train Loss : 0.06217685414594598 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05779081955552101 Score : 0.9538239240646362\n",
      "[366/1000]\n",
      "- Train Loss : 0.06212178243731614 Score : 0.9708994776010513\n",
      "- Val Loss : 0.057843830436468124 Score : 0.9538239240646362\n",
      "[367/1000]\n",
      "- Train Loss : 0.06206535644014366 Score : 0.9708994776010513\n",
      "- Val Loss : 0.057919371873140335 Score : 0.9538239240646362\n",
      "[368/1000]\n",
      "- Train Loss : 0.06201059042359702 Score : 0.9708994776010513\n",
      "- Val Loss : 0.057999975979328156 Score : 0.9538239240646362\n",
      "[369/1000]\n",
      "- Train Loss : 0.061957021403941326 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05807187035679817 Score : 0.9538239240646362\n",
      "[370/1000]\n",
      "- Train Loss : 0.06190324175986461 Score : 0.9708994776010513\n",
      "- Val Loss : 0.058140601962804794 Score : 0.9538239240646362\n",
      "[371/1000]\n",
      "- Train Loss : 0.06184917615610175 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05821271613240242 Score : 0.9538239240646362\n",
      "[372/1000]\n",
      "- Train Loss : 0.061795500572770834 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05828722193837166 Score : 0.9538239240646362\n",
      "[373/1000]\n",
      "- Train Loss : 0.06174226301664021 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05836092680692673 Score : 0.9538239240646362\n",
      "[374/1000]\n",
      "- Train Loss : 0.061689247930189595 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05843362212181091 Score : 0.9538239240646362\n",
      "[375/1000]\n",
      "- Train Loss : 0.06163633150572423 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05850713700056076 Score : 0.9538239240646362\n",
      "[376/1000]\n",
      "- Train Loss : 0.061583659509778954 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05858127400279045 Score : 0.9538239240646362\n",
      "[377/1000]\n",
      "- Train Loss : 0.061531255778390914 Score : 0.9708994776010513\n",
      "- Val Loss : 0.058655690401792526 Score : 0.9538239240646362\n",
      "[378/1000]\n",
      "- Train Loss : 0.06147905471152626 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05873005464673042 Score : 0.9538239240646362\n",
      "[379/1000]\n",
      "- Train Loss : 0.06142705664387904 Score : 0.9708994776010513\n",
      "- Val Loss : 0.058804742991924286 Score : 0.9538239240646362\n",
      "[380/1000]\n",
      "- Train Loss : 0.06137524177029263 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05887961760163307 Score : 0.9538239240646362\n",
      "[381/1000]\n",
      "- Train Loss : 0.061323678397457115 Score : 0.9708994776010513\n",
      "- Val Loss : 0.058954868465662 Score : 0.9538239240646362\n",
      "[382/1000]\n",
      "- Train Loss : 0.06127231578284409 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05903041735291481 Score : 0.9538239240646362\n",
      "[383/1000]\n",
      "- Train Loss : 0.06122118927305564 Score : 0.9708994776010513\n",
      "- Val Loss : 0.059105951339006424 Score : 0.9538239240646362\n",
      "[384/1000]\n",
      "- Train Loss : 0.06117021803220268 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05918189510703087 Score : 0.9538239240646362\n",
      "[385/1000]\n",
      "- Train Loss : 0.06111943854193669 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05925809592008591 Score : 0.9538239240646362\n",
      "[386/1000]\n",
      "- Train Loss : 0.06106895401899237 Score : 0.9708994776010513\n",
      "- Val Loss : 0.059334516525268555 Score : 0.9538239240646362\n",
      "[387/1000]\n",
      "- Train Loss : 0.06101860592025332 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05941106006503105 Score : 0.9538239240646362\n",
      "[388/1000]\n",
      "- Train Loss : 0.06096838951634709 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05948779731988907 Score : 0.9538239240646362\n",
      "[389/1000]\n",
      "- Train Loss : 0.0609184217028087 Score : 0.9708994776010513\n",
      "- Val Loss : 0.059564895927906036 Score : 0.9538239240646362\n",
      "[390/1000]\n",
      "- Train Loss : 0.06086866579425987 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05964217707514763 Score : 0.9538239240646362\n",
      "[391/1000]\n",
      "- Train Loss : 0.060819080885266885 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05971978232264519 Score : 0.9538239240646362\n",
      "[392/1000]\n",
      "- Train Loss : 0.0607696765800938 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05979757383465767 Score : 0.9538239240646362\n",
      "[393/1000]\n",
      "- Train Loss : 0.06072050301008858 Score : 0.9708994776010513\n",
      "- Val Loss : 0.059875424951314926 Score : 0.9538239240646362\n",
      "[394/1000]\n",
      "- Train Loss : 0.06067144488042686 Score : 0.9708994776010513\n",
      "- Val Loss : 0.05995367094874382 Score : 0.9538239240646362\n",
      "[395/1000]\n",
      "- Train Loss : 0.0606225794472266 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06003204733133316 Score : 0.9538239240646362\n",
      "[396/1000]\n",
      "- Train Loss : 0.06057394559320528 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06011052802205086 Score : 0.9538239240646362\n",
      "[397/1000]\n",
      "- Train Loss : 0.060525459179189056 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06018944829702377 Score : 0.9538239240646362\n",
      "[398/1000]\n",
      "- Train Loss : 0.060477117891423404 Score : 0.9708994776010513\n",
      "- Val Loss : 0.060268573462963104 Score : 0.9538239240646362\n",
      "[399/1000]\n",
      "- Train Loss : 0.060428973723901436 Score : 0.9708994776010513\n",
      "- Val Loss : 0.060347769409418106 Score : 0.9538239240646362\n",
      "[400/1000]\n",
      "- Train Loss : 0.06038101980811916 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06042720749974251 Score : 0.9538239240646362\n",
      "[401/1000]\n",
      "- Train Loss : 0.060333278903272 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06050681322813034 Score : 0.9538239240646362\n",
      "[402/1000]\n",
      "- Train Loss : 0.06028561135462951 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06058666110038757 Score : 0.9538239240646362\n",
      "[403/1000]\n",
      "- Train Loss : 0.06023819478286896 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0606667697429657 Score : 0.9538239240646362\n",
      "[404/1000]\n",
      "- Train Loss : 0.060190910793608055 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06074700132012367 Score : 0.9538239240646362\n",
      "[405/1000]\n",
      "- Train Loss : 0.06014374355436303 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06082746013998985 Score : 0.9538239240646362\n",
      "[406/1000]\n",
      "- Train Loss : 0.060096823290223256 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06090828403830528 Score : 0.9538239240646362\n",
      "[407/1000]\n",
      "- Train Loss : 0.06005005670886021 Score : 0.9708994776010513\n",
      "- Val Loss : 0.060989052057266235 Score : 0.9538239240646362\n",
      "[408/1000]\n",
      "- Train Loss : 0.060003374732332304 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06107011064887047 Score : 0.9538239240646362\n",
      "[409/1000]\n",
      "- Train Loss : 0.059956907731248066 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06115146353840828 Score : 0.9538239240646362\n",
      "[410/1000]\n",
      "- Train Loss : 0.05991058467770927 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06123296543955803 Score : 0.9538239240646362\n",
      "[411/1000]\n",
      "- Train Loss : 0.05986441067943815 Score : 0.9708994776010513\n",
      "- Val Loss : 0.061314601451158524 Score : 0.9538239240646362\n",
      "[412/1000]\n",
      "- Train Loss : 0.059818403839017265 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06139635667204857 Score : 0.9538239240646362\n",
      "[413/1000]\n",
      "- Train Loss : 0.059772511245682836 Score : 0.9708994776010513\n",
      "- Val Loss : 0.061478324234485626 Score : 0.9538239240646362\n",
      "[414/1000]\n",
      "- Train Loss : 0.05972678976831958 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06156067177653313 Score : 0.9538239240646362\n",
      "[415/1000]\n",
      "- Train Loss : 0.059681233498849906 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06164313107728958 Score : 0.9538239240646362\n",
      "[416/1000]\n",
      "- Train Loss : 0.059635860452544875 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06172564625740051 Score : 0.9538239240646362\n",
      "[417/1000]\n",
      "- Train Loss : 0.05959057461586781 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06180838868021965 Score : 0.9538239240646362\n",
      "[418/1000]\n",
      "- Train Loss : 0.059545449854340404 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06189141422510147 Score : 0.9538239240646362\n",
      "[419/1000]\n",
      "- Train Loss : 0.05950046940415632 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06197451800107956 Score : 0.9538239240646362\n",
      "[420/1000]\n",
      "- Train Loss : 0.05945563214481808 Score : 0.9708994776010513\n",
      "- Val Loss : 0.062057893723249435 Score : 0.9538239240646362\n",
      "[421/1000]\n",
      "- Train Loss : 0.05941093625733629 Score : 0.9708994776010513\n",
      "- Val Loss : 0.062141358852386475 Score : 0.9538239240646362\n",
      "[422/1000]\n",
      "- Train Loss : 0.05936637715785764 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0622250959277153 Score : 0.9538239240646362\n",
      "[423/1000]\n",
      "- Train Loss : 0.0593219383736141 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06230897083878517 Score : 0.9538239240646362\n",
      "[424/1000]\n",
      "- Train Loss : 0.059277658379869536 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06239299848675728 Score : 0.9538239240646362\n",
      "[425/1000]\n",
      "- Train Loss : 0.059233478910755366 Score : 0.9708994776010513\n",
      "- Val Loss : 0.062477294355630875 Score : 0.9538239240646362\n",
      "[426/1000]\n",
      "- Train Loss : 0.05918950402701739 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0625617653131485 Score : 0.9538239240646362\n",
      "[427/1000]\n",
      "- Train Loss : 0.05914564154227264 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06264621019363403 Score : 0.9538239240646362\n",
      "[428/1000]\n",
      "- Train Loss : 0.05910186335677281 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06273102015256882 Score : 0.9538239240646362\n",
      "[429/1000]\n",
      "- Train Loss : 0.059058275088318624 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06281600147485733 Score : 0.9538239240646362\n",
      "[430/1000]\n",
      "- Train Loss : 0.059014736936660483 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06290099024772644 Score : 0.9538239240646362\n",
      "[431/1000]\n",
      "- Train Loss : 0.058971427235519513 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06298644840717316 Score : 0.9538239240646362\n",
      "[432/1000]\n",
      "- Train Loss : 0.058928187034325674 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06307172775268555 Score : 0.9538239240646362\n",
      "[433/1000]\n",
      "- Train Loss : 0.0588850860731327 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06315715610980988 Score : 0.9538239240646362\n",
      "[434/1000]\n",
      "- Train Loss : 0.05884211722877808 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0632430836558342 Score : 0.9538239240646362\n",
      "[435/1000]\n",
      "- Train Loss : 0.058799235717742704 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06332903355360031 Score : 0.9538239240646362\n",
      "[436/1000]\n",
      "- Train Loss : 0.05875649629160762 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06341535598039627 Score : 0.9538239240646362\n",
      "[437/1000]\n",
      "- Train Loss : 0.058713929502118845 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06350143998861313 Score : 0.9538239240646362\n",
      "[438/1000]\n",
      "- Train Loss : 0.05867143865907565 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06358779966831207 Score : 0.9538239240646362\n",
      "[439/1000]\n",
      "- Train Loss : 0.05862906550464686 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06367446482181549 Score : 0.9538239240646362\n",
      "[440/1000]\n",
      "- Train Loss : 0.058586795661540236 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0637611597776413 Score : 0.9538239240646362\n",
      "[441/1000]\n",
      "- Train Loss : 0.05854467494646087 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06384816765785217 Score : 0.9538239240646362\n",
      "[442/1000]\n",
      "- Train Loss : 0.05850266075140098 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06393532454967499 Score : 0.9538239240646362\n",
      "[443/1000]\n",
      "- Train Loss : 0.058460757667489816 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0640224888920784 Score : 0.9538239240646362\n",
      "[444/1000]\n",
      "- Train Loss : 0.058418960572453216 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06410970538854599 Score : 0.9538239240646362\n",
      "[445/1000]\n",
      "- Train Loss : 0.05837726193567505 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06419739872217178 Score : 0.9538239240646362\n",
      "[446/1000]\n",
      "- Train Loss : 0.05833579214231577 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06428505480289459 Score : 0.9538239240646362\n",
      "[447/1000]\n",
      "- Train Loss : 0.05829427963180933 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06437301635742188 Score : 0.9538239240646362\n",
      "[448/1000]\n",
      "- Train Loss : 0.05825296222610632 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06446104496717453 Score : 0.9538239240646362\n",
      "[449/1000]\n",
      "- Train Loss : 0.058211727235175204 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06454920768737793 Score : 0.9538239240646362\n",
      "[450/1000]\n",
      "- Train Loss : 0.05817064109578496 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06463731080293655 Score : 0.9538239240646362\n",
      "[451/1000]\n",
      "- Train Loss : 0.058129605640715454 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06472592055797577 Score : 0.9538239240646362\n",
      "[452/1000]\n",
      "- Train Loss : 0.05808870954933809 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06481464952230453 Score : 0.9538239240646362\n",
      "[453/1000]\n",
      "- Train Loss : 0.05804798582539661 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06490323692560196 Score : 0.9538239240646362\n",
      "[454/1000]\n",
      "- Train Loss : 0.058007248560898006 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06499228626489639 Score : 0.9538239240646362\n",
      "[455/1000]\n",
      "- Train Loss : 0.05796665851084981 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06508149951696396 Score : 0.9538239240646362\n",
      "[456/1000]\n",
      "- Train Loss : 0.05792618387204129 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06517069041728973 Score : 0.9538239240646362\n",
      "[457/1000]\n",
      "- Train Loss : 0.05788579825457418 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06526007503271103 Score : 0.9538239240646362\n",
      "[458/1000]\n",
      "- Train Loss : 0.05784552561090095 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06534966826438904 Score : 0.9538239240646362\n",
      "[459/1000]\n",
      "- Train Loss : 0.05780535752273863 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06543939560651779 Score : 0.9538239240646362\n",
      "[460/1000]\n",
      "- Train Loss : 0.05776526243425906 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0655292421579361 Score : 0.9538239240646362\n",
      "[461/1000]\n",
      "- Train Loss : 0.057725280967133585 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06561930477619171 Score : 0.9538239240646362\n",
      "[462/1000]\n",
      "- Train Loss : 0.05768541390716564 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06570933014154434 Score : 0.9538239240646362\n",
      "[463/1000]\n",
      "- Train Loss : 0.05764564154378604 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06579957902431488 Score : 0.9538239240646362\n",
      "[464/1000]\n",
      "- Train Loss : 0.05760596165782772 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06589005887508392 Score : 0.9538239240646362\n",
      "[465/1000]\n",
      "- Train Loss : 0.0575663252529921 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0659807026386261 Score : 0.9538239240646362\n",
      "[466/1000]\n",
      "- Train Loss : 0.05752684563776711 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06607142835855484 Score : 0.9538239240646362\n",
      "[467/1000]\n",
      "- Train Loss : 0.057487403028062545 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06616228073835373 Score : 0.9538239240646362\n",
      "[468/1000]\n",
      "- Train Loss : 0.05744813220371725 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06625330448150635 Score : 0.9538239240646362\n",
      "[469/1000]\n",
      "- Train Loss : 0.05740891337336507 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06634441018104553 Score : 0.9538239240646362\n",
      "[470/1000]\n",
      "- Train Loss : 0.05736980245274026 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06643576920032501 Score : 0.9538239240646362\n",
      "[471/1000]\n",
      "- Train Loss : 0.05733073724695714 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06652729213237762 Score : 0.9538239240646362\n",
      "[472/1000]\n",
      "- Train Loss : 0.0572918245306937 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06661879271268845 Score : 0.9538239240646362\n",
      "[473/1000]\n",
      "- Train Loss : 0.057252975755545776 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06671050190925598 Score : 0.9538239240646362\n",
      "[474/1000]\n",
      "- Train Loss : 0.05721419657493243 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06680238991975784 Score : 0.9538239240646362\n",
      "[475/1000]\n",
      "- Train Loss : 0.0571759989761631 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06689005345106125 Score : 0.9538239240646362\n",
      "[476/1000]\n",
      "- Train Loss : 0.057137657830026 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0669742301106453 Score : 0.9538239240646362\n",
      "[477/1000]\n",
      "- Train Loss : 0.05709859415219398 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0670669749379158 Score : 0.9538239240646362\n",
      "[478/1000]\n",
      "- Train Loss : 0.05706000224745367 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0671626627445221 Score : 0.9538239240646362\n",
      "[479/1000]\n",
      "- Train Loss : 0.057021938511752523 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06725559383630753 Score : 0.9538239240646362\n",
      "[480/1000]\n",
      "- Train Loss : 0.05698383716662647 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06734687834978104 Score : 0.9538239240646362\n",
      "[481/1000]\n",
      "- Train Loss : 0.05694568629405694 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06743913143873215 Score : 0.9538239240646362\n",
      "[482/1000]\n",
      "- Train Loss : 0.05690756903641159 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06753258407115936 Score : 0.9538239240646362\n",
      "[483/1000]\n",
      "- Train Loss : 0.05686963436892256 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06762589514255524 Score : 0.9538239240646362\n",
      "[484/1000]\n",
      "- Train Loss : 0.05683179343759548 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06771879643201828 Score : 0.9538239240646362\n",
      "[485/1000]\n",
      "- Train Loss : 0.05679398054780904 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06781211495399475 Score : 0.9538239240646362\n",
      "[486/1000]\n",
      "- Train Loss : 0.05675627402524697 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0679054707288742 Score : 0.9538239240646362\n",
      "[487/1000]\n",
      "- Train Loss : 0.056718613966950215 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06799932569265366 Score : 0.9538239240646362\n",
      "[488/1000]\n",
      "- Train Loss : 0.056681082722207066 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06809285283088684 Score : 0.9538239240646362\n",
      "[489/1000]\n",
      "- Train Loss : 0.05664360444643535 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0681867003440857 Score : 0.9538239240646362\n",
      "[490/1000]\n",
      "- Train Loss : 0.05660624431766337 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06828044354915619 Score : 0.9538239240646362\n",
      "[491/1000]\n",
      "- Train Loss : 0.05656892390106805 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06837467104196548 Score : 0.9538239240646362\n",
      "[492/1000]\n",
      "- Train Loss : 0.05653164457180537 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06846922636032104 Score : 0.9538239240646362\n",
      "[493/1000]\n",
      "- Train Loss : 0.05649454034573864 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06856334209442139 Score : 0.9538239240646362\n",
      "[494/1000]\n",
      "- Train Loss : 0.05645742251363117 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06865786761045456 Score : 0.9538239240646362\n",
      "[495/1000]\n",
      "- Train Loss : 0.05642040465318132 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06875251978635788 Score : 0.9538239240646362\n",
      "[496/1000]\n",
      "- Train Loss : 0.05638343819009606 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06884748488664627 Score : 0.9538239240646362\n",
      "[497/1000]\n",
      "- Train Loss : 0.05634661948715802 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06894226372241974 Score : 0.9538239240646362\n",
      "[498/1000]\n",
      "- Train Loss : 0.056309777064598165 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06903738528490067 Score : 0.9538239240646362\n",
      "[499/1000]\n",
      "- Train Loss : 0.056273086156579666 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06913264095783234 Score : 0.9538239240646362\n",
      "[500/1000]\n",
      "- Train Loss : 0.056236456119222566 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06922777742147446 Score : 0.9538239240646362\n",
      "[501/1000]\n",
      "- Train Loss : 0.05619988658872899 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06932329386472702 Score : 0.9538239240646362\n",
      "[502/1000]\n",
      "- Train Loss : 0.05616340292908717 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06941884756088257 Score : 0.9538239240646362\n",
      "[503/1000]\n",
      "- Train Loss : 0.05612698370532598 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0695144310593605 Score : 0.9538239240646362\n",
      "[504/1000]\n",
      "- Train Loss : 0.05609061562427087 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06961052119731903 Score : 0.9538239240646362\n",
      "[505/1000]\n",
      "- Train Loss : 0.05605433105665725 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06970631331205368 Score : 0.9538239240646362\n",
      "[506/1000]\n",
      "- Train Loss : 0.05601815569389146 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06980238854885101 Score : 0.9538239240646362\n",
      "[507/1000]\n",
      "- Train Loss : 0.055981979516218416 Score : 0.9708994776010513\n",
      "- Val Loss : 0.06989859789609909 Score : 0.9538239240646362\n",
      "[508/1000]\n",
      "- Train Loss : 0.055945928164874204 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0699952021241188 Score : 0.9538239240646362\n",
      "[509/1000]\n",
      "- Train Loss : 0.05590991026110714 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07009170204401016 Score : 0.9538239240646362\n",
      "[510/1000]\n",
      "- Train Loss : 0.055873938341392204 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07018810510635376 Score : 0.9538239240646362\n",
      "[511/1000]\n",
      "- Train Loss : 0.05583804793423042 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07028509676456451 Score : 0.9538239240646362\n",
      "[512/1000]\n",
      "- Train Loss : 0.05580470224231249 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07035742700099945 Score : 0.9538239240646362\n",
      "[513/1000]\n",
      "- Train Loss : 0.05576813334482722 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07044250518083572 Score : 0.9538239240646362\n",
      "[514/1000]\n",
      "- Train Loss : 0.05573131025448674 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07054762542247772 Score : 0.9538239240646362\n",
      "[515/1000]\n",
      "- Train Loss : 0.055695917646517046 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07065138965845108 Score : 0.9538239240646362\n",
      "[516/1000]\n",
      "- Train Loss : 0.0556609984851093 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07074636965990067 Score : 0.9538239240646362\n",
      "[517/1000]\n",
      "- Train Loss : 0.055625670924200676 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07084042578935623 Score : 0.9538239240646362\n",
      "[518/1000]\n",
      "- Train Loss : 0.05559013548918301 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07093822956085205 Score : 0.9538239240646362\n",
      "[519/1000]\n",
      "- Train Loss : 0.05555478979658801 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07103705406188965 Score : 0.9538239240646362\n",
      "[520/1000]\n",
      "- Train Loss : 0.05551969323278172 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07113496214151382 Score : 0.9538239240646362\n",
      "[521/1000]\n",
      "- Train Loss : 0.05548456320684636 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07123211771249771 Score : 0.9538239240646362\n",
      "[522/1000]\n",
      "- Train Loss : 0.05544946127338335 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07132986932992935 Score : 0.9538239240646362\n",
      "[523/1000]\n",
      "- Train Loss : 0.05541442068351898 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07142815738916397 Score : 0.9538239240646362\n",
      "[524/1000]\n",
      "- Train Loss : 0.055379419136443175 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07152650505304337 Score : 0.9538239240646362\n",
      "[525/1000]\n",
      "- Train Loss : 0.05534454335429473 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07162470370531082 Score : 0.9538239240646362\n",
      "[526/1000]\n",
      "- Train Loss : 0.05530972606356954 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07172305136919022 Score : 0.9538239240646362\n",
      "[527/1000]\n",
      "- Train Loss : 0.05527491809334606 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07182164490222931 Score : 0.9538239240646362\n",
      "[528/1000]\n",
      "- Train Loss : 0.055240180059627164 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07192061841487885 Score : 0.9538239240646362\n",
      "[529/1000]\n",
      "- Train Loss : 0.05520553526002914 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07201921194791794 Score : 0.9538239240646362\n",
      "[530/1000]\n",
      "- Train Loss : 0.055170922983961646 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07211806625127792 Score : 0.9538239240646362\n",
      "[531/1000]\n",
      "- Train Loss : 0.055136375114670955 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07221714407205582 Score : 0.9538239240646362\n",
      "[532/1000]\n",
      "- Train Loss : 0.055101840960560367 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07231636345386505 Score : 0.9538239240646362\n",
      "[533/1000]\n",
      "- Train Loss : 0.0550673999969149 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07241567224264145 Score : 0.9538239240646362\n",
      "[534/1000]\n",
      "- Train Loss : 0.05503302912984509 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07251501828432083 Score : 0.9538239240646362\n",
      "[535/1000]\n",
      "- Train Loss : 0.054998683954181615 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07261483371257782 Score : 0.9538239240646362\n",
      "[536/1000]\n",
      "- Train Loss : 0.05496441578725353 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07271432876586914 Score : 0.9538239240646362\n",
      "[537/1000]\n",
      "- Train Loss : 0.054930192782194354 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07281415164470673 Score : 0.9538239240646362\n",
      "[538/1000]\n",
      "- Train Loss : 0.0548959884326905 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07291434705257416 Score : 0.9538239240646362\n",
      "[539/1000]\n",
      "- Train Loss : 0.054861890348547604 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07301431894302368 Score : 0.9538239240646362\n",
      "[540/1000]\n",
      "- Train Loss : 0.054827859268698376 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07311428338289261 Score : 0.9538239240646362\n",
      "[541/1000]\n",
      "- Train Loss : 0.054793794282886665 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07321452349424362 Score : 0.9538239240646362\n",
      "[542/1000]\n",
      "- Train Loss : 0.054759839083999395 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07331504672765732 Score : 0.9538239240646362\n",
      "[543/1000]\n",
      "- Train Loss : 0.054726610687794164 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07340758293867111 Score : 0.9538239240646362\n",
      "[544/1000]\n",
      "- Train Loss : 0.05469242585240863 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0735054686665535 Score : 0.9538239240646362\n",
      "[545/1000]\n",
      "- Train Loss : 0.05465831120091025 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0736091211438179 Score : 0.9538239240646362\n",
      "[546/1000]\n",
      "- Train Loss : 0.05462465279561002 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0737118273973465 Score : 0.9538239240646362\n",
      "[547/1000]\n",
      "- Train Loss : 0.054591154264926445 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07381177693605423 Score : 0.9538239240646362\n",
      "[548/1000]\n",
      "- Train Loss : 0.054557533949264325 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07391153275966644 Score : 0.9538239240646362\n",
      "[549/1000]\n",
      "- Train Loss : 0.05452383729425492 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07401322573423386 Score : 0.9538239240646362\n",
      "[550/1000]\n",
      "- Train Loss : 0.05449035726633156 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07411497831344604 Score : 0.9538239240646362\n",
      "[551/1000]\n",
      "- Train Loss : 0.05445689281623345 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07421650737524033 Score : 0.9538239240646362\n",
      "[552/1000]\n",
      "- Train Loss : 0.05442350250814343 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0743178129196167 Score : 0.9538239240646362\n",
      "[553/1000]\n",
      "- Train Loss : 0.054390093147958396 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07441957294940948 Score : 0.9538239240646362\n",
      "[554/1000]\n",
      "- Train Loss : 0.054356762739189435 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07452128827571869 Score : 0.9538239240646362\n",
      "[555/1000]\n",
      "- Train Loss : 0.05432350673072506 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07462318241596222 Score : 0.9538239240646362\n",
      "[556/1000]\n",
      "- Train Loss : 0.054290262964059366 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07472524791955948 Score : 0.9538239240646362\n",
      "[557/1000]\n",
      "- Train Loss : 0.05425708236725768 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07482756674289703 Score : 0.9538239240646362\n",
      "[558/1000]\n",
      "- Train Loss : 0.05422393582557561 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07492965459823608 Score : 0.9538239240646362\n",
      "[559/1000]\n",
      "- Train Loss : 0.054190823175304104 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07503216713666916 Score : 0.9538239240646362\n",
      "[560/1000]\n",
      "- Train Loss : 0.05415783769421978 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07513463497161865 Score : 0.9538239240646362\n",
      "[561/1000]\n",
      "- Train Loss : 0.054124833150126506 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07523716986179352 Score : 0.9538239240646362\n",
      "[562/1000]\n",
      "- Train Loss : 0.0540918642655015 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07534020394086838 Score : 0.9538239240646362\n",
      "[563/1000]\n",
      "- Train Loss : 0.05405896136289812 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07544299215078354 Score : 0.9538239240646362\n",
      "[564/1000]\n",
      "- Train Loss : 0.05402611180397798 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07554585486650467 Score : 0.9538239240646362\n",
      "[565/1000]\n",
      "- Train Loss : 0.053993268233170966 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07564924657344818 Score : 0.9538239240646362\n",
      "[566/1000]\n",
      "- Train Loss : 0.05396053343793028 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0757526159286499 Score : 0.9538239240646362\n",
      "[567/1000]\n",
      "- Train Loss : 0.05392783565548598 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07585562765598297 Score : 0.9538239240646362\n",
      "[568/1000]\n",
      "- Train Loss : 0.05389512386682327 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07595938444137573 Score : 0.9538239240646362\n",
      "[569/1000]\n",
      "- Train Loss : 0.053862496326473774 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07606305927038193 Score : 0.9538239240646362\n",
      "[570/1000]\n",
      "- Train Loss : 0.0538299067193293 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0761667937040329 Score : 0.9538239240646362\n",
      "[571/1000]\n",
      "- Train Loss : 0.053797389613464475 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07627059519290924 Score : 0.9538239240646362\n",
      "[572/1000]\n",
      "- Train Loss : 0.053764836560731055 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07637452334165573 Score : 0.9538239240646362\n",
      "[573/1000]\n",
      "- Train Loss : 0.0537323865100916 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07647876441478729 Score : 0.9538239240646362\n",
      "[574/1000]\n",
      "- Train Loss : 0.053699953143222956 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07658301293849945 Score : 0.9538239240646362\n",
      "[575/1000]\n",
      "- Train Loss : 0.05366760511969915 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07668720930814743 Score : 0.9538239240646362\n",
      "[576/1000]\n",
      "- Train Loss : 0.05363525507345912 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07679186016321182 Score : 0.9538239240646362\n",
      "[577/1000]\n",
      "- Train Loss : 0.053602951898938045 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0768963098526001 Score : 0.9538239240646362\n",
      "[578/1000]\n",
      "- Train Loss : 0.0535707262060896 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07700101286172867 Score : 0.9538239240646362\n",
      "[579/1000]\n",
      "- Train Loss : 0.05353851634936291 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07710597664117813 Score : 0.9538239240646362\n",
      "[580/1000]\n",
      "- Train Loss : 0.053506325850321446 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07721108198165894 Score : 0.9538239240646362\n",
      "[581/1000]\n",
      "- Train Loss : 0.053474237967748195 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07731606811285019 Score : 0.9538239240646362\n",
      "[582/1000]\n",
      "- Train Loss : 0.05344211499323137 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07742108404636383 Score : 0.9538239240646362\n",
      "[583/1000]\n",
      "- Train Loss : 0.05341005053560366 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07752668112516403 Score : 0.9538239240646362\n",
      "[584/1000]\n",
      "- Train Loss : 0.0533780305122491 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07763204723596573 Score : 0.9538239240646362\n",
      "[585/1000]\n",
      "- Train Loss : 0.05334609863712103 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07773753255605698 Score : 0.9538239240646362\n",
      "[586/1000]\n",
      "- Train Loss : 0.0533141279265692 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07784344255924225 Score : 0.9538239240646362\n",
      "[587/1000]\n",
      "- Train Loss : 0.05328224876939203 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07794950157403946 Score : 0.9538239240646362\n",
      "[588/1000]\n",
      "- Train Loss : 0.05325041786272777 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07805507630109787 Score : 0.9538239240646362\n",
      "[589/1000]\n",
      "- Train Loss : 0.05321853608984384 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07816118001937866 Score : 0.9538239240646362\n",
      "[590/1000]\n",
      "- Train Loss : 0.05318679692209116 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07826734334230423 Score : 0.9538239240646362\n",
      "[591/1000]\n",
      "- Train Loss : 0.053155030065681785 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07837378978729248 Score : 0.9538239240646362\n",
      "[592/1000]\n",
      "- Train Loss : 0.053123307134228526 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07848022133111954 Score : 0.9538239240646362\n",
      "[593/1000]\n",
      "- Train Loss : 0.05309167988525587 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0785868763923645 Score : 0.9538239240646362\n",
      "[594/1000]\n",
      "- Train Loss : 0.05306003338773735 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0786934494972229 Score : 0.9538239240646362\n",
      "[595/1000]\n",
      "- Train Loss : 0.05302842249511741 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07880005985498428 Score : 0.9538239240646362\n",
      "[596/1000]\n",
      "- Train Loss : 0.05299681068936479 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07890728861093521 Score : 0.9538239240646362\n",
      "[597/1000]\n",
      "- Train Loss : 0.05296530133637134 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07901432365179062 Score : 0.9538239240646362\n",
      "[598/1000]\n",
      "- Train Loss : 0.052933861188648734 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07912146300077438 Score : 0.9538239240646362\n",
      "[599/1000]\n",
      "- Train Loss : 0.05290238798261271 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07922878116369247 Score : 0.9538239240646362\n",
      "[600/1000]\n",
      "- Train Loss : 0.052870983599859755 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07933630049228668 Score : 0.9538239240646362\n",
      "[601/1000]\n",
      "- Train Loss : 0.0528395763503795 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07944376021623611 Score : 0.9538239240646362\n",
      "[602/1000]\n",
      "- Train Loss : 0.052808252432441805 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07955144345760345 Score : 0.9538239240646362\n",
      "[603/1000]\n",
      "- Train Loss : 0.052776936048758216 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07965915650129318 Score : 0.9538239240646362\n",
      "[604/1000]\n",
      "- Train Loss : 0.052745626868272666 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07976704090833664 Score : 0.9538239240646362\n",
      "[605/1000]\n",
      "- Train Loss : 0.052714437264512526 Score : 0.9708994776010513\n",
      "- Val Loss : 0.07987505197525024 Score : 0.9538239240646362\n",
      "[606/1000]\n",
      "- Train Loss : 0.0526831919887627 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0799831971526146 Score : 0.9538239240646362\n",
      "[607/1000]\n",
      "- Train Loss : 0.05265203490489512 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08009153604507446 Score : 0.9538239240646362\n",
      "[608/1000]\n",
      "- Train Loss : 0.052620883976487676 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0801999494433403 Score : 0.9538239240646362\n",
      "[609/1000]\n",
      "- Train Loss : 0.05258979712380096 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0803084746003151 Score : 0.9538239240646362\n",
      "[610/1000]\n",
      "- Train Loss : 0.05255870874316315 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08041710406541824 Score : 0.9538239240646362\n",
      "[611/1000]\n",
      "- Train Loss : 0.0525276916087023 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08052593469619751 Score : 0.9538239240646362\n",
      "[612/1000]\n",
      "- Train Loss : 0.05249667701718863 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08063489943742752 Score : 0.9538239240646362\n",
      "[613/1000]\n",
      "- Train Loss : 0.05246573363911011 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08074339479207993 Score : 0.9538239240646362\n",
      "[614/1000]\n",
      "- Train Loss : 0.05243474144299398 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08085273206233978 Score : 0.9538239240646362\n",
      "[615/1000]\n",
      "- Train Loss : 0.052403833586140536 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08096210658550262 Score : 0.9538239240646362\n",
      "[616/1000]\n",
      "- Train Loss : 0.052372984624526 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08107150346040726 Score : 0.9538239240646362\n",
      "[617/1000]\n",
      "- Train Loss : 0.05234211117931409 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08118084818124771 Score : 0.9538239240646362\n",
      "[618/1000]\n",
      "- Train Loss : 0.05231129068124574 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08129046112298965 Score : 0.9538239240646362\n",
      "[619/1000]\n",
      "- Train Loss : 0.05228049874131102 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08140043169260025 Score : 0.9538239240646362\n",
      "[620/1000]\n",
      "- Train Loss : 0.052249768650654005 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08151019364595413 Score : 0.9538239240646362\n",
      "[621/1000]\n",
      "- Train Loss : 0.052219104509276804 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08162020146846771 Score : 0.9538239240646362\n",
      "[622/1000]\n",
      "- Train Loss : 0.05218842412068625 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08173052966594696 Score : 0.9538239240646362\n",
      "[623/1000]\n",
      "- Train Loss : 0.052157756297674496 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08184057474136353 Score : 0.9538239240646362\n",
      "[624/1000]\n",
      "- Train Loss : 0.052127197028312366 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08195088803768158 Score : 0.9538239240646362\n",
      "[625/1000]\n",
      "- Train Loss : 0.05209660563195939 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08206155151128769 Score : 0.9538239240646362\n",
      "[626/1000]\n",
      "- Train Loss : 0.052066044889215846 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08217229694128036 Score : 0.9538239240646362\n",
      "[627/1000]\n",
      "- Train Loss : 0.052035582724784035 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08228279650211334 Score : 0.9538239240646362\n",
      "[628/1000]\n",
      "- Train Loss : 0.052005030014697695 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08239366114139557 Score : 0.9538239240646362\n",
      "[629/1000]\n",
      "- Train Loss : 0.051974555899505503 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08250495046377182 Score : 0.9538239240646362\n",
      "[630/1000]\n",
      "- Train Loss : 0.05194416020458448 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08261574804782867 Score : 0.9538239240646362\n",
      "[631/1000]\n",
      "- Train Loss : 0.05191374454807374 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08272720873355865 Score : 0.9538239240646362\n",
      "[632/1000]\n",
      "- Train Loss : 0.05188339344022097 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08283849060535431 Score : 0.9538239240646362\n",
      "[633/1000]\n",
      "- Train Loss : 0.051853054286766564 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08294977247714996 Score : 0.9538239240646362\n",
      "[634/1000]\n",
      "- Train Loss : 0.051822722558426904 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08306185901165009 Score : 0.9538239240646362\n",
      "[635/1000]\n",
      "- Train Loss : 0.051792451329674805 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0831737145781517 Score : 0.9538239240646362\n",
      "[636/1000]\n",
      "- Train Loss : 0.051762239963863976 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0832853689789772 Score : 0.9538239240646362\n",
      "[637/1000]\n",
      "- Train Loss : 0.051732052215811564 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08339719474315643 Score : 0.9538239240646362\n",
      "[638/1000]\n",
      "- Train Loss : 0.05170182113943156 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08350943773984909 Score : 0.9538239240646362\n",
      "[639/1000]\n",
      "- Train Loss : 0.051671680637809914 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08362185209989548 Score : 0.9538239240646362\n",
      "[640/1000]\n",
      "- Train Loss : 0.051641554451634875 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0837341919541359 Score : 0.9538239240646362\n",
      "[641/1000]\n",
      "- Train Loss : 0.05161145176680293 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08384677022695541 Score : 0.9538239240646362\n",
      "[642/1000]\n",
      "- Train Loss : 0.05158136940008262 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08395946025848389 Score : 0.9538239240646362\n",
      "[643/1000]\n",
      "- Train Loss : 0.051551359705626965 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08407188951969147 Score : 0.9538239240646362\n",
      "[644/1000]\n",
      "- Train Loss : 0.051521336841688026 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08418464660644531 Score : 0.9538239240646362\n",
      "[645/1000]\n",
      "- Train Loss : 0.05149130957215675 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08429813385009766 Score : 0.9538239240646362\n",
      "[646/1000]\n",
      "- Train Loss : 0.05146140924261999 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08441126346588135 Score : 0.9538239240646362\n",
      "[647/1000]\n",
      "- Train Loss : 0.05143144965040847 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08452459424734116 Score : 0.9538239240646362\n",
      "[648/1000]\n",
      "- Train Loss : 0.051401565069681965 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08463791757822037 Score : 0.9538239240646362\n",
      "[649/1000]\n",
      "- Train Loss : 0.05137170232410426 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08475124835968018 Score : 0.9538239240646362\n",
      "[650/1000]\n",
      "- Train Loss : 0.05134182784240693 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08486498892307281 Score : 0.9538239240646362\n",
      "[651/1000]\n",
      "- Train Loss : 0.05131196080765221 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08497888594865799 Score : 0.9538239240646362\n",
      "[652/1000]\n",
      "- Train Loss : 0.05128221206905437 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0850927010178566 Score : 0.9538239240646362\n",
      "[653/1000]\n",
      "- Train Loss : 0.051252437089715386 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08520669490098953 Score : 0.9538239240646362\n",
      "[654/1000]\n",
      "- Train Loss : 0.05122262422446511 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08532120287418365 Score : 0.9538239240646362\n",
      "[655/1000]\n",
      "- Train Loss : 0.05119292764356942 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08543518930673599 Score : 0.9538239240646362\n",
      "[656/1000]\n",
      "- Train Loss : 0.05116322209505597 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08554952591657639 Score : 0.9538239240646362\n",
      "[657/1000]\n",
      "- Train Loss : 0.05113354654167779 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08566419780254364 Score : 0.9538239240646362\n",
      "[658/1000]\n",
      "- Train Loss : 0.051103896661516046 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08577904850244522 Score : 0.9538239240646362\n",
      "[659/1000]\n",
      "- Train Loss : 0.051074324088403955 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08589371293783188 Score : 0.9538239240646362\n",
      "[660/1000]\n",
      "- Train Loss : 0.05104469160141889 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08600864559412003 Score : 0.9538239240646362\n",
      "[661/1000]\n",
      "- Train Loss : 0.05101509786982206 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08612377941608429 Score : 0.9538239240646362\n",
      "[662/1000]\n",
      "- Train Loss : 0.05098555392032722 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08623890578746796 Score : 0.9538239240646362\n",
      "[663/1000]\n",
      "- Train Loss : 0.0509560273567331 Score : 0.9708994776010513\n",
      "- Val Loss : 0.086354099214077 Score : 0.9538239240646362\n",
      "[664/1000]\n",
      "- Train Loss : 0.05092654054169543 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08646952360868454 Score : 0.9538239240646362\n",
      "[665/1000]\n",
      "- Train Loss : 0.050897064047603635 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08658543974161148 Score : 0.9538239240646362\n",
      "[666/1000]\n",
      "- Train Loss : 0.05086763174767839 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08670125901699066 Score : 0.9538239240646362\n",
      "[667/1000]\n",
      "- Train Loss : 0.050838243016187334 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08681667596101761 Score : 0.9538239240646362\n",
      "[668/1000]\n",
      "- Train Loss : 0.05080876471402007 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0869327113032341 Score : 0.9538239240646362\n",
      "[669/1000]\n",
      "- Train Loss : 0.050779426041117404 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08704890310764313 Score : 0.9538239240646362\n",
      "[670/1000]\n",
      "- Train Loss : 0.050750025555316824 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0871649757027626 Score : 0.9538239240646362\n",
      "[671/1000]\n",
      "- Train Loss : 0.0507207227346953 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08728138357400894 Score : 0.9538239240646362\n",
      "[672/1000]\n",
      "- Train Loss : 0.05069142379943514 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08739800751209259 Score : 0.9538239240646362\n",
      "[673/1000]\n",
      "- Train Loss : 0.05066215740953339 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08751442283391953 Score : 0.9538239240646362\n",
      "[674/1000]\n",
      "- Train Loss : 0.05063286873701145 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08763118833303452 Score : 0.9538239240646362\n",
      "[675/1000]\n",
      "- Train Loss : 0.05060357590264175 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08774863928556442 Score : 0.9538239240646362\n",
      "[676/1000]\n",
      "- Train Loss : 0.0505744471156504 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08786576986312866 Score : 0.9538239240646362\n",
      "[677/1000]\n",
      "- Train Loss : 0.050545208065159386 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08798261731863022 Score : 0.9538239240646362\n",
      "[678/1000]\n",
      "- Train Loss : 0.05051603483298095 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0880998969078064 Score : 0.9538239240646362\n",
      "[679/1000]\n",
      "- Train Loss : 0.05048689429895603 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08821741491556168 Score : 0.9538239240646362\n",
      "[680/1000]\n",
      "- Train Loss : 0.05045776887709508 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0883350521326065 Score : 0.9538239240646362\n",
      "[681/1000]\n",
      "- Train Loss : 0.050428665254003135 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08845256268978119 Score : 0.9538239240646362\n",
      "[682/1000]\n",
      "- Train Loss : 0.05039956094697118 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08857046067714691 Score : 0.9538239240646362\n",
      "[683/1000]\n",
      "- Train Loss : 0.05037054280546727 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08868847042322159 Score : 0.9538239240646362\n",
      "[684/1000]\n",
      "- Train Loss : 0.05034149409766542 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08880621194839478 Score : 0.9538239240646362\n",
      "[685/1000]\n",
      "- Train Loss : 0.050312458395637805 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08892454952001572 Score : 0.9538239240646362\n",
      "[686/1000]\n",
      "- Train Loss : 0.050283534790651174 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08904266357421875 Score : 0.9538239240646362\n",
      "[687/1000]\n",
      "- Train Loss : 0.05025451448454987 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08916135877370834 Score : 0.9538239240646362\n",
      "[688/1000]\n",
      "- Train Loss : 0.05022550680223503 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08927984535694122 Score : 0.9538239240646362\n",
      "[689/1000]\n",
      "- Train Loss : 0.05019658579840325 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0893988162279129 Score : 0.9538239240646362\n",
      "[690/1000]\n",
      "- Train Loss : 0.050167762901764945 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0895174965262413 Score : 0.9538239240646362\n",
      "[691/1000]\n",
      "- Train Loss : 0.05013879935722798 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08963645994663239 Score : 0.9538239240646362\n",
      "[692/1000]\n",
      "- Train Loss : 0.050109926363802515 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08975569903850555 Score : 0.9538239240646362\n",
      "[693/1000]\n",
      "- Train Loss : 0.05008109715345199 Score : 0.9708994776010513\n",
      "- Val Loss : 0.08987496793270111 Score : 0.9538239240646362\n",
      "[694/1000]\n",
      "- Train Loss : 0.05005226624052739 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0899943932890892 Score : 0.9538239240646362\n",
      "[695/1000]\n",
      "- Train Loss : 0.050023511104882346 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0901135727763176 Score : 0.9538239240646362\n",
      "[696/1000]\n",
      "- Train Loss : 0.0499946738818835 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0902332067489624 Score : 0.9538239240646362\n",
      "[697/1000]\n",
      "- Train Loss : 0.04996589992151712 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09035319834947586 Score : 0.9538239240646362\n",
      "[698/1000]\n",
      "- Train Loss : 0.04993714359989099 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09047319740056992 Score : 0.9538239240646362\n",
      "[699/1000]\n",
      "- Train Loss : 0.04990849496607552 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09059292078018188 Score : 0.9538239240646362\n",
      "[700/1000]\n",
      "- Train Loss : 0.04987975477706641 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09071322530508041 Score : 0.9538239240646362\n",
      "[701/1000]\n",
      "- Train Loss : 0.049851085675982176 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09083373844623566 Score : 0.9538239240646362\n",
      "[702/1000]\n",
      "- Train Loss : 0.04982246730287443 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0909540131688118 Score : 0.9538239240646362\n",
      "[703/1000]\n",
      "- Train Loss : 0.049793805763329146 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09107426553964615 Score : 0.9538239240646362\n",
      "[704/1000]\n",
      "- Train Loss : 0.04976517497743771 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09119512140750885 Score : 0.9538239240646362\n",
      "[705/1000]\n",
      "- Train Loss : 0.049736589193344116 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09131628274917603 Score : 0.9538239240646362\n",
      "[706/1000]\n",
      "- Train Loss : 0.049708033258866635 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09143710136413574 Score : 0.9538239240646362\n",
      "[707/1000]\n",
      "- Train Loss : 0.049679456706144265 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09155819565057755 Score : 0.9538239240646362\n",
      "[708/1000]\n",
      "- Train Loss : 0.04965089760298724 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09167955815792084 Score : 0.9538239240646362\n",
      "[709/1000]\n",
      "- Train Loss : 0.0496223733680381 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09180086106061935 Score : 0.9538239240646362\n",
      "[710/1000]\n",
      "- Train Loss : 0.04959391860757023 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0919225886464119 Score : 0.9538239240646362\n",
      "[711/1000]\n",
      "- Train Loss : 0.04956540911734919 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0920441672205925 Score : 0.9538239240646362\n",
      "[712/1000]\n",
      "- Train Loss : 0.049536943890416296 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0921657532453537 Score : 0.9538239240646362\n",
      "[713/1000]\n",
      "- Train Loss : 0.04950848494809179 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09228802472352982 Score : 0.9538239240646362\n",
      "[714/1000]\n",
      "- Train Loss : 0.04948009045619983 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09240996092557907 Score : 0.9538239240646362\n",
      "[715/1000]\n",
      "- Train Loss : 0.049451663693616865 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09253207594156265 Score : 0.9538239240646362\n",
      "[716/1000]\n",
      "- Train Loss : 0.04942329269579204 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09265444427728653 Score : 0.9538239240646362\n",
      "[717/1000]\n",
      "- Train Loss : 0.04939493965321162 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09277690201997757 Score : 0.9538239240646362\n",
      "[718/1000]\n",
      "- Train Loss : 0.04936660014936933 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09289950877428055 Score : 0.9538239240646362\n",
      "[719/1000]\n",
      "- Train Loss : 0.049338326269207755 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09302214533090591 Score : 0.9538239240646362\n",
      "[720/1000]\n",
      "- Train Loss : 0.04930993962807406 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09314517676830292 Score : 0.9538239240646362\n",
      "[721/1000]\n",
      "- Train Loss : 0.04928163496879279 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09326858073472977 Score : 0.9538239240646362\n",
      "[722/1000]\n",
      "- Train Loss : 0.04925344367075013 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09339118003845215 Score : 0.9538239240646362\n",
      "[723/1000]\n",
      "- Train Loss : 0.049225143155126716 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09351445734500885 Score : 0.9538239240646362\n",
      "[724/1000]\n",
      "- Train Loss : 0.04919693854753859 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09363802522420883 Score : 0.9538239240646362\n",
      "[725/1000]\n",
      "- Train Loss : 0.049168656341862516 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0937616303563118 Score : 0.9538239240646362\n",
      "[726/1000]\n",
      "- Train Loss : 0.049140496403197176 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09388554841279984 Score : 0.9538239240646362\n",
      "[727/1000]\n",
      "- Train Loss : 0.04911229687786545 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09400917589664459 Score : 0.9538239240646362\n",
      "[728/1000]\n",
      "- Train Loss : 0.0490841396531323 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0941333919763565 Score : 0.9538239240646362\n",
      "[729/1000]\n",
      "- Train Loss : 0.04905602639519202 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09425757080316544 Score : 0.9538239240646362\n",
      "[730/1000]\n",
      "- Train Loss : 0.04902787594073743 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09438186883926392 Score : 0.9538239240646362\n",
      "[731/1000]\n",
      "- Train Loss : 0.048999803026163136 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09450632333755493 Score : 0.9538239240646362\n",
      "[732/1000]\n",
      "- Train Loss : 0.04897171004085976 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09463071823120117 Score : 0.9538239240646362\n",
      "[733/1000]\n",
      "- Train Loss : 0.04894361018523341 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09475550800561905 Score : 0.9538239240646362\n",
      "[734/1000]\n",
      "- Train Loss : 0.04891557397058932 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09488041698932648 Score : 0.9538239240646362\n",
      "[735/1000]\n",
      "- Train Loss : 0.04888754117200733 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09500549733638763 Score : 0.9538239240646362\n",
      "[736/1000]\n",
      "- Train Loss : 0.04885951619507978 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09513066709041595 Score : 0.9538239240646362\n",
      "[737/1000]\n",
      "- Train Loss : 0.0488315542388591 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09525549411773682 Score : 0.9538239240646362\n",
      "[738/1000]\n",
      "- Train Loss : 0.048803514011524385 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09538094699382782 Score : 0.9538239240646362\n",
      "[739/1000]\n",
      "- Train Loss : 0.04877549208140408 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09550680965185165 Score : 0.9538239240646362\n",
      "[740/1000]\n",
      "- Train Loss : 0.04874761670544103 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09563250094652176 Score : 0.9538239240646362\n",
      "[741/1000]\n",
      "- Train Loss : 0.048719692074882914 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0957581177353859 Score : 0.9538239240646362\n",
      "[742/1000]\n",
      "- Train Loss : 0.048691740241338266 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09588407725095749 Score : 0.9538239240646362\n",
      "[743/1000]\n",
      "- Train Loss : 0.048663860934539116 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09601040184497833 Score : 0.9538239240646362\n",
      "[744/1000]\n",
      "- Train Loss : 0.04863598988958984 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09613672643899918 Score : 0.9538239240646362\n",
      "[745/1000]\n",
      "- Train Loss : 0.04860810734862753 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09626328945159912 Score : 0.9538239240646362\n",
      "[746/1000]\n",
      "- Train Loss : 0.04858020415485953 Score : 0.9708994776010513\n",
      "- Val Loss : 0.096390001475811 Score : 0.9538239240646362\n",
      "[747/1000]\n",
      "- Train Loss : 0.04855245247927087 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09651641547679901 Score : 0.9538239240646362\n",
      "[748/1000]\n",
      "- Train Loss : 0.04852458682398719 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09664322435855865 Score : 0.9538239240646362\n",
      "[749/1000]\n",
      "- Train Loss : 0.04849682097119512 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09677032381296158 Score : 0.9538239240646362\n",
      "[750/1000]\n",
      "- Train Loss : 0.04846897098104819 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09689760208129883 Score : 0.9538239240646362\n",
      "[751/1000]\n",
      "- Train Loss : 0.0484412576934119 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09702493250370026 Score : 0.9538239240646362\n",
      "[752/1000]\n",
      "- Train Loss : 0.04841352436596935 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09715187549591064 Score : 0.9538239240646362\n",
      "[753/1000]\n",
      "- Train Loss : 0.048385745165433036 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09727960079908371 Score : 0.9538239240646362\n",
      "[754/1000]\n",
      "- Train Loss : 0.04835798063140828 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0974075123667717 Score : 0.9538239240646362\n",
      "[755/1000]\n",
      "- Train Loss : 0.048330338331652456 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09753532707691193 Score : 0.9538239240646362\n",
      "[756/1000]\n",
      "- Train Loss : 0.048303238978405716 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09765005856752396 Score : 0.9538239240646362\n",
      "[757/1000]\n",
      "- Train Loss : 0.04827512574229331 Score : 0.9708994776010513\n",
      "- Val Loss : 0.097775898873806 Score : 0.9538239240646362\n",
      "[758/1000]\n",
      "- Train Loss : 0.04824715071663377 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09790926426649094 Score : 0.9538239240646362\n",
      "[759/1000]\n",
      "- Train Loss : 0.04821975086633756 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0980396717786789 Score : 0.9538239240646362\n",
      "[760/1000]\n",
      "- Train Loss : 0.04819237842093571 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0981663390994072 Score : 0.9538239240646362\n",
      "[761/1000]\n",
      "- Train Loss : 0.04816482301430369 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09829362481832504 Score : 0.9538239240646362\n",
      "[762/1000]\n",
      "- Train Loss : 0.04813716447097249 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09842260181903839 Score : 0.9538239240646362\n",
      "[763/1000]\n",
      "- Train Loss : 0.048109631024999544 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09855262190103531 Score : 0.9538239240646362\n",
      "[764/1000]\n",
      "- Train Loss : 0.048082193308800925 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09868143498897552 Score : 0.9538239240646362\n",
      "[765/1000]\n",
      "- Train Loss : 0.048054727018097765 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09881007671356201 Score : 0.9538239240646362\n",
      "[766/1000]\n",
      "- Train Loss : 0.04802721230589668 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09893932193517685 Score : 0.9538239240646362\n",
      "[767/1000]\n",
      "- Train Loss : 0.04799974633169768 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09906908124685287 Score : 0.9538239240646362\n",
      "[768/1000]\n",
      "- Train Loss : 0.04797235025034752 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09919868409633636 Score : 0.9538239240646362\n",
      "[769/1000]\n",
      "- Train Loss : 0.04794486811806564 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09932845830917358 Score : 0.9538239240646362\n",
      "[770/1000]\n",
      "- Train Loss : 0.04791754063262488 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09945813566446304 Score : 0.9538239240646362\n",
      "[771/1000]\n",
      "- Train Loss : 0.0478900489397347 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09958846867084503 Score : 0.9538239240646362\n",
      "[772/1000]\n",
      "- Train Loss : 0.04786270085241995 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09971880912780762 Score : 0.9538239240646362\n",
      "[773/1000]\n",
      "- Train Loss : 0.04783533263616846 Score : 0.9708994776010513\n",
      "- Val Loss : 0.09984902292490005 Score : 0.9538239240646362\n",
      "[774/1000]\n",
      "- Train Loss : 0.047807959559577284 Score : 0.9708994776010513\n",
      "- Val Loss : 0.0999796986579895 Score : 0.9538239240646362\n",
      "[775/1000]\n",
      "- Train Loss : 0.047780653374502435 Score : 0.9708994776010513\n",
      "- Val Loss : 0.10010989010334015 Score : 0.9538239240646362\n",
      "[776/1000]\n",
      "- Train Loss : 0.04775328008145152 Score : 0.9708994776010513\n",
      "- Val Loss : 0.1002410501241684 Score : 0.9538239240646362\n",
      "[777/1000]\n",
      "- Train Loss : 0.04772601809054322 Score : 0.9708994776010513\n",
      "- Val Loss : 0.10037223994731903 Score : 0.9538239240646362\n",
      "[778/1000]\n",
      "- Train Loss : 0.04769873954501236 Score : 0.9708994776010513\n",
      "- Val Loss : 0.10050320625305176 Score : 0.9538239240646362\n",
      "[779/1000]\n",
      "- Train Loss : 0.04767138556780992 Score : 0.9708994776010513\n",
      "- Val Loss : 0.10063444823026657 Score : 0.9538239240646362\n",
      "[780/1000]\n",
      "- Train Loss : 0.04764415501267649 Score : 0.9708994776010513\n",
      "- Val Loss : 0.10076570510864258 Score : 0.9538239240646362\n",
      "[781/1000]\n",
      "- Train Loss : 0.04761689513725287 Score : 0.9708994776010513\n",
      "- Val Loss : 0.10089714825153351 Score : 0.9538239240646362\n",
      "[782/1000]\n",
      "- Train Loss : 0.04758964322900283 Score : 0.9708994776010513\n",
      "- Val Loss : 0.10102895647287369 Score : 0.9538239240646362\n",
      "[783/1000]\n",
      "- Train Loss : 0.04756242049188586 Score : 0.9708994776010513\n",
      "- Val Loss : 0.10116098076105118 Score : 0.9538239240646362\n",
      "[784/1000]\n",
      "- Train Loss : 0.04753522166356561 Score : 0.9708994776010513\n",
      "- Val Loss : 0.10129290819168091 Score : 0.9538239240646362\n",
      "[785/1000]\n",
      "- Train Loss : 0.047508000907328096 Score : 0.9708994776010513\n",
      "- Val Loss : 0.10142502933740616 Score : 0.9538239240646362\n",
      "[786/1000]\n",
      "- Train Loss : 0.04748084573475353 Score : 0.9708994776010513\n",
      "- Val Loss : 0.10155726224184036 Score : 0.9538239240646362\n",
      "[787/1000]\n",
      "- Train Loss : 0.047453665578359505 Score : 0.9708994776010513\n",
      "- Val Loss : 0.10168975591659546 Score : 0.9538239240646362\n",
      "[788/1000]\n",
      "- Train Loss : 0.04742648149476736 Score : 0.9708994776010513\n",
      "- Val Loss : 0.10182251036167145 Score : 0.9538239240646362\n",
      "[789/1000]\n",
      "- Train Loss : 0.04739934598910622 Score : 0.9708994776010513\n",
      "- Val Loss : 0.1019553616642952 Score : 0.9538239240646362\n",
      "[790/1000]\n",
      "- Train Loss : 0.04737222825315257 Score : 0.9708994776010513\n",
      "- Val Loss : 0.10208821296691895 Score : 0.9538239240646362\n",
      "[791/1000]\n",
      "- Train Loss : 0.04734512017239467 Score : 0.9708994776010513\n",
      "- Val Loss : 0.1022210568189621 Score : 0.9538239240646362\n",
      "[792/1000]\n",
      "- Train Loss : 0.04731801676643954 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10235434025526047 Score : 0.9538239240646362\n",
      "[793/1000]\n",
      "- Train Loss : 0.047290904780311394 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10248792171478271 Score : 0.9538239240646362\n",
      "[794/1000]\n",
      "- Train Loss : 0.0472638602641382 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10262130945920944 Score : 0.9538239240646362\n",
      "[795/1000]\n",
      "- Train Loss : 0.04723681185532769 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10275495797395706 Score : 0.9538239240646362\n",
      "[796/1000]\n",
      "- Train Loss : 0.04720972511495347 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1028888002038002 Score : 0.9538239240646362\n",
      "[797/1000]\n",
      "- Train Loss : 0.047182717604300706 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10302294790744781 Score : 0.9538239240646362\n",
      "[798/1000]\n",
      "- Train Loss : 0.04715568615938537 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10315710306167603 Score : 0.9538239240646362\n",
      "[799/1000]\n",
      "- Train Loss : 0.047128704351052875 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10329138487577438 Score : 0.9538239240646362\n",
      "[800/1000]\n",
      "- Train Loss : 0.04710167330085824 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10342603921890259 Score : 0.9538239240646362\n",
      "[801/1000]\n",
      "- Train Loss : 0.04707475430041086 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10356055200099945 Score : 0.9538239240646362\n",
      "[802/1000]\n",
      "- Train Loss : 0.047047733338331454 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10369537770748138 Score : 0.9538239240646362\n",
      "[803/1000]\n",
      "- Train Loss : 0.047020818852615776 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10383032262325287 Score : 0.9538239240646362\n",
      "[804/1000]\n",
      "- Train Loss : 0.046993827127153054 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10396559536457062 Score : 0.9538239240646362\n",
      "[805/1000]\n",
      "- Train Loss : 0.04696690812124871 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10410071164369583 Score : 0.9538239240646362\n",
      "[806/1000]\n",
      "- Train Loss : 0.04693999707706098 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10423614084720612 Score : 0.9538239240646362\n",
      "[807/1000]\n",
      "- Train Loss : 0.046913102953112684 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10437154024839401 Score : 0.9538239240646362\n",
      "[808/1000]\n",
      "- Train Loss : 0.04688616847852245 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10450708121061325 Score : 0.9538239240646362\n",
      "[809/1000]\n",
      "- Train Loss : 0.04685926517777261 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10464327037334442 Score : 0.9538239240646362\n",
      "[810/1000]\n",
      "- Train Loss : 0.046832405796521925 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10477941483259201 Score : 0.9538239240646362\n",
      "[811/1000]\n",
      "- Train Loss : 0.04680552042555064 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10491538047790527 Score : 0.9538239240646362\n",
      "[812/1000]\n",
      "- Train Loss : 0.046778702879237244 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10505171120166779 Score : 0.9538239240646362\n",
      "[813/1000]\n",
      "- Train Loss : 0.04675184360166895 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1051882728934288 Score : 0.9538239240646362\n",
      "[814/1000]\n",
      "- Train Loss : 0.046725069183594314 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10532506555318832 Score : 0.9538239240646362\n",
      "[815/1000]\n",
      "- Train Loss : 0.0466982190246199 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10546188056468964 Score : 0.9538239240646362\n",
      "[816/1000]\n",
      "- Train Loss : 0.0466714599642728 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10559874027967453 Score : 0.9538239240646362\n",
      "[817/1000]\n",
      "- Train Loss : 0.046644686903164256 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10573586076498032 Score : 0.9538239240646362\n",
      "[818/1000]\n",
      "- Train Loss : 0.04661787779696169 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10587351769208908 Score : 0.9538239240646362\n",
      "[819/1000]\n",
      "- Train Loss : 0.04659115386857593 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10601071268320084 Score : 0.9538239240646362\n",
      "[820/1000]\n",
      "- Train Loss : 0.04656437505764188 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10614826530218124 Score : 0.9538239240646362\n",
      "[821/1000]\n",
      "- Train Loss : 0.04653764512295311 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10628587752580643 Score : 0.9538239240646362\n",
      "[822/1000]\n",
      "- Train Loss : 0.04651090388870216 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10642407834529877 Score : 0.9538239240646362\n",
      "[823/1000]\n",
      "- Train Loss : 0.04648425060622685 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10656232386827469 Score : 0.9538239240646362\n",
      "[824/1000]\n",
      "- Train Loss : 0.04645759590493981 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10670024156570435 Score : 0.9538239240646362\n",
      "[825/1000]\n",
      "- Train Loss : 0.046430856511506136 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10683850198984146 Score : 0.9538239240646362\n",
      "[826/1000]\n",
      "- Train Loss : 0.046404190430621384 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10697714239358902 Score : 0.9538239240646362\n",
      "[827/1000]\n",
      "- Train Loss : 0.04637785520571924 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1071106418967247 Score : 0.9538239240646362\n",
      "[828/1000]\n",
      "- Train Loss : 0.046351035443876754 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10724783688783646 Score : 0.9538239240646362\n",
      "[829/1000]\n",
      "- Train Loss : 0.04632422517533996 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10738902539014816 Score : 0.9538239240646362\n",
      "[830/1000]\n",
      "- Train Loss : 0.04629768201630213 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10752946138381958 Score : 0.9538239240646362\n",
      "[831/1000]\n",
      "- Train Loss : 0.04627110724868544 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10766836255788803 Score : 0.9538239240646362\n",
      "[832/1000]\n",
      "- Train Loss : 0.046244543029388296 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10780710726976395 Score : 0.9538239240646362\n",
      "[833/1000]\n",
      "- Train Loss : 0.046217952916776994 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10794681310653687 Score : 0.9538239240646362\n",
      "[834/1000]\n",
      "- Train Loss : 0.04619136893234099 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10808714479207993 Score : 0.9538239240646362\n",
      "[835/1000]\n",
      "- Train Loss : 0.04616487797193258 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10822715610265732 Score : 0.9538239240646362\n",
      "[836/1000]\n",
      "- Train Loss : 0.04613829992922547 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10836736857891083 Score : 0.9538239240646362\n",
      "[837/1000]\n",
      "- Train Loss : 0.04611177425886126 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10850746184587479 Score : 0.9538239240646362\n",
      "[838/1000]\n",
      "- Train Loss : 0.04608524602917896 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10864812880754471 Score : 0.9538239240646362\n",
      "[839/1000]\n",
      "- Train Loss : 0.046058749248913955 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10878857225179672 Score : 0.9538239240646362\n",
      "[840/1000]\n",
      "- Train Loss : 0.04603224621496338 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10892975330352783 Score : 0.9538239240646362\n",
      "[841/1000]\n",
      "- Train Loss : 0.046005781543499324 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10907066613435745 Score : 0.9538239240646362\n",
      "[842/1000]\n",
      "- Train Loss : 0.045979297181474976 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1092119812965393 Score : 0.9538239240646362\n",
      "[843/1000]\n",
      "- Train Loss : 0.045952883902828034 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10935308784246445 Score : 0.9538239240646362\n",
      "[844/1000]\n",
      "- Train Loss : 0.04592639445945679 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10949474573135376 Score : 0.9538239240646362\n",
      "[845/1000]\n",
      "- Train Loss : 0.04589993852459884 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10963660478591919 Score : 0.9538239240646362\n",
      "[846/1000]\n",
      "- Train Loss : 0.045873533877056616 Score : 0.9894179925322533\n",
      "- Val Loss : 0.10977811366319656 Score : 0.9538239240646362\n",
      "[847/1000]\n",
      "- Train Loss : 0.045847108532143466 Score : 0.9894179925322533\n",
      "- Val Loss : 0.109919972717762 Score : 0.9538239240646362\n",
      "[848/1000]\n",
      "- Train Loss : 0.04582068546551454 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11006235331296921 Score : 0.9538239240646362\n",
      "[849/1000]\n",
      "- Train Loss : 0.04579428870965785 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11020468920469284 Score : 0.9538239240646362\n",
      "[850/1000]\n",
      "- Train Loss : 0.045767886525936774 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11034725606441498 Score : 0.9538239240646362\n",
      "[851/1000]\n",
      "- Train Loss : 0.04574151662927761 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11049001663923264 Score : 0.9538239240646362\n",
      "[852/1000]\n",
      "- Train Loss : 0.04571518441753142 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11063297837972641 Score : 0.9538239240646362\n",
      "[853/1000]\n",
      "- Train Loss : 0.04568880095757777 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11077578365802765 Score : 0.9538239240646362\n",
      "[854/1000]\n",
      "- Train Loss : 0.04566249249364773 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11091924458742142 Score : 0.9538239240646362\n",
      "[855/1000]\n",
      "- Train Loss : 0.045636212750650884 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11106272041797638 Score : 0.9538239240646362\n",
      "[856/1000]\n",
      "- Train Loss : 0.04560986106935161 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11120588332414627 Score : 0.9538239240646362\n",
      "[857/1000]\n",
      "- Train Loss : 0.045583591681861435 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11134945601224899 Score : 0.9538239240646362\n",
      "[858/1000]\n",
      "- Train Loss : 0.04555725177942804 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11149360984563828 Score : 0.9538239240646362\n",
      "[859/1000]\n",
      "- Train Loss : 0.04553101264718862 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11163746565580368 Score : 0.9538239240646362\n",
      "[860/1000]\n",
      "- Train Loss : 0.04550472630398872 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11178170144557953 Score : 0.9538239240646362\n",
      "[861/1000]\n",
      "- Train Loss : 0.0454784397925323 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11192614585161209 Score : 0.9538239240646362\n",
      "[862/1000]\n",
      "- Train Loss : 0.045452192907760036 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11207070201635361 Score : 0.9538239240646362\n",
      "[863/1000]\n",
      "- Train Loss : 0.04542599873911968 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11221512407064438 Score : 0.9538239240646362\n",
      "[864/1000]\n",
      "- Train Loss : 0.04539976092382858 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11236006766557693 Score : 0.9538239240646362\n",
      "[865/1000]\n",
      "- Train Loss : 0.04537356724176789 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11250502616167068 Score : 0.9538239240646362\n",
      "[866/1000]\n",
      "- Train Loss : 0.0453473517063685 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11265057325363159 Score : 0.9538239240646362\n",
      "[867/1000]\n",
      "- Train Loss : 0.04532117257895152 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11279593408107758 Score : 0.9538239240646362\n",
      "[868/1000]\n",
      "- Train Loss : 0.045295041655663226 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11294124275445938 Score : 0.9538239240646362\n",
      "[869/1000]\n",
      "- Train Loss : 0.04526879378045123 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11308687925338745 Score : 0.9538239240646362\n",
      "[870/1000]\n",
      "- Train Loss : 0.04524265698637464 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1132328137755394 Score : 0.9538239240646362\n",
      "[871/1000]\n",
      "- Train Loss : 0.04521645150452969 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11337918788194656 Score : 0.9538239240646362\n",
      "[872/1000]\n",
      "- Train Loss : 0.04519031449490285 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11352571099996567 Score : 0.9538239240646362\n",
      "[873/1000]\n",
      "- Train Loss : 0.04516420090840256 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11367207765579224 Score : 0.9538239240646362\n",
      "[874/1000]\n",
      "- Train Loss : 0.045138065062928945 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11381815373897552 Score : 0.9538239240646362\n",
      "[875/1000]\n",
      "- Train Loss : 0.045112002731002576 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11396510154008865 Score : 0.9538239240646362\n",
      "[876/1000]\n",
      "- Train Loss : 0.045085821239808865 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11411212384700775 Score : 0.9538239240646362\n",
      "[877/1000]\n",
      "- Train Loss : 0.04505976418931823 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1142592504620552 Score : 0.9538239240646362\n",
      "[878/1000]\n",
      "- Train Loss : 0.045033703036096995 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11440659314393997 Score : 0.9538239240646362\n",
      "[879/1000]\n",
      "- Train Loss : 0.0450075857606862 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11455404758453369 Score : 0.9538239240646362\n",
      "[880/1000]\n",
      "- Train Loss : 0.04498154419889033 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11470137536525726 Score : 0.9538239240646362\n",
      "[881/1000]\n",
      "- Train Loss : 0.0449555224067808 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11484900861978531 Score : 0.9538239240646362\n",
      "[882/1000]\n",
      "- Train Loss : 0.0449294721311162 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11499733477830887 Score : 0.9538239240646362\n",
      "[883/1000]\n",
      "- Train Loss : 0.04490348530543997 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11514562368392944 Score : 0.9538239240646362\n",
      "[884/1000]\n",
      "- Train Loss : 0.04487745072765392 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11529373377561569 Score : 0.9538239240646362\n",
      "[885/1000]\n",
      "- Train Loss : 0.04485141107397794 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11544188112020493 Score : 0.9538239240646362\n",
      "[886/1000]\n",
      "- Train Loss : 0.04482540307890304 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1155911460518837 Score : 0.9538239240646362\n",
      "[887/1000]\n",
      "- Train Loss : 0.044799442126532085 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11574000120162964 Score : 0.9538239240646362\n",
      "[888/1000]\n",
      "- Train Loss : 0.04477347342799476 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11588896811008453 Score : 0.9538239240646362\n",
      "[889/1000]\n",
      "- Train Loss : 0.044747475842086715 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11603819578886032 Score : 0.9538239240646362\n",
      "[890/1000]\n",
      "- Train Loss : 0.0447215234144096 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11618761718273163 Score : 0.9538239240646362\n",
      "[891/1000]\n",
      "- Train Loss : 0.044695598769067146 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1163371279835701 Score : 0.9538239240646362\n",
      "[892/1000]\n",
      "- Train Loss : 0.04466964121093042 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1164867952466011 Score : 0.9538239240646362\n",
      "[893/1000]\n",
      "- Train Loss : 0.044643729833296675 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11663653701543808 Score : 0.9538239240646362\n",
      "[894/1000]\n",
      "- Train Loss : 0.04461781319241709 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11678684502840042 Score : 0.9538239240646362\n",
      "[895/1000]\n",
      "- Train Loss : 0.044591882808163064 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11693733930587769 Score : 0.9538239240646362\n",
      "[896/1000]\n",
      "- Train Loss : 0.0445659753813743 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11708755046129227 Score : 0.9538239240646362\n",
      "[897/1000]\n",
      "- Train Loss : 0.0445401216911705 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11723782867193222 Score : 0.9538239240646362\n",
      "[898/1000]\n",
      "- Train Loss : 0.044514199578770786 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11738894879817963 Score : 0.9538239240646362\n",
      "[899/1000]\n",
      "- Train Loss : 0.04448840937402565 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11753980070352554 Score : 0.9538239240646362\n",
      "[900/1000]\n",
      "- Train Loss : 0.04446254176036746 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11769086867570877 Score : 0.9538239240646362\n",
      "[901/1000]\n",
      "- Train Loss : 0.04443662793528347 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11784233152866364 Score : 0.9538239240646362\n",
      "[902/1000]\n",
      "- Train Loss : 0.044410779916688625 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11799407005310059 Score : 0.9538239240646362\n",
      "[903/1000]\n",
      "- Train Loss : 0.04438497697083221 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11814581602811813 Score : 0.9538239240646362\n",
      "[904/1000]\n",
      "- Train Loss : 0.04435915357498743 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1182975247502327 Score : 0.9538239240646362\n",
      "[905/1000]\n",
      "- Train Loss : 0.04433333590077382 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11844956129789352 Score : 0.9538239240646362\n",
      "[906/1000]\n",
      "- Train Loss : 0.04430752867847332 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11860202252864838 Score : 0.9538239240646362\n",
      "[907/1000]\n",
      "- Train Loss : 0.0442817054772604 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1187543123960495 Score : 0.9538239240646362\n",
      "[908/1000]\n",
      "- Train Loss : 0.04425596652163222 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11890692263841629 Score : 0.9538239240646362\n",
      "[909/1000]\n",
      "- Train Loss : 0.04423014309577411 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11905979365110397 Score : 0.9538239240646362\n",
      "[910/1000]\n",
      "- Train Loss : 0.044204437004736974 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11921273916959763 Score : 0.9538239240646362\n",
      "[911/1000]\n",
      "- Train Loss : 0.044178650497997296 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11936593800783157 Score : 0.9538239240646362\n",
      "[912/1000]\n",
      "- Train Loss : 0.04415292083467648 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1195189505815506 Score : 0.9538239240646362\n",
      "[913/1000]\n",
      "- Train Loss : 0.044127177528935135 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11967263370752335 Score : 0.9538239240646362\n",
      "[914/1000]\n",
      "- Train Loss : 0.04410214951985836 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11981656402349472 Score : 0.9538239240646362\n",
      "[915/1000]\n",
      "- Train Loss : 0.044076106229113066 Score : 0.9894179925322533\n",
      "- Val Loss : 0.11996615678071976 Score : 0.9538239240646362\n",
      "[916/1000]\n",
      "- Train Loss : 0.044050166545275715 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12012366950511932 Score : 0.9538239240646362\n",
      "[917/1000]\n",
      "- Train Loss : 0.044024527500369004 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12028004229068756 Score : 0.9538239240646362\n",
      "[918/1000]\n",
      "- Train Loss : 0.04399906588423619 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12043291330337524 Score : 0.9538239240646362\n",
      "[919/1000]\n",
      "- Train Loss : 0.04397337267528201 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12058625370264053 Score : 0.9538239240646362\n",
      "[920/1000]\n",
      "- Train Loss : 0.04394766939094552 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12074177712202072 Score : 0.9538239240646362\n",
      "[921/1000]\n",
      "- Train Loss : 0.04392198264213221 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12089747935533524 Score : 0.9538239240646362\n",
      "[922/1000]\n",
      "- Train Loss : 0.043896511501770874 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1210516169667244 Score : 0.9538239240646362\n",
      "[923/1000]\n",
      "- Train Loss : 0.04387084429345123 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12120646983385086 Score : 0.9538239240646362\n",
      "[924/1000]\n",
      "- Train Loss : 0.043845216106092266 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12136232852935791 Score : 0.9538239240646362\n",
      "[925/1000]\n",
      "- Train Loss : 0.043819611067192454 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1215180903673172 Score : 0.9538239240646362\n",
      "[926/1000]\n",
      "- Train Loss : 0.0437940597475972 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12167371809482574 Score : 0.9538239240646362\n",
      "[927/1000]\n",
      "- Train Loss : 0.04376848564879765 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12182938307523727 Score : 0.9538239240646362\n",
      "[928/1000]\n",
      "- Train Loss : 0.04374288459348463 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12198573350906372 Score : 0.9538239240646362\n",
      "[929/1000]\n",
      "- Train Loss : 0.043717291920074786 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12214218825101852 Score : 0.9538239240646362\n",
      "[930/1000]\n",
      "- Train Loss : 0.043691733903870045 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1222987100481987 Score : 0.9538239240646362\n",
      "[931/1000]\n",
      "- Train Loss : 0.04366631245829922 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12245497852563858 Score : 0.9538239240646362\n",
      "[932/1000]\n",
      "- Train Loss : 0.04364069715848018 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12261205166578293 Score : 0.9538239240646362\n",
      "[933/1000]\n",
      "- Train Loss : 0.04361516231438145 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12276937067508698 Score : 0.9538239240646362\n",
      "[934/1000]\n",
      "- Train Loss : 0.04358964019684208 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12292670458555222 Score : 0.9538239240646362\n",
      "[935/1000]\n",
      "- Train Loss : 0.04356418609040702 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12308373302221298 Score : 0.9538239240646362\n",
      "[936/1000]\n",
      "- Train Loss : 0.04353871502553375 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12324149161577225 Score : 0.9538239240646362\n",
      "[937/1000]\n",
      "- Train Loss : 0.04351317585133074 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12339971214532852 Score : 0.9538239240646362\n",
      "[938/1000]\n",
      "- Train Loss : 0.043487680986800115 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12355774641036987 Score : 0.9538239240646362\n",
      "[939/1000]\n",
      "- Train Loss : 0.043462229519718676 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12371552735567093 Score : 0.9538239240646362\n",
      "[940/1000]\n",
      "- Train Loss : 0.043436729230961646 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12387419492006302 Score : 0.9538239240646362\n",
      "[941/1000]\n",
      "- Train Loss : 0.04341132736408326 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12403256446123123 Score : 0.9538239240646362\n",
      "[942/1000]\n",
      "- Train Loss : 0.04338582811487868 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12419156730175018 Score : 0.9538239240646362\n",
      "[943/1000]\n",
      "- Train Loss : 0.04336043118746602 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12435005605220795 Score : 0.9538239240646362\n",
      "[944/1000]\n",
      "- Train Loss : 0.04333496399158321 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12450941652059555 Score : 0.9538239240646362\n",
      "[945/1000]\n",
      "- Train Loss : 0.04330952959571732 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1246686726808548 Score : 0.9538239240646362\n",
      "[946/1000]\n",
      "- Train Loss : 0.04328408805668005 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12482846528291702 Score : 0.9538239240646362\n",
      "[947/1000]\n",
      "- Train Loss : 0.04325872584558965 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12498835474252701 Score : 0.9538239240646362\n",
      "[948/1000]\n",
      "- Train Loss : 0.04323336571724212 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1251479685306549 Score : 0.9538239240646362\n",
      "[949/1000]\n",
      "- Train Loss : 0.04320789177199913 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12530812621116638 Score : 0.9538239240646362\n",
      "[950/1000]\n",
      "- Train Loss : 0.043182556025385566 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12546835839748383 Score : 0.9538239240646362\n",
      "[951/1000]\n",
      "- Train Loss : 0.04315719546411856 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12562867999076843 Score : 0.9538239240646362\n",
      "[952/1000]\n",
      "- Train Loss : 0.04313182773785229 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12578968703746796 Score : 0.9538239240646362\n",
      "[953/1000]\n",
      "- Train Loss : 0.043106462586365524 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12595054507255554 Score : 0.9538239240646362\n",
      "[954/1000]\n",
      "- Train Loss : 0.043081153550701856 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12611141800880432 Score : 0.9538239240646362\n",
      "[955/1000]\n",
      "- Train Loss : 0.043055783668933145 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12627284228801727 Score : 0.9538239240646362\n",
      "[956/1000]\n",
      "- Train Loss : 0.04303042644642119 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1264340579509735 Score : 0.9538239240646362\n",
      "[957/1000]\n",
      "- Train Loss : 0.04300509193308244 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12659578025341034 Score : 0.9538239240646362\n",
      "[958/1000]\n",
      "- Train Loss : 0.042979806976291 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1267574578523636 Score : 0.9538239240646362\n",
      "[959/1000]\n",
      "- Train Loss : 0.04295453214672307 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12691934406757355 Score : 0.9538239240646362\n",
      "[960/1000]\n",
      "- Train Loss : 0.04292925829486194 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12708154320716858 Score : 0.9538239240646362\n",
      "[961/1000]\n",
      "- Train Loss : 0.04290393023075012 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12724421918392181 Score : 0.9538239240646362\n",
      "[962/1000]\n",
      "- Train Loss : 0.04287869946801948 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12740643322467804 Score : 0.9538239240646362\n",
      "[963/1000]\n",
      "- Train Loss : 0.04285334034739208 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1275695115327835 Score : 0.9538239240646362\n",
      "[964/1000]\n",
      "- Train Loss : 0.04282808425159601 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12773273885250092 Score : 0.9538239240646362\n",
      "[965/1000]\n",
      "- Train Loss : 0.04280284892593045 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12789557874202728 Score : 0.9538239240646362\n",
      "[966/1000]\n",
      "- Train Loss : 0.042777636996106594 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1280585080385208 Score : 0.9538239240646362\n",
      "[967/1000]\n",
      "- Train Loss : 0.042752331038173 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12822261452674866 Score : 0.9538239240646362\n",
      "[968/1000]\n",
      "- Train Loss : 0.04272714953276591 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1283864676952362 Score : 0.9538239240646362\n",
      "[969/1000]\n",
      "- Train Loss : 0.0427019676699274 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12855024635791779 Score : 0.9538239240646362\n",
      "[970/1000]\n",
      "- Train Loss : 0.042676720503550314 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12871427834033966 Score : 0.9538239240646362\n",
      "[971/1000]\n",
      "- Train Loss : 0.0426515299386665 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12887844443321228 Score : 0.9538239240646362\n",
      "[972/1000]\n",
      "- Train Loss : 0.042626291763554036 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1290431171655655 Score : 0.9538239240646362\n",
      "[973/1000]\n",
      "- Train Loss : 0.04260112436077179 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12920789420604706 Score : 0.9538239240646362\n",
      "[974/1000]\n",
      "- Train Loss : 0.04257595010494697 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12937262654304504 Score : 0.9538239240646362\n",
      "[975/1000]\n",
      "- Train Loss : 0.04255079960785224 Score : 0.9894179925322533\n",
      "- Val Loss : 0.129537895321846 Score : 0.9538239240646362\n",
      "[976/1000]\n",
      "- Train Loss : 0.04252563609134086 Score : 0.9894179925322533\n",
      "- Val Loss : 0.12970320880413055 Score : 0.9538239240646362\n",
      "[977/1000]\n",
      "- Train Loss : 0.04250051665712817 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1298685520887375 Score : 0.9538239240646362\n",
      "[978/1000]\n",
      "- Train Loss : 0.04247545099678973 Score : 0.9894179925322533\n",
      "- Val Loss : 0.13002537190914154 Score : 0.9538239240646362\n",
      "[979/1000]\n",
      "- Train Loss : 0.04245023993189534 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1301845759153366 Score : 0.9538239240646362\n",
      "[980/1000]\n",
      "- Train Loss : 0.04242478666583338 Score : 0.9894179925322533\n",
      "- Val Loss : 0.13035331666469574 Score : 0.9538239240646362\n",
      "[981/1000]\n",
      "- Train Loss : 0.042399692049002624 Score : 0.9894179925322533\n",
      "- Val Loss : 0.13052262365818024 Score : 0.9538239240646362\n",
      "[982/1000]\n",
      "- Train Loss : 0.04237472199747572 Score : 0.9894179925322533\n",
      "- Val Loss : 0.13068845868110657 Score : 0.9538239240646362\n",
      "[983/1000]\n",
      "- Train Loss : 0.04234969836033997 Score : 0.9894179925322533\n",
      "- Val Loss : 0.13085350394248962 Score : 0.9538239240646362\n",
      "[984/1000]\n",
      "- Train Loss : 0.04232457078160223 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1310202181339264 Score : 0.9538239240646362\n",
      "[985/1000]\n",
      "- Train Loss : 0.0422994001319239 Score : 0.9894179925322533\n",
      "- Val Loss : 0.13118810951709747 Score : 0.9538239240646362\n",
      "[986/1000]\n",
      "- Train Loss : 0.04227442644696566 Score : 0.9894179925322533\n",
      "- Val Loss : 0.13135504722595215 Score : 0.9538239240646362\n",
      "[987/1000]\n",
      "- Train Loss : 0.04224935304955579 Score : 0.9894179925322533\n",
      "- Val Loss : 0.13152220845222473 Score : 0.9538239240646362\n",
      "[988/1000]\n",
      "- Train Loss : 0.04222433063569042 Score : 0.9894179925322533\n",
      "- Val Loss : 0.13168971240520477 Score : 0.9538239240646362\n",
      "[989/1000]\n",
      "- Train Loss : 0.04219927271878987 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1318579763174057 Score : 0.9538239240646362\n",
      "[990/1000]\n",
      "- Train Loss : 0.042174247251750785 Score : 0.9894179925322533\n",
      "- Val Loss : 0.13202567398548126 Score : 0.9538239240646362\n",
      "[991/1000]\n",
      "- Train Loss : 0.04214924446750956 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1321936398744583 Score : 0.9538239240646362\n",
      "[992/1000]\n",
      "- Train Loss : 0.04212423899298301 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1323620080947876 Score : 0.9538239240646362\n",
      "[993/1000]\n",
      "- Train Loss : 0.04209920723315008 Score : 0.9894179925322533\n",
      "- Val Loss : 0.13253073394298553 Score : 0.9538239240646362\n",
      "[994/1000]\n",
      "- Train Loss : 0.04207421827413782 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1326993852853775 Score : 0.9538239240646362\n",
      "[995/1000]\n",
      "- Train Loss : 0.042049231608871196 Score : 0.9894179925322533\n",
      "- Val Loss : 0.13286848366260529 Score : 0.9538239240646362\n",
      "[996/1000]\n",
      "- Train Loss : 0.04202426183837815 Score : 0.9894179925322533\n",
      "- Val Loss : 0.13303765654563904 Score : 0.9538239240646362\n",
      "[997/1000]\n",
      "- Train Loss : 0.04199927845456841 Score : 0.9894179925322533\n",
      "- Val Loss : 0.13320697844028473 Score : 0.9538239240646362\n",
      "[998/1000]\n",
      "- Train Loss : 0.04197430322165019 Score : 0.9894179925322533\n",
      "- Val Loss : 0.1333766132593155 Score : 0.9538239240646362\n",
      "[999/1000]\n",
      "- Train Loss : 0.04194936602470989 Score : 0.9894179925322533\n",
      "- Val Loss : 0.13354630768299103 Score : 0.9538239240646362\n"
     ]
    }
   ],
   "source": [
    "## 학습의 효과 확인 - 손실값과 성능평가값 저장 필요\n",
    "LOSS_HISTORY, SCORE_HISTORY = [[],[]], [[],[]]\n",
    "CNT = len(trainDL)\n",
    "print(f'CNT =>{CNT}')\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # 학습 모드로 모델 성정\n",
    "    model.train()\n",
    "\n",
    "    # 배치크기만큼 데이터 로딩해서 학습 진행\n",
    "    loss_total, score_total = 0,0\n",
    "    for featureTS, targetTS in trainDL :\n",
    "        # 학습 진행\n",
    "        pre_y = model(featureTS)\n",
    "\n",
    "        # 손실계산 : CrossEntropyLoss 요구사항 : 타겟(정답)은 0차원 또는 1차원, 타입은 long \n",
    "        loss = crossLoss(pre_y, targetTS.reshape(-1).long())\n",
    "        loss_total += loss.item()\n",
    "\n",
    "        # 성능평가 계산 : MulticlassF1Score 타입은 long으로 바꾸면 안됨\n",
    "        score = MulticlassF1Score(num_classes=3)(pre_y, targetTS.reshape(-1))\n",
    "        # 방법2 : score = F1Score(task='multiclass', num_classes = 3)(pre_y, targetTS)\n",
    "        score_total += score.item()\n",
    "\n",
    "        # 최적화 진행\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 에포크 당 검증기능\n",
    "    # 모델 검증 모드 설정\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 검증용 데이터셋 생성\n",
    "        val_feature_TS = torch.FloatTensor(valDS.featureDF.values)\n",
    "        val_target_TS = torch.FloatTensor(valDS.targetDF.values)\n",
    "        # 평가\n",
    "        pre_val = model(val_feature_TS)\n",
    "        # 손실 계산\n",
    "        loss_val = crossLoss(pre_val, val_target_TS.reshape(-1).long())\n",
    "        # 성능 평가\n",
    "        score_val = MulticlassF1Score(num_classes=3)(pre_val, val_target_TS.reshape(-1))\n",
    "\n",
    "    # 에포크 당 손실과 성능평가값 저장\n",
    "    LOSS_HISTORY[0].append(loss_total/CNT)\n",
    "    SCORE_HISTORY[0].append(score_total/CNT)\n",
    "    \n",
    "    LOSS_HISTORY[1].append(loss_val)\n",
    "    SCORE_HISTORY[1].append(score_val)\n",
    "\n",
    "    print(f'[{epoch}/{EPOCH}]\\n- Train Loss : {LOSS_HISTORY[0][-1]} Score : {SCORE_HISTORY[0][-1]}')\n",
    "    print(f'- Val Loss : {LOSS_HISTORY[1][-1]} Score : {SCORE_HISTORY[1][-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 테스트 할때 : no_grad() 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 19.1227,   5.2893, -26.7095],\n",
      "        [ -2.1508,   4.1566,  -7.3629],\n",
      "        [ -6.6795,   4.8348,  -5.6214],\n",
      "        [ 17.1657,   4.7537, -24.0097],\n",
      "        [-12.8677,   2.0972,   1.6928],\n",
      "        [ -7.9063,   4.0278,  -3.6308],\n",
      "        [-13.8907,  -0.8883,   6.8884],\n",
      "        [ 15.0252,   4.2275, -21.1841],\n",
      "        [ 14.4032,   3.9977, -20.1989],\n",
      "        [-14.9144,  -1.8676,   8.9188],\n",
      "        [ -6.6377,   3.9785,  -4.0533],\n",
      "        [ 16.2860,   4.5129, -22.7962],\n",
      "        [-14.8486,  -0.9194,   7.5562],\n",
      "        [ -7.0209,   4.5941,  -5.0224],\n",
      "        [ -8.1113,   3.6588,  -2.8104],\n",
      "        [ 14.6253,   4.0584, -20.5053],\n",
      "        [ -5.6534,   4.3935,  -5.3431],\n",
      "        [ -7.6954,   3.0892,  -2.1780],\n",
      "        [ 15.0239,   4.3426, -21.4284],\n",
      "        [ 15.7205,   4.3582, -22.0161],\n",
      "        [ -6.7848,   3.8454,  -3.7055],\n",
      "        [ -7.6540,   2.6954,  -1.5926],\n",
      "        [-10.2794,   2.8680,  -0.5468],\n",
      "        [ 15.9017,   4.4078, -22.2660],\n",
      "        [-12.5909,   1.5983,   2.5494],\n",
      "        [ -5.9784,   4.2413,  -4.9141],\n",
      "        [ 18.8040,   5.2021, -26.2698],\n",
      "        [ 16.4459,   4.5567, -23.0168],\n",
      "        [ -6.7526,   4.3733,  -4.6976],\n",
      "        [-13.0874,  -0.3994,   6.0264],\n",
      "        [ -7.3028,   4.2505,  -4.2076],\n",
      "        [-16.6507,  -1.5266,   9.3666],\n",
      "        [ -5.0056,   4.3833,  -5.6709],\n",
      "        [-14.6913,  -2.1352,   9.3975],\n",
      "        [-15.1577,  -4.1934,  12.7021],\n",
      "        [ 16.8954,   4.6797, -23.6368],\n",
      "        [ -7.6026,   3.6161,  -2.8118],\n",
      "        [ 16.1312,   4.4706, -22.5827]])\n"
     ]
    }
   ],
   "source": [
    "# 모델 검증 모드 설정\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 검증용 데이터셋 생성\n",
    "    test_featureTS = torch.FloatTensor(testDS.featureDF.values)\n",
    "    test_targetTS = torch.FloatTensor(testDS.targetDF.values)\n",
    "    # 추론 / 평가\n",
    "    pre_test = model(test_featureTS)\n",
    "    print(pre_test)\n",
    "    # 손실 계산\n",
    "    loss_test = crossLoss(pre_test, test_targetTS.reshape(-1).long())\n",
    "    # 성능 평가\n",
    "    score_test = MulticlassF1Score(num_classes=3)(pre_test, test_targetTS.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0343) tensor(0.9703)\n"
     ]
    }
   ],
   "source": [
    "print(loss_test, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
