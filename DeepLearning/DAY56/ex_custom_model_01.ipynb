{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용자 정의 모델 클래스\n",
    "- 부모 클래스 : nn.Module\n",
    "- 필수 오버라이딩\n",
    "    * _init_() : 모델 층 구성, 즉 설계\n",
    "    * forward() : 순방향 학습 진행 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모듈 로딩\n",
    "import torch                                    # 텐서 관련 모듈\n",
    "import torch.nn as nn                           # 인공신경망 관련 모듈\n",
    "import torch.nn.functional as F                 # 인공신경망 관련 함수들 모듈 (손실함수, 활성화 함수 등등)\n",
    "import torch.optim as optim                     # 최적화 관련 모듈 (가중치, 절편 빠르게 찾아주는 알고리즘)\n",
    "\n",
    "from torchinfo import summary                   # 모델 구조 및 정보 관련 모듈\n",
    "from torchmetrics.regression import *           # 회귀 성능 지표 관련 모듈\n",
    "from torchmetrics.classification import *       # 분류 성능 지표 관련 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 고정\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 텐서 저장 및 실행 위치 설정\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [기본] 신경망 클래스 <hr>\n",
    "    * 입력층 - 입력 피처수 고정\n",
    "    * 출력층 - 출력 타겟수 고정\n",
    "    * 은닉층 - 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설계\n",
    "# 데이터셋 : 피처 4개, 타겟 1개, 회귀\n",
    "# 입력층 : 입력 -   4개 / 출력 -  20개 / AF ReLU\n",
    "# 은닉층 : 입력 -  20개 / 출력 - 100개 / AF ReLU\n",
    "# 출력층 : 입력 - 100개 / 출력 -   1개 / AF X, Sigmoid & Softmax (분류)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피처수가 고정인 모델\n",
    "class MyModel(nn.Module):\n",
    "    # 인스턴스/객체 생성 시 자동 호출되는 메서드 (콜백 함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    def __init__(self):\n",
    "        # 부모 클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(4,20)      # W   4  + b 1 =>  1P ,  5*20 =   100개 변수 \n",
    "        self.hidden_layer = nn.Linear(20,100)   # W  20 + b 1 =>  21P , 21*100 =  2100개 변수\n",
    "        self.output_layer = nn.Linear(100,1)    # W 100 + b 1 => 101P\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동 호출되는 메서드 (콜백 함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x)  # 1개 퍼셉트론 : y=x1w1 + x2w2 + x3w3 + x4w4 + b ==> 20개\n",
    "        y = F.relu(y)                #  0 <= y  ----> 죽은 relu ==> leakyReLU\n",
    "        \n",
    "        y = self.hidden_layer(y) # 1개 퍼셉트론 : y = x1w1 + x2w2 + ... + x20w20 + b ==> 100개\n",
    "        y = F.relu(y)\n",
    "\n",
    "        return self.output_layer(y) # 1개 퍼셉트론 : y = x1w1 + x2w2 + ... + x100w100 + b ==> 1개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피처수가 동적인 모델\n",
    "class MyModel2(nn.Module):\n",
    "    # 인스턴스/객체 생성 시 자동 호출되는 메서드 (콜백 함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    def __init__(self, in_features):\n",
    "        # 부모 클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(in_features,20)      # W   4  + b 1 =>  1P ,  5*20 =   100개 변수 \n",
    "        self.hidden_layer = nn.Linear(20,100)   # W  20 + b 1 =>  21P , 21*100 =  2100개 변수\n",
    "        self.output_layer = nn.Linear(100,1)    # W 100 + b 1 => 101P\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동 호출되는 메서드 (콜백 함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x)  # 1개 퍼셉트론 : y=x1w1 + x2w2 + x3w3 + x4w4 + b ==> 20개\n",
    "        y = F.relu(y)                #  0 <= y  ----> 죽은 relu ==> leakyReLU\n",
    "        \n",
    "        y = self.hidden_layer(y) # 1개 퍼셉트론 : y = x1w1 + x2w2 + ... + x20w20 + b ==> 100개\n",
    "        y = F.relu(y)\n",
    "\n",
    "        return self.output_layer(y) # 1개 퍼셉트론 : y = x1w1 + x2w2 + ... + x100w100 + b ==> 1개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피처수, 은닉층 퍼셉트론 수가 동적인 모델\n",
    "class MyModel3(nn.Module):\n",
    "    # 인스턴스/객체 생성 시 자동 호출되는 메서드 (콜백 함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    def __init__(self, in_features, in_out, h_out):\n",
    "        # 부모 클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(in_features,in_out)      # W   4  + b 1 =>  1P ,  5*20 =   100개 변수 \n",
    "        self.hidden_layer = nn.Linear(in_out,h_out)   # W  20 + b 1 =>  21P , 21*100 =  2100개 변수\n",
    "        self.output_layer = nn.Linear(h_out,1)    # W 100 + b 1 => 101P\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동 호출되는 메서드 (콜백 함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x)  # 1개 퍼셉트론 : y=x1w1 + x2w2 + x3w3 + x4w4 + b ==> 20개\n",
    "        y = F.relu(y)                #  0 <= y  ----> 죽은 relu ==> leakyReLU\n",
    "        \n",
    "        y = self.hidden_layer(y) # 1개 퍼셉트론 : y = x1w1 + x2w2 + ... + x20w20 + b ==> 100개\n",
    "        y = F.relu(y)\n",
    "\n",
    "        return self.output_layer(y) # 1개 퍼셉트론 : y = x1w1 + x2w2 + ... + x100w100 + b ==> 1개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은닉층의 개수가 동적인 모델\n",
    "class MyModel4(nn.Module):\n",
    "    # 인스턴스/객체 생성 시 자동 호출되는 메서드 (콜백 함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    def __init__(self, in_features, in_out, h_out):\n",
    "        # 부모 클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(in_features,in_out)      # W   4  + b 1 =>  1P ,  5*20 =   100개 변수\n",
    "        self.hidden_layer = nn.Linear(in_out,h_out)   # W  20 + b 1 =>  21P , 21*100 =  2100개 변수\n",
    "        self.output_layer = nn.Linear(h_out,1)    # W 100 + b 1 => 101P\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동 호출되는 메서드 (콜백 함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x)  # 1개 퍼셉트론 : y=x1w1 + x2w2 + x3w3 + x4w4 + b ==> 20개\n",
    "        y = F.relu(y)                #  0 <= y  ----> 죽은 relu ==> leakyReLU\n",
    "        \n",
    "        y = self.hidden_layer(y) # 1개 퍼셉트론 : y = x1w1 + x2w2 + ... + x20w20 + b ==> 100개\n",
    "        y = F.relu(y)\n",
    "\n",
    "        return self.output_layer(y) # 1개 퍼셉트론 : y = x1w1 + x2w2 + ... + x100w100 + b ==> 1개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 인스턴스 생성\n",
    "m1 = MyModel()\n",
    "m2 = MyModel3(3,50,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyModel1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_parameters at 0x000002104605E5F0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1025,  0.4477, -0.4962, -0.0496],\n",
      "        [ 0.2259,  0.3448,  0.0065, -0.2702],\n",
      "        [-0.3865, -0.3786,  0.4026, -0.4354],\n",
      "        [-0.1978,  0.2621,  0.2734, -0.4374],\n",
      "        [ 0.0991,  0.0604,  0.4589, -0.0177],\n",
      "        [ 0.2123, -0.0432,  0.4463, -0.0510],\n",
      "        [-0.2938,  0.0922, -0.2647,  0.1250],\n",
      "        [ 0.3476, -0.3889, -0.3638,  0.0068],\n",
      "        [-0.0292, -0.1556,  0.4185,  0.2242],\n",
      "        [-0.1658,  0.0725, -0.0752, -0.3201],\n",
      "        [ 0.2046, -0.3299,  0.0605,  0.3394],\n",
      "        [-0.4592,  0.0219,  0.4617,  0.2258],\n",
      "        [-0.1972,  0.0787, -0.4695,  0.0803],\n",
      "        [ 0.4347,  0.4345, -0.0080,  0.1813],\n",
      "        [-0.0830, -0.1403,  0.0096, -0.2913],\n",
      "        [ 0.2013,  0.2451, -0.1355, -0.1138],\n",
      "        [-0.3484,  0.0639,  0.1989,  0.4747],\n",
      "        [-0.4711, -0.4215,  0.0141,  0.4621],\n",
      "        [ 0.0322,  0.1388,  0.0710, -0.0598],\n",
      "        [ 0.3569,  0.3670, -0.2883, -0.2994]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3888, -0.1216, -0.0113,  0.4185, -0.4839,  0.3549, -0.2793,  0.0408,\n",
      "        -0.0234,  0.2824, -0.4183, -0.3492,  0.0115,  0.2521, -0.2406,  0.1550,\n",
      "        -0.1321, -0.4663,  0.0571, -0.1066], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1323,  0.1473,  0.1463,  ..., -0.1648,  0.1378,  0.1404],\n",
      "        [-0.0419, -0.1773,  0.1660,  ..., -0.1322, -0.1156, -0.1834],\n",
      "        [ 0.1916, -0.1501,  0.0647,  ..., -0.1309, -0.2061, -0.1127],\n",
      "        ...,\n",
      "        [-0.0697, -0.1501,  0.2130,  ..., -0.0376,  0.1674, -0.0612],\n",
      "        [-0.0473, -0.0510, -0.1284,  ..., -0.1133, -0.0810, -0.0360],\n",
      "        [-0.1129,  0.1151, -0.1233,  ...,  0.1510,  0.0374,  0.0398]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2154,  0.1013, -0.1076,  0.1199, -0.1438, -0.1516,  0.1571,  0.0632,\n",
      "        -0.0650, -0.1445, -0.1959,  0.1575, -0.0763,  0.0484,  0.1331, -0.0552,\n",
      "        -0.1051, -0.1246,  0.0191, -0.0239,  0.0014,  0.0575, -0.2004,  0.1130,\n",
      "        -0.0353, -0.1928, -0.0809,  0.0036,  0.1951,  0.2174, -0.0839, -0.1524,\n",
      "         0.1744,  0.1812,  0.1296, -0.2067,  0.0377,  0.0506,  0.0670, -0.1542,\n",
      "         0.0184, -0.0882,  0.1399, -0.1470,  0.1892,  0.0629,  0.0542, -0.1015,\n",
      "        -0.0131,  0.0556, -0.1697,  0.0301,  0.1207,  0.0893,  0.1873, -0.1130,\n",
      "        -0.2226, -0.0875, -0.1267,  0.1279,  0.0683, -0.2129, -0.1578, -0.2012,\n",
      "         0.0732,  0.0563,  0.0169, -0.0633,  0.1953,  0.1715, -0.2021, -0.1411,\n",
      "         0.0596, -0.1523,  0.0334,  0.0486, -0.1636,  0.1548, -0.1460, -0.1396,\n",
      "        -0.0369, -0.1004,  0.0182, -0.1394,  0.1408, -0.2114, -0.0715,  0.0579,\n",
      "         0.0676,  0.1113, -0.0606,  0.0411, -0.1858, -0.0732,  0.0078, -0.2015,\n",
      "        -0.1234,  0.0595, -0.1960,  0.1634], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0480, -0.0397,  0.0592, -0.0289,  0.0263,  0.0876,  0.0768,  0.0810,\n",
      "         -0.0867,  0.0934,  0.0337,  0.0321, -0.0262, -0.0577, -0.0723,  0.0065,\n",
      "         -0.0771, -0.0961,  0.0719, -0.0372, -0.0863, -0.0966,  0.0210, -0.0655,\n",
      "         -0.0301, -0.0633, -0.0469,  0.0548,  0.1000,  0.0524,  0.0063,  0.0090,\n",
      "         -0.0779, -0.0296,  0.0563, -0.0679,  0.0548, -0.0588, -0.0911,  0.0351,\n",
      "          0.0941,  0.0934, -0.0674, -0.0844,  0.0668, -0.0830,  0.0670,  0.0997,\n",
      "         -0.0010,  0.0876, -0.0814,  0.0776,  0.0774,  0.0981,  0.0264,  0.0609,\n",
      "         -0.0623,  0.0141,  0.0335,  0.0123,  0.0352,  0.0380,  0.0819, -0.0795,\n",
      "          0.0309, -0.0724,  0.0224,  0.0701, -0.0620, -0.0422, -0.0671,  0.0863,\n",
      "         -0.0110, -0.0376, -0.0106,  0.0019, -0.0543, -0.0258, -0.0618,  0.0933,\n",
      "          0.0820,  0.0543, -0.0264, -0.0510,  0.0980, -0.0624, -0.0869,  0.0516,\n",
      "          0.0712,  0.0733,  0.0202, -0.0084, -0.0995, -0.0208,  0.0482, -0.0424,\n",
      "         -0.0359,  0.0529, -0.0322,  0.0037]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0967], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 모델 파라미터, 즉 W와 b 확인\n",
    "for m in m1.parameters():print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling forward()\n",
      "tensor([[-0.1395],\n",
      "        [-0.1858]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 학습 진행 ==> 모델 인스턴스명(데이터)\n",
    "# 임의의 데이터\n",
    "dataTS = torch.FloatTensor([[1,3,5,7],[2,4,6,8]])\n",
    "targetTS = torch.FloatTensor([[4],[5]])\n",
    "\n",
    "# 학습\n",
    "pre_y = m1(dataTS)\n",
    "print(pre_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyModel3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1055,  0.1731,  0.3126],\n",
      "        [ 0.1047,  0.2473,  0.4548],\n",
      "        [ 0.3073,  0.2808,  0.5694],\n",
      "        [ 0.1517,  0.4245,  0.3458],\n",
      "        [-0.4783, -0.3805,  0.4100],\n",
      "        [ 0.5529, -0.2264,  0.2219],\n",
      "        [ 0.4732, -0.4674, -0.2097],\n",
      "        [ 0.5043, -0.0995,  0.1917],\n",
      "        [ 0.1908,  0.1261,  0.5377],\n",
      "        [ 0.5048, -0.4805,  0.1511],\n",
      "        [ 0.1609,  0.0817, -0.1890],\n",
      "        [ 0.2374, -0.1757,  0.2817],\n",
      "        [ 0.1493,  0.3702, -0.0461],\n",
      "        [-0.1539,  0.4455,  0.3673],\n",
      "        [ 0.2049, -0.5536,  0.2376],\n",
      "        [ 0.0480, -0.5712, -0.5656],\n",
      "        [ 0.0847, -0.1088,  0.4414],\n",
      "        [ 0.0405, -0.4604, -0.0941],\n",
      "        [ 0.3139,  0.1358,  0.5173],\n",
      "        [ 0.4296,  0.1824,  0.5300],\n",
      "        [-0.3494,  0.0475, -0.1926],\n",
      "        [-0.1424,  0.0779,  0.3946],\n",
      "        [-0.1145,  0.2290,  0.4468],\n",
      "        [ 0.0802, -0.0741, -0.4250],\n",
      "        [-0.3210,  0.0176,  0.1149],\n",
      "        [-0.0788, -0.5200, -0.3425],\n",
      "        [ 0.3381, -0.0300, -0.5727],\n",
      "        [-0.5279, -0.2678, -0.5203],\n",
      "        [ 0.4328,  0.4365, -0.0628],\n",
      "        [ 0.3852,  0.1030, -0.2242],\n",
      "        [-0.0360, -0.2001, -0.1863],\n",
      "        [ 0.3563, -0.0236,  0.2599],\n",
      "        [ 0.3004,  0.3518,  0.0367],\n",
      "        [ 0.0309,  0.3349, -0.3110],\n",
      "        [ 0.0069,  0.1606, -0.0405],\n",
      "        [-0.1035,  0.2858,  0.2429],\n",
      "        [-0.4117, -0.1387, -0.3469],\n",
      "        [ 0.3030,  0.0927,  0.0616],\n",
      "        [ 0.2800, -0.0414,  0.3829],\n",
      "        [-0.0972,  0.3194,  0.2537],\n",
      "        [ 0.5057, -0.0846,  0.0777],\n",
      "        [-0.5652,  0.4813,  0.0454],\n",
      "        [ 0.1943, -0.5218, -0.0898],\n",
      "        [ 0.4843, -0.2771,  0.1621],\n",
      "        [ 0.3379,  0.3148,  0.5356],\n",
      "        [-0.2887, -0.4054,  0.1658],\n",
      "        [-0.0870, -0.4086, -0.5390],\n",
      "        [ 0.4713, -0.3117,  0.5013],\n",
      "        [-0.4032,  0.2909, -0.2200],\n",
      "        [ 0.4161,  0.5556,  0.4882]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2931, -0.1603, -0.3670, -0.0955,  0.1751,  0.3964, -0.2756,  0.3715,\n",
      "        -0.1196,  0.0129, -0.3791,  0.1356,  0.0726,  0.3758,  0.4141,  0.4707,\n",
      "         0.3363, -0.0121,  0.3449,  0.4308,  0.4099,  0.3687, -0.1884,  0.0239,\n",
      "        -0.2343,  0.2911, -0.1338, -0.5699, -0.2505, -0.4420,  0.1224,  0.4177,\n",
      "        -0.0284, -0.2341, -0.1799, -0.4621,  0.2589,  0.3399, -0.4099,  0.4139,\n",
      "         0.1978, -0.5070,  0.3510, -0.0395, -0.3010,  0.2821, -0.3019,  0.1108,\n",
      "         0.5545,  0.3221], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0913, -0.0771, -0.0825,  ...,  0.0529,  0.0398,  0.0735],\n",
      "        [-0.0817,  0.1353,  0.0705,  ...,  0.0634, -0.0301,  0.0478],\n",
      "        [ 0.0343,  0.1101, -0.1229,  ..., -0.0675,  0.0892,  0.0405],\n",
      "        ...,\n",
      "        [ 0.0913,  0.1013,  0.1004,  ...,  0.1220,  0.0585,  0.0794],\n",
      "        [ 0.1039, -0.0677,  0.0443,  ...,  0.0328,  0.0693,  0.0069],\n",
      "        [-0.0839, -0.0839,  0.0071,  ...,  0.0121, -0.0265,  0.1383]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0477, -0.0529,  0.0313, -0.0790, -0.0761,  0.1200,  0.0755, -0.0986,\n",
      "        -0.0245, -0.0144, -0.0702, -0.0780,  0.0840,  0.0920, -0.0499, -0.1362,\n",
      "         0.1047, -0.0875, -0.0104, -0.0722, -0.0560,  0.0926,  0.1151, -0.0684,\n",
      "         0.1058, -0.0848, -0.1165,  0.0378, -0.0092,  0.0729],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1313, -0.0678,  0.0367,  0.0649,  0.0070, -0.0281,  0.0503, -0.1535,\n",
      "         -0.0136, -0.1127, -0.1723,  0.1520, -0.1200, -0.0752,  0.1535, -0.1110,\n",
      "         -0.0265,  0.1519,  0.0804,  0.0965,  0.0586, -0.0183,  0.1409,  0.1547,\n",
      "         -0.0181, -0.0611, -0.0780,  0.0196, -0.1408, -0.1598]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1793], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 모델 파라미터, 즉 W와 b 확인\n",
    "for m in m2.parameters():print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling forward()\n",
      "tensor([[-0.4719],\n",
      "        [-0.5332]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 학습 진행 ==> 모델 인스턴스명(데이터)\n",
    "# 임의의 데이터\n",
    "dataTS = torch.FloatTensor([[1,3,5],[2,4,6]])\n",
    "targetTS = torch.FloatTensor([[4],[5]])\n",
    "\n",
    "# 학습\n",
    "pre_y = m2(dataTS)\n",
    "print(pre_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
